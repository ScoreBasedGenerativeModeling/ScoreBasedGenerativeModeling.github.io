---
layout: default
---

<div class="row">
 {% capture x %}
  * [James Thornton](https://jtt94.github.io/)
  * [Valentin De Bortoli](https://vdeborto.github.io/)
 {% endcapture %}{{ x | markdownify }}
 </div>

<div class="row">
  <span class="selector">
    <span>Select Keywords: </span>
    <select id="titleselection" class="selectpicker" multiple data-live-search="true">
      <option>Variational Diffusion Models</option>
<option>Generative Modeling by Estimating Gradients of the Data Distribution</option>
<option>Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling</option>
<option>Score-Based Generative Modeling through Stochastic Differential Equations</option>
<option>Diffusion Models Beat GANs on Image Synthesis</option>
<option>Image Super-Resolution via Iterative Refinement</option>
<option>Symbolic Music Generation with Diffusion Models</option>
<option>Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech</option>
<option>Gotta Go Fast When Generating Data with Score-Based Models</option>
<option>D2C: Diffusion-Denoising Models for Few-shot Conditional Generation</option>
<option>Cascaded Diffusion Models for High Fidelity Image Generation</option>
<option>Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation</option>
<option>Structured Denoising Diffusion Models in Discrete State-Spaces</option>
<option>PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior</option>
<option>CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation</option>
<option>Non Gaussian Denoising Diffusion Models</option>
<option>Deep Generative Learning via Schrödinger Bridge</option>
<option>On Fast Sampling of Diffusion Probabilistic Models</option>
<option>A Variational Perspective on Diffusion-Based Generative Models and Score Matching</option>
<option>Score-based Generative Modeling in Latent Space</option>
<option>Learning to Efficiently Sample from Diffusion Probabilistic Models</option>
<option>Denoising Diffusion Probabilistic Models</option>
<option>Adversarial score matching and improved sampling for image generation</option>
<option>Learning Gradient Fields for Shape Generation</option>
<option>WaveGrad: Estimating Gradients for Waveform Generation</option>
<option>DiffWave: A Versatile Diffusion Model for Audio Synthesis</option>
<option>Learning Energy-Based Models by Diffusion Recovery Likelihood</option>
<option>Improved Techniques for Training Score-Based Generative Models</option>
<option>Denoising Diffusion Implicit Models</option>
<option>Permutation Invariant Graph Generation via Score-Based Generative Modeling</option>
<option>Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions</option>
<option>Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed</option>
<option>Improved Denoising Diffusion Probabilistic Models</option>
<option>Maximum Likelihood Training of Score-Based Diffusion Models</option>
<option>From data to noise to data: mixing physics across temperatures with generative artificial intelligence</option>
<option>VoiceGrad: Non-Parallel Any-to-Many Voice Conversion with Annealed Langevin Dynamics</option>
<option>UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models</option>
<option>Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting</option>
<option>Diffusion-Based Representation Learning</option>
<option>Beyond In-Place Corruption: Insertion and Deletion In Denoising Probabilistic Models</option>
<option>Diffusion Priors In Variational Autoencoders</option>
<option>ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models</option>
<option>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</option>
<option>Interpreting diffusion score matching using normalizing flow</option>
<option>CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis</option>
<option>SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</option>
<option>Noise Estimation for Generative Diffusion Models</option>
<option>NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling</option>
<option>Bilateral Denoising Diffusion Models</option>
<option>Stochastic Image Denoising by Sampling from the Posterior Distribution</option>
<option>SNIPS: Solving Noisy Inverse Problems Stochastically</option>
<option>Efficient Learning of Generative Models via Finite-Difference Score Matching</option>
<option>DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism</option>
<option>Adversarial purification with Score-based generative models</option>
<option>DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion</option>
<option>On tuning consistent annealed sampling for denoising score matching</option>
<option>Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme</option>
<option>Score-Based Generative Classifiers</option>
<option>Autoregressive Diffusion Models</option>
<option>Score-based Generative Neural Networks for Large-Scale Optimal Transport</option>
<option>Score-based diffusion models for accelerated MRI</option>
<option>Crystal Diffusion Variational Autoencoder for Periodic Material Generation</option>
<option>Diffusion Normalizing Flow</option>
<option>Score-Based Point Cloud Denoising (Learning Gradient Fields for Point Cloud Denoising)</option>
<option>Zero-Shot Translation using Diffusion Models</option>
<option>Estimating High Order Gradients of the Data Distribution by Denoising</option>
<option>Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory</option>
<option>A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion</option>
<option>Tackling the Generative Learning Trilemma with Denoising Diffusion GANs</option>
<option>Controllable and Compositional Generation with Latent-Space Energy-Based Models</option>
<option>Palette: Image-to-Image Diffusion Models</option>
<option>Conditional Image Generation with Score-Based Diffusion Models</option>
<option>Vector Quantized Diffusion Model for Text-to-Image Synthesis</option>
<option>Blended Diffusion for Text-driven Editing of Natural Images</option>
<option>SegDiff: Image Segmentation with Diffusion Probabilistic Models</option>
<option>Noise Distribution Adaptive Self-Supervised Image Denoising using Tweedie Distribution and Score Matching</option>
<option>Deblurring via Stochastic Refinement</option>
<option>Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction</option>
<option>DiffuseMorph: Unsupervised Deformable Image Registration Using Diffusion Model</option>
<option>More Control for Free! Image Synthesis with Semantic Diffusion Guidance</option>
<option>Score-Based Generative Modeling with Critically-Damped Langevin Diffusion</option>
<option>Heavy-tailed denoising score matching</option>
<option>High-Resolution Image Synthesis with Latent Diffusion Models</option>
<option>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</option>
<option>Quasi-Taylor Samplers for Diffusion Generative Models based on Ideal Derivatives</option>
<option>DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents</option>
<option>Diffusion Probabilistic Models for 3D Point Cloud Generation</option>
<option>3D Shape Generation and Completion through Point-Voxel Diffusion</option>
<option>Diffusion Autoencoders: Toward a Meaningful and Decodable Representation</option>
<option>Realistic galaxy image simulation via score-based generative models</option>
<option>Probabilistic Mass Mapping with Neural Score Estimation</option>
<option>Probabilistic Mapping of Dark Matter by Neural Score Matching</option>
<option>Denoising Score-Matching for Uncertainty Quantification in Inverse Problems</option>
<option>Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models</option>
<option>Step-unrolled Denoising Autoencoders for Text Generation</option>
<option>RePaint: Inpainting using Denoising Diffusion Probabilistic Models</option>
<option>Robust Compressed Sensing MRI with Deep Generative Priors</option>
<option>Progressive Distillation for Fast Sampling of Diffusion Models</option>
<option>DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs</option>
<option>Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model</option>
<option>Denoising Diffusion Restoration Models</option>
<option>From data to functa: Your data point is a function and you can treat it like one</option>
<option>Simulating Diffusion Bridges with Score Matching</option>
<option>Rethinking the Role of Gradient-Based Attribution Methods for Model Interpretability</option>
<option>Riemannian Score-Based Generative Modelling</option>
<option>Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations</option>
<option>InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training</option>
<option>ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis</option>
<option>Conditional Diffusion Probabilistic Model for Speech Enhancement</option>
<option>Diffusion bridges vector quantized Variational AutoEncoders</option>
<option>GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation</option>
<option>Predicting Molecular Conformation via Dynamic Graph Score Matching</option>
<option>Learning Gradient Fields for Molecular Conformation Generation</option>
<option>Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality</option>
<option>Understanding DDPM Latent Codes Through Optimal Transport</option>
<option>ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models</option>
<option>Diffusion Causal Models for Counterfactual Estimation</option>
<option>Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders</option>
<option>Conditional Simulation Using Diffusion Schrödinger Bridges</option>
<option>Source Separation with Deep Generative Priors</option>
<option>Pseudo Numerical Methods for Diffusion Models on Manifolds</option>
<option>Score matching enables causal discovery of nonlinear additive noise models</option>
<option>Towards performant and reliable undersampled MR reconstruction via diffusion model sampling</option>
<option>Score-Based Generative Models for Molecule Generation</option>
<option>Dynamic Dual-Output Diffusion Models</option>
<option>Diffusion Models for Medical Anomaly Detection</option>
<option>Dual Diffusion Implicit Bridges for Image-to-Image Translation</option>
<option>Diffusion Probabilistic Modeling for Video Generation</option>
<option>Perception Prioritized Training of Diffusion Models</option>
<option>Equivariant Diffusion for Molecule Generation in 3D</option>
<option>Generating High Fidelity Data from Low-density Regions using Diffusion Models</option>
<option>SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping</option>
<option>Diffusion Models for Counterfactual Explanations</option>
<option>Denoising Likelihood Score Matching for Conditional Score-based Data Generation</option>
<option>BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis</option>
<option>Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders</option>
<option>MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion</option>
<option>Video Diffusion Models</option>
<option>KNN-Diffusion: Image Generation via Large-Scale Retrieval</option>
<option>Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain</option>
<option>Fast Sampling of Diffusion Models with Exponential Integrator</option>
<option>Diffusion Models for Adversarial Purification</option>
<option>On Conditioning the Input Noise for Controlled Image Generation with Diffusion Models</option>
<option>Subspace Diffusion Generative Models</option>
<option>Semi-Parametric Neural Image Synthesis</option>
<option>FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</option>
<option>DiffMD: A Geometric Diffusion Model for Molecular Dynamics Simulations</option>
<option>Accelerating Diffusion Models via Early Stop of the Diffusion Process</option>
<option>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</option>
<option>Flexible Diffusion Modeling of Long Videos</option>
<option>Planning with Diffusion for Flexible Behavior Synthesis</option>
<option>MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation</option>
<option>Solving Inverse Problems in Medical Imaging with Score-Based Generative Models</option>
<option>Discovering the Hidden Vocabulary of DALLE-2</option>
<option>Few-Shot Diffusion Models</option>
<option>Guided Diffusion Model for Adversarial Purification</option>
<option>Improving Diffusion Models for Inverse Problems using Manifold Constraints</option>
<option>Elucidating the Design Space of Diffusion-Based Generative Models</option>
<option>DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps</option>
<option>Score-Based Generative Models Detect Manifolds</option>
<option>On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models</option>
<option>Maximum Likelihood Training of Implicit Nonlinear Diffusion Models</option>
<option>Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data</option>
<option>Universal Speech Enhancement with Score-based Diffusion</option>
<option>Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models</option>
<option>Torsional Diffusion for Molecular Conformer Generation</option>
<option>Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models</option>
<option>Diffusion-GAN: Training GANs with Diffusion</option>
<option>Blended Latent Diffusion</option>
<option>Compositional Visual Generation with Composable Diffusion Models</option>
<option>Diffusion-LM Improves Controllable Text Generation</option>
<option>Neural Diffusion Processes</option>
<option>Accelerating Score-based Generative Models for High-Resolution Image Synthesis</option>
<option>Probability flow solution of the Fokker-Planck equation</option>
<option>Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem</option>
<option>Multi-instrument Music Synthesis with Spectrogram Diffusion</option>
<option>How Much is Enough? A Study on Diffusion Times in Score-based Generative Models</option>
<option>Image Generation with Multimodal Priors using Denoising Diffusion Probabilistic Models</option>
<option>Convergence for score-based generative modeling with polynomial complexity</option>
<option>Realistic Gramophone Noise Synthesis using a Diffusion Model</option>
<option>Diffusion Models for Video Prediction and Infilling</option>
<option>CARD: Classification and Regression Diffusion Models</option>
<option>Latent Diffusion Energy-Based Model for Interpretable Text Modeling</option>
<option>Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation</option>
<option>Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching</option>
<option>gDDIM: Generalized denoising diffusion implicit models</option>
<option>Lossy Compression with Gaussian Diffusion</option>
<option>SOS: Score-based Oversampling for Tabular Data</option>
<option>Faster Diffusion Cardiac MRI with Deep Learning-based breath hold reduction</option>
<option>A Flexible Diffusion Model</option>
<option>Diffusion models as plug-and-play priors</option>
<option>(Certified!!) Adversarial Robustness for Free!</option>
<option>Guided Diffusion Model for Adversarial Purification from Random Noise</option>
<option>DDPM-CD: Remote Sensing Change Detection using Denoising Diffusion Probabilistic Models</option>
<option>Score-based Generative Models for Calorimeter Shower Simulation</option>
<option>Diffusion Deformable Model for 4D Temporal Medical Image Generation</option>
<option>Generative Modelling With Inverse Heat Dissipation</option>
<option>SPI-GAN: Distilling Score-based Generative Models with Straight-Path Interpolations</option>
<option>Semantic Image Synthesis via Diffusion Models</option>
<option>Back to the Source: Diffusion-Driven Test-Time Adaptation</option>
<option>A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion</option>
<option>Riemannian Diffusion Schrödinger Bridge</option>
<option>Improving Diffusion Model Efficiency Through Patching</option>
<option>Wavelet Score-Based Generative Modeling</option>
<option>Adaptive Diffusion Priors for Accelerated MRI Reconstruction</option>
<option>DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps</option>
<option>Threat Model-Agnostic Adversarial Defense using Diffusion Models</option>
<option>Unsupervised Medical Image Translation with Adversarial Diffusion Models</option>
<option>DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras</option>
<option>Non-Uniform Diffusion Models</option>
<option>Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems</option>
<option>EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations</option>
<option>Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models</option>
<option>Classifier-Free Diffusion Guidance</option>
<option>Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis</option>
<option>Pyramidal Denoising Diffusion Probabilistic Models</option>
<option>Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning</option>
<option>Convergence of denoising diffusion models under the manifold hypothesis</option>
<option>Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning</option>
<option>Applying Regularized Schrödinger-Bridge-Based Stochastic Process in Generative Modeling</option>
<option>Your ViT is Secretly a Hybrid Discriminative-Generative Diffusion Model</option>
<option>Score-Based Diffusion meets Annealed Importance Sampling</option>
<option>Langevin Diffusion Variational Inference</option>
<option>Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance</option>
<option>Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models</option>
<option>Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise</option>
<option>Vector Quantized Diffusion Model with CodeUnet for Text-to-Sign Pose Sequences Generation</option>
<option>Diffusion Models Beat GANs on Topology Optimization</option>
<option>PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition</option>
<option>AT-DDPM: Restoring Faces degraded by Atmospheric Turbulence using Denoising Diffusion Probabilistic Models</option>
<option>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</option>
<option>Understanding Diffusion Models: A Unified Perspective</option>
<option>Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis</option>
<option>A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images</option>
<option>MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model</option>
<option>Let us Build Bridges: Understanding and Extending Diffusion Generative Models</option>
<option>Diffusion Models: A Comprehensive Survey of Methods and Applications</option>
<option>First Hitting Diffusion Models for Generating Manifold, Graph and Categorical Data</option>
<option>Unifying Generative Models with GFlowNets and Beyond</option>
<option>Diffusion-based Molecule Generation with Informative Prior Bridges</option>
<option>SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion</option>
<option>Instrument Separation of Symbolic Music by Explicitly Guided Diffusion Model</option>
<option>Diffusion Models in Vision: A Survey</option>
<option>Soft Diffusion: Score Matching for General Corruptions</option>
<option>Blurring Diffusion Models</option>
<option>PET image denoising based on denoising diffusion probabilistic models</option>
<option>Lossy Image Compression with Conditional Diffusion Models</option>
<option>Brain Imaging Generation with Latent Diffusion Models</option>
<option>T2V-DDPM: Thermal to Visible Face Translation using Denoising Diffusion Probabilistic Models</option>
<option>Deep Generalized Schrödinger Bridge</option>
<option>Metal Inpainting in CBCT Projections Using Score-based Generative Model</option>
<option>Poisson Flow Generative Models</option>
<option>Mandarin Singing Voice Synthesis with Denoising Diffusion Probabilistic Wasserstein GAN</option>
<option>MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation</option>
<option>Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions</option>
<option>Implementing and Experimenting with Diffusion Models for Text-to-Image Generation</option>
<option>Creative Painting with Latent Diffusion Models</option>
<option>Re-Imagen: Retrieval-Augmented Text-to-Image Generator</option>
<option>Diffusion Posterior Sampling for General Noisy Inverse Problems</option>
<option>Denoising MCMC for Accelerating Diffusion-Based Generative Models</option>
<option>Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation</option>
<option>Human Motion Diffusion Model</option>
<option>Make-A-Video: Text-to-Video Generation without Text-Video Data</option>
<option>Denoising Diffusion Probabilistic Models for Styled Walking Synthesis</option>
<option>Analyzing Diffusion as Serial Reproduction</option>
<option>DiGress: Discrete Denoising diffusion for graph generation</option>
<option>DreamFusion: Text-to-3D using 2D Diffusion</option>
<option>Convergence of score-based generative modeling for general data distributions</option>
<option>Learning to Learn with Generative Models of Neural Network Checkpoints</option>
<option>On Investigating the Conservative Property of Score-Based Generative Models</option>
<option>Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models</option>
<option>Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion</option>
<option>Denoising Diffusion Error Correction Codes</option>
<option>Spectral Diffusion Processes</option>
<option>Compositional Score Modeling for Simulation-based Inference</option>
<option>DiGress: Discrete Denoising diffusion for graph generation</option>
<option>Flow Matching for Generative Modeling</option>
<option>DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics</option>
<option>LDEdit: Towards Generalized Text Guided Image Manipulation via Latent Diffusion Models</option>
<option>Progressive Text-to-Image Generation</option>
<option>Imagen Video: High Definition Video Generation with Diffusion Models</option>
<option>clip2latent: Text driven sampling of a pre-trained StyleGAN using denoising diffusion and CLIP</option>
<option>DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking</option>
<option>OCD: Learning to Overfit with Conditional Diffusion Models</option>
<option>Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2</option>
<option>Improving Sample Quality of Diffusion Models Using Self-Attention Guidance</option>
<option>Diffusion-based Image Translation using Disentangled Style and Content Representation</option>
<option>TabDDPM: Modelling Tabular Data with Diffusion Models</option>
<option>Protein structure generation via folding diffusion</option>
<option>Diffusion Posterior Sampling for General Noisy Inverse Problems</option>
<option>Creative Painting with Latent Diffusion Models</option>
<option>Denoising Diffusion Probabilistic Models for Styled Walking Synthesis</option>
<option>Denoising MCMC for Accelerating Diffusion-Based Generative Models</option>
<option>Analyzing Diffusion as Serial Reproduction</option>
<option>What the DAAM: Interpreting Stable Diffusion Using Cross Attention</option>
<option>Novel View Synthesis with Diffusion Models</option>
<option>CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning</option>
<option>Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation</option>
<option>STaSy: Score-based Tabular data Synthesis</option>
<option>Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models</option>
<option>Red-Teaming the Stable Diffusion Safety Filter</option>
<option>Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance</option>
<option>Markup-to-Image Diffusion Models with Scheduled Sampling</option>
<option>GENIE: Higher-Order Denoising Diffusion Solvers</option>
<option>DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability</option>
<option>f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation</option>
<option>Self-Guided Diffusion Models</option>
<option>LION: Latent Point Diffusion Models for 3D Shape Generation</option>
<option>DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models</option>
<option>Action Matching: Learning Stochastic Dynamics from Samples</option>
<option>Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design</option>
<option>Diffusion Models for Causal Discovery via Topological Ordering</option>
<option>Hierarchical Diffusion Models for Singing Voice Neural Vocoder</option>
<option>TransFusion: Transcribing Speech with Multinomial Diffusion</option>
<option>Efficient Diffusion Models for Vision: A Survey</option>
<option>Imagic: Text-Based Real Image Editing with Diffusion Models</option>
<option>DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models</option>
<option>DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models</option>
<option>Improving Adversarial Robustness by Contrastive Guided Diffusion Process</option>
<option>Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models</option>
<option>Differentially Private Diffusion Models</option>
<option>Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation</option>
<option>UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image</option>
<option>Representation Learning with Diffusion Models</option>
<option>DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation</option>
<option>On Distillation of Guided Diffusion Models</option>
<option>Representation Learning with Diffusion Models</option>
<option>Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models</option>
<option>Diffusion Visual Counterfactual Explanations</option>
<option>Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report</option>
<option>Graphically Structured Diffusion Models</option>
<option>Boomerang: Local sampling on image manifolds using diffusion models</option>
<option>High-Resolution Image Editing via Multi-Stage Blended Diffusion</option>
<option>Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model</option>
<option>Deep Equilibrium Approaches to Diffusion Models</option>
<option>Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models</option>
<option>MARS: Meta-Learning as Score Matching in the Function Space</option>
<option>Conditional Diffusion with Less Explicit Guidance via Model Predictive Control</option>
<option>On the failure of variational score matching for VAE models</option>
<option>Structure-based Drug Design with Equivariant Diffusion Models</option>
<option>Denoising Likelihood Score Matching for Conditional Score-based Data Generation</option>
<option>DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models</option>
<option>Towards the Detection of Diffusion Model Deepfakes</option>
<option>Full-band General Audio Synthesis with Score-based Diffusion</option>
<option>Categorical SDEs with Simplex Diffusion</option>
<option>Statistical Efficiency of Score Matching: The View from Isoperimetry</option>
<option>Solving Audio Inverse Problems with a Diffusion Model</option>
<option>Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks</option>
<option>Denoising MCMC for Accelerating Diffusion-Based Generative Models</option>
<option>Conditioning and Sampling in Variational Diffusion Models for Speech Super-Resolution</option>
<option>Diffusion models for missing value imputation in tabular data</option>
<option>Guided Conditional Diffusion for Controllable Traffic Simulation</option>
<option>Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling for Medical Image Segmentation</option>
<option>DiffusER: Discrete Diffusion via Edit-based Reconstruction</option>
<option>SDMuse: Stochastic Differential Music Editing and Generation via Hybrid Representation</option>
<option>MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model</option>
<option>DOLPH: Diffusion Models for Phase Retrieval</option>
<option>Accelerated Motion Correction for MRI using Score-Based Generative Models</option>
<option>An optimal control perspective on diffusion-based generative modeling</option>
<option>On the detection of synthetic images generated by diffusion models</option>
<option>Generation of Anonymous Chest Radiographs Using Latent Diffusion Models for Training Thoracic Abnormality Classification Systems</option>
<option>DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models</option>
<option>Entropic Neural Optimal Transport via Diffusion Processes</option>
<option>Concrete Score Matching: Generalized Score Matching for Discrete Data</option>
<option>Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures</option>
<option>DriftRec: Adapting diffusion models to blind image restoration tasks</option>
<option>A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces</option>
<option>HumanDiffusion: a Coarse-to-Fine Alignment Diffusion Framework for Controllable Text-Driven Person Image Generation</option>
<option>What the DAAM: Interpreting Stable Diffusion Using Cross Attention</option>
<option>StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects</option>
<option>SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control</option>
<option>Unsupervised vocal dereverberation with diffusion-based generative models</option>
<option>DiffPhase: Generative Diffusion-based STFT Phase Retrieval</option>
<option>Self-conditioned Embedding Diffusion for Text Generation</option>
<option>Medical Diffusion: Denoising Diffusion Probabilistic Models for 3D Medical Image Generation</option>
<option>Cold Diffusion for Speech Enhancement</option>
<option>Analysing Diffusion-based Generative Approaches versus Discriminative Approaches for Speech Restoration</option>
<option>Modeling Temporal Data as Continuous Functions with Stochastic Process Diffusion</option>
<option>Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis</option>
<option>Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models</option>
<option>Evaluating a Synthetic Image Dataset Generated with Stable Diffusion</option>
<option>Convergence in KL Divergence of the Inexact Langevin Algorithm with Application to Score-based Generative Models</option>
<option>Spot the fake lungs: Generating Synthetic Medical Images using Neural Diffusion Models</option>
<option>eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</option>
<option>Versatile Diffusion: Text, Images and Variations All in One Diffusion Model</option>
<option>DeS3: Attention-driven Self and Soft Shadow Removal using ViT Similarity and Color Convergence</option>
<option>Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models</option>
<option>Diffusion Models for Medical Image Analysis: A Comprehensive Survey</option>
<option>Fast Graph Generation via Spectral Diffusion</option>
<option>Arbitrary Style Guidance for Enhanced Diffusion-Based Text-to-Image Generation</option>
<option>Extreme Generative Image Compression by Learning Text Embedding from Diffusion Models</option>
<option>Denoising diffusion models for out-of-distribution detection</option>
<option>CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming</option>
<option>Conffusion: Confidence Intervals for Diffusion Models</option>
<option>Null-text Inversion for Editing Real Images using Guided Diffusion Models</option>
<option>DiffusionDet: Diffusion Model for Object Detection</option>
<option>Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models</option>
<option>EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance</option>
<option>Grad-StyleSpeech: Any-speaker Adaptive Text-to-Speech Synthesis with Diffusion Models</option>
<option>Invariant Learning via Diffusion Dreamed Distribution Shifts</option>
<option>Patch-Based Denoising Diffusion Probabilistic Model for Sparse-View CT Reconstruction</option>
<option>RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation</option>
<option>A Structure-Guided Diffusion Model for Large-Hole Diverse Image Completion</option>
<option>Magic3D: High-Resolution Text-to-3D Content Creation</option>
<option>From Denoising Diffusions to Denoising Markov Models</option>
<option>Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection</option>
<option>TIER-A: Denoising Learning Framework for Information Extraction</option>
<option>NVDiff: Graph Generation through the Diffusion of Node Vectors</option>
<option>Parallel Diffusion Models of Operator and Image for Blind Inverse Problems</option>
<option>IC3D: Image-Conditioned 3D Diffusion for Shape Generation</option>
<option>MagicVideo: Efficient Video Generation With Latent Diffusion Models</option>
<option>Diffusion-Based Scene Graph to Image Generation with Masked Contrastive Pre-Training</option>
<option>VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models</option>
<option>SinFusion: Training Diffusion Models on a Single Image or Video</option>
<option>Person Image Synthesis via Denoising Diffusion Model</option>
<option>EDICT: Exact Diffusion Inversion via Coupled Transformations</option>
<option>SinDiffusion: Learning a Diffusion Model from a Single Natural Image</option>
<option>DiffDreamer: Towards Consistent Unsupervised Single-view Scene Extrapolation with Conditional Diffusion Models</option>
<option>Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems</option>
<option>Can denoising diffusion probabilistic models generate realistic astrophysical fields?</option>
<option>DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction</option>
<option>Paint by Example: Exemplar-based Image Editing with Diffusion Models</option>
<option>Tetrahedral Diffusion Models for 3D Shape Generation</option>
<option>Inversion-Based Style Transfer with Diffusion Models</option>
<option>Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</option>
<option>BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction</option>
<option>3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models</option>
<option>Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions</option>
<option>HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising</option>
<option>Fast Sampling of Diffusion Models via Operator Learning</option>
<option>Improving dermatology classifiers across populations using images generated by large diffusion models</option>
<option>Sketch-Guided Text-to-Image Diffusion Models</option>
<option>Latent Space Diffusion Models of Cryo-EM Structures</option>
<option>Investigating Prompt Engineering in Diffusion Models</option>
<option>Shifted Diffusion for Text-to-image Generation</option>
<option>Traditional Classification Neural Networks are Good Generators: They are Competitive with DDPMs and GANs</option>
<option>Continuous diffusion for categorical data</option>
<option>DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models</option>
<option>Wavelet Diffusion Models are fast and scalable Image Generators</option>
<option>Post-training Quantization on Diffusion Models</option>
<option>Dimensionality-Varying Diffusion Process</option>
<option>Diffusion Probabilistic Model Made Slim</option>
<option>Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models</option>
<option>Multiresolution Textual Inversion</option>
<option>High-Fidelity Guided Image Synthesis with Latent Diffusion Models</option>
<option>DiffPose: Toward More Reliable 3D Pose Estimation</option>
<option>3D Neural Field Generation using Triplane Diffusion</option>
<option>SinDDM: A Single Image Denoising Diffusion Model</option>
<option>Score-based Continuous-time Discrete Diffusion Models</option>
<option>SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction</option>
<option>Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation</option>
<option>VIDM: Video Implicit Diffusion Models</option>
<option>Shape-Guided Diffusion with Inside-Outside Attention</option>
<option>Denoising Diffusion for Sampling SAT Solutions</option>
<option>DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models</option>
<option>Hierarchical Text-Conditional Image Generation with CLIP Latents</option>
<option>One Sample Diffusion Model in Projection Domain for Low-Dose CT Imaging</option>
<option>Diffusion-SDF: Text-to-Shape via Voxelized Diffusion</option>
<option>Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models</option>
<option>NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors</option>
<option>Proposal of a Score Based Approach to Sampling Using Monte Carlo Estimation of Score and Oracle Access to Target Density</option>
<option>Diffusion Guided Domain Adaptation of Image Generators</option>
<option>Executing your Commands via Motion Diffusion in Latent Space</option>
<option>Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors</option>
<option>SINE: SINgle Image Editing with Text-to-Image Diffusion Models</option>
<option>MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis</option>
<option>ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal</option>
<option>Diff-Font: Diffusion Model for Robust One-Shot Font Generation</option>
<option>DiffAlign : Few-shot learning using diffusion based synthesis and alignment</option>
<option>How to Backdoor Diffusion Models?</option>
<option>The Stable Artist: Steering Semantics in Diffusion Latent Space</option>
<option>DifFace: Blind Face Restoration with Diffused Error Contraction</option>
<option>HS-Diffusion: Learning a Semantic-Mixing Diffusion Model for Head Swapping</option>
<option>Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance</option>
<option>Diffusion Probabilistic Models beat GANs on Medical Images</option>
<option>Point-E: A System for Generating 3D Point Clouds from Complex Prompts</option>
<option>Difformer: Empowering Diffusion Models on the Embedding Space for Text Generation</option>
<option>Latent Diffusion for Language Generation</option>
<option>Scalable Diffusion Models with Transformers</option>
<option>Diffusing Surrogate Dreams of Video Scenes to Predict Video Memorability</option>
<option>Character-Aware Models Improve Visual Text Rendering</option>
<option>SPIRiT-Diffusion: SPIRiT-driven Score-Based Generative Modeling for Vessel Wall imaging</option>
<option>Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise</option>
<option>StoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation</option>
<option>Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models</option>
<option>Your diffusion model secretly knows the dimension of the data manifold</option>
<option>Exploring Vision Transformers as Diffusion Learners</option>
<option>DiffFace: Diffusion-based Face Swapping with Facial Guidance</option>
<option>ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech</option>
<option>Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models</option>
<option>Exploring Transformer Backbones for Image Diffusion Models</option>
<option>Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data</option>
<option>Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification</option>
<option>Conditional Diffusion Based on Discrete Graph Structures for Molecular Graph Generation</option>
<option>Speed up the inference of diffusion models via shortcut MCMC sampling</option>
<option>Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --</option>
<option>Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement</option>
<option>Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction</option>
<option>Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation</option>
<option>Scalable Adaptive Computation for Iterative Generation</option>
<option>DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation</option>
<option>Input Perturbation Reduces Exposure Bias in Diffusion Models</option>
<option>Long Horizon Temperature Scaling</option>
<option>How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control</option>
<option>Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models</option>
<option>Noise2Music: Text-conditioned Music Generation with Diffusion Models</option>
<option>GraphGUIDE: interpretable and controllable conditional graph generation with discrete Bernoulli diffusion</option>
<option>Information-Theoretic Diffusion</option>
<option>PFGM++: Unlocking the Potential of Physics-Inspired Generative Models</option>
<option>Riemannian Flow Matching on General Geometries</option>
<option>Graph Generation with Destination-Predicting Diffusion Mixture</option>
<option>Multi-Source Diffusion Models for Simultaneous Music Generation and Separation</option>
<option>Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation</option>
<option>Eliminating Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion</option>
<option>Structure and Content-Guided Video Synthesis with Diffusion Models</option>
<option>SE(3) diffusion model with application to protein backbone generation</option>
<option>Generative Diffusion Models on Graphs: Methods and Applications</option>
<option>DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models</option>
<option>Divide and Compose with Score Based Generative Models</option>
<option>Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation</option>
<option>ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval</option>
<option>ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories</option>
<option>Diffusion Model for Generative Image Denoising</option>
<option>Mixture of Diffusers for scene composition and high resolution image generation</option>
<option>Learning End-to-End Channel Coding with Diffusion Models</option>
<option>AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners</option>
<option>MorDIFF: Recognition Vulnerability and Attack Detectability of Face Morphing Attacks Created by Diffusion Autoencoders</option>
<option>Learning End-to-End Channel Coding with Diffusion Models</option>
<option>A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models</option>
<option>Are Diffusion Models Vulnerable to Membership Inference Attacks?</option>
<option>Dreamix: Video Diffusion Models are General Video Editors</option>
<option>Learning Data Representations with Joint Diffusion Models</option>
<option>Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics</option>
<option>Building Normalizing Flows with Stochastic Interpolants</option>
<option>Stable Target Field for Reduced Variance Score Estimation in Diffusion Models</option>
<option>Conditional Flow Matching: Simulation-Free Dynamic Optimal Transport</option>
<option>Diffusion Models for High-Resolution Solar Forecasts</option>
<option>Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</option>
<option>Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion</option>
<option>ArchiSound: Audio Generation with Diffusion</option>
<option>Optimizing DDPM Sampling with Shortcut Fine-Tuning</option>
<option>DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models</option>
<option>Transport with Support: Data-Conditional Diffusion Bridges</option>
<option>Salient Conditional Diffusion for Defending Against Backdoor Attacks</option>
<option>DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models</option>
<option>AudioLDM: Text-to-Audio Generation with Latent Diffusion Models</option>
<option>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</option>
<option>Minimizing Trajectory Curvature of ODE-based Generative Models</option>
<option>GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration</option>
<option>ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models</option>
<option>Extracting Training Data from Diffusion Models</option>
<option>SEGA: Instructing Diffusion using Semantic Dimensions</option>
<option>A Denoising Diffusion Model for Fluid Field Prediction</option>
<option>Diffusion Denoising for Low-Dose-CT Model</option>
<option>Image Restoration with Mean-Reverting Stochastic Differential Equations</option>
<option>MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer</option>
<option>Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization</option>
<option>3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models</option>
<option>Accelerating Guided Diffusion Sampling with Splitting Numerical Methods</option>
<option>PLay: Parametrically Conditioned Layout Generation using Latent Diffusion</option>
<option>Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion</option>
<option>simple diffusion: End-to-end diffusion for high resolution images</option>
<option>On the Importance of Noise Scheduling for Diffusion Models</option>
<option>On the Mathematics of Diffusion Models</option>
<option>Imitating Human Behaviour with Diffusion Models</option>
<option>Score Matching via Differentiable Physics</option>
<option>Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation</option>
<option>Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --</option>
<option>DiffSDS: A language diffusion model for protein backbone inpainting under geometric conditions and constraints</option>
<option>Membership Inference of Diffusion Models</option>
<option>DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model</option>
<option>Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets</option>
<option>Bipartite Graph Diffusion Model for Human Interaction Generation</option>
<option>DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion</option>
<option>DiffusionCT: Latent Diffusion Model for CT Image Standardization</option>
<option>RainDiffusion: When Unsupervised Learning Meets Diffusion Models for Real-world Image Deraining</option>
<option>Regular Time-series Generation using SGM</option>
<option>Mathematical analysis of singularities in the diffusion model under the submanifold assumption</option>
<option>Diffusion-based Conditional ECG Generation with Structured State Space Models</option>
<option>Fast Inference in Denoising Diffusion Models via MMD Finetuning</option>
<option>Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models</option>
<option>Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks</option>
<option>Diffusion-based Generation, Optimization, and Planning in 3D Scenes</option>
<option>A Residual Diffusion Model for High Perceptual Quality Codec Augmentation</option>
<option>Thompson Sampling with Diffusion Generative Prior</option>
<option>Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images</option>
<option>Guiding Text-to-Image Diffusion Model Towards Grounded Generation</option>
<option>Leveraging Diffusion For Strong and High Quality Face Morphing Attacks</option>
<option>Speech Driven Video Editing via an Audio-Conditioned Diffusion Model</option>
<option>Example-Based Sampling with Diffusion Models</option>
<option>Removing Structured Noise with Diffusion Models</option>
<option>Star-Shaped Denoising Diffusion Probabilistic Models</option>
<option>Q-Diffusion: Quantizing Diffusion Models</option>
<option>Better Diffusion Models Further Improve Adversarial Training</option>
<option>Geometry-Complete Diffusion for 3D Molecule Generation</option>
<option>MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model</option>
<option>UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models</option>
<option>Geometry of Score Based Generative Models</option>
<option>Preconditioned Score-based Generative Models</option>
<option>Single Motion Diffusion</option>
<option>I$^2$SB: Image-to-Image Schrödinger Bridge</option>
<option>Raising the Cost of Malicious AI-Powered Image Editing</option>
<option>Raising the Cost of Malicious AI-Powered Image Editing</option>
<option>Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild</option>
<option>Video Probabilistic Diffusion Models in Projected Latent Space</option>
<option>Score-based Diffusion Models in Function Space</option>
<option>Effective Data Augmentation With Diffusion Models</option>
<option>Text-driven Visual Synthesis with Latent Diffusion Prior</option>
<option>T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models</option>
<option>Explicit Diffusion of Gaussian Mixture Model Based Image Priors</option>
<option>MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation</option>
<option>PRedItOR: Text Guided Image Editing with Diffusion Prior</option>
<option>LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation</option>
<option>Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent</option>
<option>NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion</option>
<option>Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension</option>
<option>DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises</option>
<option>Infinite-Dimensional Diffusion Models for Function Spaces</option>
<option>Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness</option>
<option>$PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction</option>
<option>Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels</option>
<option>On Calibrating Diffusion Probabilistic Models</option>
<option>Diffusion Probabilistic Models for Graph-Structured Prediction</option>
<option>Learning Gradually Non-convex Image Priors Using Score Matching</option>
<option>Aligned Diffusion Schrödinger Bridges</option>
<option>Diffusion Models in Bioinformatics: A New Wave of Deep Learning Revolution in Action</option>
<option>To the Noise and Back: Diffusion for Shared Autonomy</option>
<option>DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models</option>
<option>Modeling Molecular Structures with Intrinsic Diffusion Models</option>
<option>Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition</option>
<option>Differentially Private Diffusion Models Generate Useful Synthetic Images</option>
<option>CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution</option>
<option>Denoising Diffusion Samplers</option>
<option>Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech</option>
<option>Diffusion Model-Augmented Behavioral Cloning</option>
<option>Denoising diffusion algorithm for inverse design of microstructures with fine-tuned nonlinear material properties</option>
<option>Directed Diffusion: Direct Control of Object Placement through Attention Guidance</option>
<option>Monocular Depth Estimation using Diffusion Models</option>
<option>Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?</option>
<option>Towards Enhanced Controllability of Diffusion Models</option>
<option>Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement</option>
<option>Unlimited-Size Diffusion Restoration</option>
<option>Collage Diffusion</option>
<option>Diffusion Probabilistic Fields</option>
<option>Adding Conditional Control to Text-to-Image Diffusion Models</option>
<option>Human Motion Diffusion as a Generative Prior</option>
<option>Consistency Models</option>
<option>Understanding the Diffusion Objective as a Weighted Integral of ELBOs</option>
<option>Continuous-Time Functional Diffusion Processes</option>
<option>Defending against Adversarial Audio via Diffusion Model</option>
<option>Deep Momentum Multi-Marginal Schrödinger Bridge</option>
<option>Diffusion Models are Minimax Optimal Distribution Estimators</option>
<option>Generative Diffusions in Augmented Spaces: A Complete Recipe</option>
<option>Unleashing Text-to-Image Diffusion Models for Visual Perception</option>
<option>Generative Diffusions in Augmented Spaces: A Complete Recipe</option>
<option>Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later</option>
<option>Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers</option>
<option>Patched Diffusion Models for Unsupervised Anomaly Detection in Brain MRI</option>
<option>3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction</option>
<option>Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation</option>
<option>Learning Enhancement From Degradation: A Diffusion Model For Fundus Image Enhancement</option>
<option>Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition</option>
<option>TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation</option>
<option>Diffusing Gaussian Mixtures for Generating Categorical Data</option>
<option>PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics</option>
<option>Restoration based Generative Models</option>
<option>StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space</option>
<option>DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation</option>
<option>MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation</option>
<option>Brain-Diffuser: Natural scene reconstruction from fMRI signals using generative latent diffusion</option>
<option>Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation</option>
<option>EEG Synthetic Data Generation Using Probabilistic Diffusion Models</option>
<option>EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models</option>
<option>Generalized Diffusion MRI Denoising and Super-Resolution using Swin Transformers</option>
<option>Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition</option>
<option>TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets</option>
<option>Score-Based Generative Models for Medical Image Segmentation using Signed Distance Functions</option>
<option>DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration</option>
<option>One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</option>
<option>PARASOL: Parametric Style Control for Diffusion Image Synthesis</option>
<option>DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration</option>
<option>Synthesizing Realistic Image Restoration Training Pairs: A Diffusion Approach</option>
<option>Erasing Concepts from Diffusion Models</option>
<option>DiffuseRoll: Multi-track multi-category music generation based on diffusion model</option>
<option>Generalised Scale-Space Properties for Probabilistic Diffusion Models</option>
<option>Point Cloud Diffusion Models for Automatic Implant Generation</option>
<option>MeshDiffusion: Score-based Generative 3D Mesh Modeling</option>
<option>Text-to-image Diffusion Models in Generative AI: A Survey</option>
<option>LayoutDM: Discrete Diffusion Model for Controllable Layout Generation</option>
<option>DiffusionAD: Denoising Diffusion for Anomaly Detection</option>
<option>ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution</option>
<option>DiffBEV: Conditional Diffusion Model for Bird's Eye View Perception</option>
<option>VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation</option>
<option>Diffusion Models for Contrast Harmonization of Magnetic Resonance Images</option>
<option>Stochastic Interpolants: A Unifying Framework for Flows and Diffusions</option>
<option>Generating symbolic music using diffusion models</option>
<option>Speech Signal Improvement Using Causal Generative Diffusion Models</option>
<option>Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels</option>
<option>Stochastic Segmentation with Conditional Categorical Diffusion Models</option>
<option>Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation</option>
<option>DIRE for Diffusion-Generated Image Detection</option>
<option>DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars</option>
<option>DiffIR: Efficient Diffusion Model for Image Restoration</option>
<option>Diffusion-HPC: Generating Synthetic Images with Realistic Humans</option>
<option>Efficient Diffusion Training via Min-SNR Weighting Strategy</option>
<option>A Recipe for Watermarking Diffusion Models</option>
<option>DiffusionRet: Generative Text-Video Retrieval with Diffusion Model</option>
<option>FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model</option>
<option>DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery</option>
<option>SUD$^2$: Supervision by Denoising Diffusion Models for Image Reconstruction</option>
<option>DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion</option>
<option>Denoising Diffusion Post-Processing for Low-Light Image Enhancement</option>
<option>Diffusing the Optimal Topology: A Generative Optimization Approach</option>
<option>Denoising Diffusion Autoencoders are Unified Self-supervised Learners</option>
<option>Diff-UNet: A Diffusion Embedded Network for Volumetric Segmentation</option>
<option>Cascaded Latent Diffusion Models for High-Resolution Chest X-ray Synthesis</option>
<option>DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification</option>
<option>Object-Centric Slot Diffusion</option>
<option>Leapfrog Diffusion Model for Stochastic Trajectory Prediction</option>
<option>Positional Diffusion: Ordering Unordered Sets with Diffusion Probabilistic Models</option>
<option>AnimeDiffusion: Anime Face Line Drawing Colorization via Diffusion Models</option>
<option>SVDiff: Compact Parameter Space for Diffusion Fine-Tuning</option>
<option>CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion</option>
<option>Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</option>
<option>LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models</option>
<option>Text2Tex: Text-driven Texture Synthesis via Diffusion Models</option>
<option>Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration</option>
<option>NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models</option>
<option>Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading</option>
<option>NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation</option>
<option>EDGI: Equivariant Diffusion for Planning with Embodied Agents</option>
<option>Compositional 3D Scene Generation using Locally Conditioned Diffusion</option>
<option>Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation</option>
<option>LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation</option>
<option>Affordance Diffusion: Synthesizing Hand-Object Interactions</option>
<option>Pix2Video: Video Editing using Image Diffusion</option>
<option>Diffuse-Denoise-Count: Accurate Crowd-Counting with Diffusion Models</option>
<option>Ablating Concepts in Text-to-Image Diffusion Models</option>
<option>Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</option>
<option>Medical diffusion on a budget: textual inversion for medical image generation</option>
<option>DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video</option>
<option>MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models</option>
<option>A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI</option>
<option>Sub-volume-based Denoising Diffusion Probabilistic Model for Cone-beam CT Reconstruction from Incomplete Data</option>
<option>MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion</option>
<option>Conditional Image-to-Video Generation with Latent Flow Diffusion Models</option>
<option>End-to-End Diffusion Latent Optimization Improves Classifier Guidance</option>
<option>CoLa-Diff: Conditional Latent Diffusion Model for Multi-Modal MRI Synthesis</option>
<option>Enhancing Unsupervised Speech Recognition with Diffusion GANs</option>
<option>DisC-Diff: Disentangled Conditional Diffusion Model for Multi-Contrast MRI Super-Resolution</option>
<option>Anti-DreamBooth: Protecting users from personalized text-to-image synthesis</option>
<option>Training-free Style Transfer Emerges from h-space in Diffusion models</option>
<option>Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation</option>
<option>Seer: Language Instructed Video Prediction with Latent Diffusion Models</option>
<option>DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion</option>
<option>GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents</option>
<option>DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis</option>
<option>Exploring Continual Learning of Diffusion Models</option>
<option>Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection</option>
<option>DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency</option>
<option>Text-to-Image Diffusion Models are Zero-Shot Classifiers</option>
<option>Conditional Score-Based Reconstructions for Multi-contrast MRI</option>
<option>Visual Chain-of-Thought Diffusion Models</option>
<option>DDMM-Synth: A Denoising Diffusion Model for Cross-modal Medical Image Synthesis with Sparse-view Measurement Embedding</option>
<option>Your Diffusion Model is Secretly a Zero-Shot Classifier</option>
<option>DiffULD: Diffusive Universal Lesion Detection</option>
<option>Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos</option>
<option>4D Facial Expression Diffusion Model</option>
<option>WordStylist: Styled Verbatim Handwritten Text Generation with Latent Diffusion Models</option>
<option>Diffusion Schrödinger Bridge Matching</option>
<option>Token Merging for Fast Stable Diffusion</option>
<option>DDP: Diffusion Model for Dense Visual Prediction</option>
<option>LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation</option>
<option>Discriminative Class Tokens for Text-to-Image Diffusion Models</option>
<option>DiffCollage: Parallel Generation of Large Content with Diffusion Models</option>
<option>HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion</option>
<option>Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models</option>
<option>A Closer Look at Parameter-Efficient Tuning in Diffusion Models</option>
<option>Diffusion Action Segmentation</option>
<option>$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States</option>
<option>PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models</option>
<option>ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model</option>
<option>DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models</option>
<option>Textile Pattern Generation Using Diffusion Models</option>
<option>Diffusion Bridge Mixture Transports, Schrödinger Bridge Problems and Generative Modeling</option>
<option>Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion</option>
<option>A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material</option>
<option>Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds</option>
<option>EigenFold: Generative Protein Structure Prediction with Diffusion Models</option>
<option>Generative Novel View Synthesis with 3D-Aware Diffusion Models</option>
<option>A Diffusion-based Method for Multi-turn Compositional Image Generation</option>
<option>GenPhys: From Physical Processes to Generative Models</option>
<option>Diffusion Models as Masked Autoencoders</option>
<option>DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model</option>
<option>Anomaly Detection via Gumbel Noise Score Matching</option>
<option>Zero-shot Medical Image Translation via Frequency-Guided Diffusion Models</option>
<option>Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models</option>
<option>Zero-shot CT Field-of-view Completion with Unconditional Generative Diffusion Prior</option>
<option>Ambiguous Medical Image Segmentation using Diffusion Models</option>
<option>BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation</option>
<option>ChiroDiff: Modelling chirographic data with Diffusion Models</option>
<option>Reflected Diffusion Models</option>
<option>Generative modeling for time series via Schr{ö}dinger bridge</option>
<option>Binary Latent Diffusion</option>
<option>DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion</option>
<option>Diffusion Models for Constrained Domains</option>
<option>Mask-conditioned latent diffusion for generating gastrointestinal polyp images</option>
<option>Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA</option>
<option>DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</option>
<option>SpectralDiff: Hyperspectral Image Classification with Spectral-Spatial Diffusion Models</option>
<option>InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions</option>
<option>CamDiff: Camouflage Image Augmentation via Diffusion Model</option>
<option>Diffusion models with location-scale noise</option>
<option>Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction</option>
<option>DiffusionRig: Learning Personalized Priors for Facial Appearance Editing</option>
<option>Learning Controllable 3D Diffusion Models from Single-view Images</option>
<option>DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning</option>
<option>An Edit Friendly DDPM Noise Space: Inversion and Manipulations</option>
<option>$E(3) \times SO(3)$-Equivariant Networks for Spherical Deconvolution in Diffusion MRI</option>
<option>Towards Controllable Diffusion Models via Reward-Guided Exploration</option>
<option>Memory Efficient Diffusion Probabilistic Models via Patch-based Generation</option>
<option>Delta Denoising Score</option>
<option>Soundini: Sound-Guided Diffusion for Natural Video Editing</option>
<option>Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models</option>
<option>Synthetic Data from Diffusion Models Improves ImageNet Classification</option>
<option>UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer</option>
<option>Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</option>
<option>Two-stage Denoising Diffusion Model for Source Localization in Graph Inverse Problems</option>
<option>NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models</option>
<option>Reference-based Image Composition with Sketch via Structure-aware Diffusion Model</option>
<option>DiFaReli: Diffusion Face Relighting</option>
<option>Denoising Diffusion Medical Models</option>
<option>A data augmentation perspective on diffusion models and retrieval</option>
<option>Collaborative Diffusion for Multi-Modal Face Generation and Editing</option>
<option>BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion Synthesis</option>
<option>Improved Diffusion-based Image Colorization via Piggybacked Models</option>
<option>Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models</option>
<option>Persistently Trained, Diffusion-assisted Energy-based Models</option>
<option>SILVR: Guided Diffusion for Molecule Generation</option>
<option>Score-Based Diffusion Models as Principled Priors for Inverse Imaging</option>
<option>Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation</option>
<option>Lookahead Diffusion Probabilistic Models for Refining Mean Estimation</option>
<option>Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations</option>
<option>On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation</option>
<option>Variational Diffusion Auto-encoder: Latent Space Extraction from Pre-trained Diffusion Models</option>
<option>DiffVoice: Text-to-Speech with Latent Diffusion</option>
<option>Score-Based Diffusion Models as Principled Priors for Inverse Imaging</option>
<option>Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification</option>
<option>GlyphDiffusion: Text Generation as Image Generation</option>
<option>Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models</option>
<option>Diffusion Probabilistic Model Based Accurate and High-Degree-of-Freedom Metasurface Inverse Design</option>
<option>Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation</option>
<option>Single-View Height Estimation with Conditional Diffusion Probabilistic Models</option>
<option>DiffuseExpand: Expanding dataset for 2D medical image segmentation using diffusion models</option>
<option>Motion-Conditioned Diffusion Model for Controllable Video Synthesis</option>
<option>Class-Balancing Diffusion Models</option>
<option>Cycle-guided Denoising Diffusion Probability Model for 3D Cross-modality MRI Synthesis</option>
<option>Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data</option>
<option>In-Context Learning Unlocked for Diffusion Models</option>
<option>Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator</option>
<option>LayoutDM: Transformer-based Diffusion Model for Layout Generation</option>
<option>Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion</option>
<option>Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition</option>
<option>Controllable Light Diffusion for Portraits</option>
<option>ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation</option>
<option>Real-World Denoising via Diffusion Model</option>
<option>DocDiff: Document Enhancement via Residual Diffusion Models</option>
<option>A Variational Perspective on Solving Inverse Problems with Diffusion Models</option>
<option>Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling</option>
<option>Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs</option>
<option>A Latent Diffusion Model for Protein Structure Generation</option>
<option>Diffusion-based Signal Refiner for Speech Separation</option>
<option>Relightify: Relightable 3D Faces from a Single Image via Diffusion Models</option>
<option>Analyzing Bias in Diffusion-based Face Generation Models</option>
<option>Exploiting Diffusion Prior for Real-World Image Super-Resolution</option>
<option>Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images</option>
<option>MolDiff: Addressing the Atom-Bond Inconsistency Problem in 3D Molecule Diffusion Generation</option>
<option>Provably Convergent Schrödinger Bridge with Applications to Probabilistic Time Series Imputation</option>
<option>TESS: Text-to-Text Self-Conditioned Simplex Diffusion</option>
<option>Meta-DM: Applications of Diffusion Models on Few-Shot Learning</option>
<option>On enhancing the robustness of Vision Transformers: Defensive Diffusion</option>
<option>A Reproducible Extraction of Training Images from Diffusion Models</option>
<option>Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models</option>
<option>Denoising Diffusion Models for Plug-and-Play Image Restoration</option>
<option>Common Diffusion Noise Schedules and Sample Steps are Flawed</option>
<option>Expressiveness Remarks for Denoising Diffusion Models and Samplers</option>
<option>A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction</option>
<option>Discrete Diffusion Probabilistic Models for Symbolic Music Generation</option>
<option>A score-based operator Newton method for measure transport</option>
<option>Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?</option>
<option>Raising the Bar for Certified Adversarial Robustness with Diffusion Models</option>
<option>AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation</option>
<option>UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</option>
<option>Unsupervised Pansharpening via Low-rank Diffusion Model</option>
<option>TextDiffuser: Diffusion Models as Text Painters</option>
<option>LDM3D: Latent Diffusion Model for 3D</option>
<option>DiffUTE: Universal Text Editing Diffusion Model</option>
<option>Discriminative Diffusion Models as Few-shot Vision and Language Learners</option>
<option>Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models</option>
<option>Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces</option>
<option>Structural Pruning for Diffusion Models</option>
<option>Democratized Diffusion Language Model</option>
<option>Sampling, Diffusions, and Stochastic Localization</option>
<option>Diffusion-Based Mel-Spectrogram Enhancement for Personalized Speech Synthesis with Found Data</option>
<option>Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders</option>
<option>Any-to-Any Generation via Composable Diffusion</option>
<option>Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots</option>
<option>SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models</option>
<option>A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model</option>
<option>Hierarchical Integration Diffusion Model for Realistic Image Deblurring</option>
<option>Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?</option>
<option>Towards Globally Consistent Stochastic Human Motion Prediction via Motion Diffusion</option>
<option>DiffUCD:Unsupervised Hyperspectral Image Change Detection with Semantic Correlation Diffusion Model</option>
<option>GSURE-Based Diffusion Model Training with Corrupted Data</option>
<option>AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation</option>
<option>Dual-Diffusion: Dual Conditional Denoising Diffusion Probabilistic Models for Blind Super-Resolution Reconstruction in RSIs</option>
<option>Training Diffusion Models with Reinforcement Learning</option>
<option>ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer</option>
<option>DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models</option>
<option>SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models</option>
<option>Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence</option>
<option>Realistic Noise Synthesis with Diffusion Models</option>
<option>Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models</option>
<option>Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models</option>
<option>WaveDM: Wavelet-Based Diffusion Models for Image Restoration</option>
<option>Improved Convergence of Score-Based Diffusion Models via Prediction-Correction</option>
<option>Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models</option>
<option>Diffusion-Based Audio Inpainting</option>
<option>Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport</option>
<option>Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution</option>
<option>On the Generalization of Diffusion Model</option>
<option>Training Energy-Based Normalizing Flow with Score-Matching Objectives</option>
<option>Unpaired Image-to-Image Translation via Neural Schrödinger Bridge</option>
<option>Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models</option>
<option>DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models</option>
<option>Robust Classification via a Single Diffusion Model</option>
<option>T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities</option>
<option>Optimal Linear Subspace Search: Learning to Construct Fast and High-Quality Schedulers for Diffusion Models</option>
<option>DuDGAN: Improving Class-Conditional GANs via Dual-Diffusion</option>
<option>Differentially Private Latent Diffusion Models</option>
<option>Manifold Diffusion Fields</option>
<option>Score-Based Multimodal Autoencoders</option>
<option>On Architectural Compression of Text-to-Image Diffusion Models</option>
<option>ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</option>
<option>Parallel Sampling of Diffusion Models</option>
<option>A Diffusion Probabilistic Prior for Low-Dose CT Image Denoising</option>
<option>Unifying GANs and Score-Based Diffusion as Generative Particle Models</option>
<option>Trans-Dimensional Generative Modeling via Jump Diffusion Models</option>
<option>Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition</option>
<option>Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models</option>
<option>DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification</option>
<option>Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models</option>
<option>UDPM: Upsampling Diffusion Probabilistic Models</option>
<option>Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models</option>
<option>Tree-Based Diffusion Schrödinger Bridge with Applications to Wasserstein Barycenters</option>
<option>Error Bounds for Flow Matching Methods</option>
<option>Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model</option>
<option>Score-based Diffusion Models for Bayesian Image Reconstruction</option>
<option>Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling</option>
    </select>
  </span>
  <span class="selector">
    <span>Select Authors: </span>
    <select id="authorselection" class="selectpicker" multiple data-live-search="true">
      <option>Aaron Courville</option>
<option>Aaron Lou</option>
<option>Aaron van den Oord</option>
<option>Abhinav Gupta</option>
<option>Abhishek Kar</option>
<option>Abhishek Kumar</option>
<option>Abhishek Sinha</option>
<option>Adam Oberman</option>
<option>Adam Polyak</option>
<option>Adam Roberts</option>
<option>Adewole S. Adamson</option>
<option>Adham Elarabawy</option>
<option>Adil Salim</option>
<option>Aditya Grover</option>
<option>Aditya Ramesh</option>
<option>Adrian Thomas Huber</option>
<option>Adrienn Poór</option>
<option>Advait Parulekar</option>
<option>Ahmed A. Elhag</option>
<option>Ahmed Elgammal</option>
<option>Ahsan Mahmood</option>
<option>Aibek Alanov</option>
<option>Aimon Rahman</option>
<option>Ajay Jain</option>
<option>Ajil Jalal</option>
<option>Ajinkya Kale</option>
<option>Akash Gokul</option>
<option>Akihiro Nakamura</option>
<option>Akim Kotelnikov</option>
<option>Akshat Pandey</option>
<option>Akshay Chaudhari</option>
<option>Alaa Khaddaj</option>
<option>Alain Durmus</option>
<option>Alain Rakotomamonjy</option>
<option>Alan F. Smeaton</option>
<option>Alan Saul</option>
<option>Alan Yuille</option>
<option>Albert Gordo</option>
<option>Albert Pumarola</option>
<option>Aleksander Holynski</option>
<option>Aleksander Madry</option>
<option>Alessandro Finamore</option>
<option>Alessio Del Bue</option>
<option>Alex Bronstein</option>
<option>Alex M. Tseng</option>
<option>Alex Morehead</option>
<option>Alex Nichol</option>
<option>Alex Rav Acha</option>
<option>Alex Redden</option>
<option>Alex Schwing</option>
<option>Alex Trevithick</option>
<option>Alex Tseng</option>
<option>Alex X. Lu</option>
<option>Alexander C. Li</option>
<option>Alexander G. D. G. Matthews</option>
<option>Alexander Heckett</option>
<option>Alexander Kolesov</option>
<option>Alexander Korotin</option>
<option>Alexander Levine</option>
<option>Alexander M. Rush</option>
<option>Alexander Richard</option>
<option>Alexander Schlaefer</option>
<option>Alexander Tong</option>
<option>Alexander W. Bergman</option>
<option>Alexandra S. Gersing</option>
<option>Alexandros G. Dimakis</option>
<option>Alexandros Graikos</option>
<option>Alexandros Lattas</option>
<option>Alexei A. Efros</option>
<option>Alexey A. Gritsenko</option>
<option>Alexey Gritsenko</option>
<option>Alexia Jolicoeur-Martineau</option>
<option>Ali B Syed</option>
<option>Ali Borji</option>
<option>Ali Mahdavi-Amiri</option>
<option>Ali Siahkoohi</option>
<option>Ali Thabet</option>
<option>Alicia Durrer</option>
<option>Alireza Makhzani</option>
<option>Alison Q O'Neil</option>
<option>Allan Jabri</option>
<option>Alon Levkovitch</option>
<option>Alper Güngör</option>
<option>Aman Shrivastava</option>
<option>Amber Xie</option>
<option>Amir Hertz</option>
<option>Amir Sadikov</option>
<option>Amirhossein Kazerouni</option>
<option>Amirmohammad Rooshenas</option>
<option>Amit H. Bermano</option>
<option>Amy Zhang</option>
<option>Anastasis Germanidis</option>
<option>Anders Eklund</option>
<option>Anderson Schneider</option>
<option>Andranik Movsisyan</option>
<option>Andre Wibisono</option>
<option>Andrea Dittadi</option>
<option>Andrea Montanari</option>
<option>Andrea Tagliasacchi</option>
<option>Andrea Vedaldi</option>
<option>Andreas Blattmann</option>
<option>Andreas Krause</option>
<option>Andreas Loukas</option>
<option>Andreas Lugmayr</option>
<option>Andreas Maier</option>
<option>Andreas Stöckl</option>
<option>Andrei Chertkov</option>
<option>Andrei Kulik</option>
<option>Andrej Risteski</option>
<option>Andres Romero</option>
<option>Andrew Barron</option>
<option>Andrew Campbell</option>
<option>Andrew D. MacKinnon</option>
<option>Andrew Gilbert</option>
<option>Andrew Gordon Wilson</option>
<option>Andrew Ilyas</option>
<option>Andrew Scott</option>
<option>Andrew Tao</option>
<option>Andrey Okhotin</option>
<option>Andrey Voynov</option>
<option>Andriy Mnih</option>
<option>Andy Ly</option>
<option>Andy Shih</option>
<option>Angela Castillo</option>
<option>Angela Dai</option>
<option>Angelica I. Aviles-Rivero</option>
<option>Angelo Porrello</option>
<option>Angus Phillips</option>
<option>Anh Tran</option>
<option>AnhDung Dinh</option>
<option>Aniket Roy</option>
<option>Anima Anandkumar</option>
<option>Animesh Garg</option>
<option>Anindo Saha</option>
<option>Anirban Roy</option>
<option>Ankan Kumar Bhunia</option>
<option>Anna Kerekes</option>
<option>Anna Kuzina</option>
<option>Anna Midgley</option>
<option>Anna Rohrbach</option>
<option>Anna S. Bosman</option>
<option>Anna-Sophia Dietrich</option>
<option>Anpei Chen</option>
<option>Anru R. Zhang</option>
<option>Anshul Shah</option>
<option>Anssi Kanervisto</option>
<option>Antoine Siraudin</option>
<option>Antoine Wehenkel</option>
<option>Anton Obukhov</option>
<option>Antoni Bigata Casademunt</option>
<option>Antonia S. J. S. Mey</option>
<option>Antonin Chambolle</option>
<option>Antonio Torralba</option>
<option>Anwaar Ulhaq</option>
<option>Aonan Zhang</option>
<option>Arash Mehrjou</option>
<option>Arash Vahdat</option>
<option>Aravind R. Krishnan</option>
<option>Arden Ma</option>
<option>Aria Masoomi</option>
<option>Arian Jamasb</option>
<option>Arjun Akula</option>
<option>Arjun K. Manrai</option>
<option>Arman Chopikyan</option>
<option>Arman Cohan</option>
<option>Arman Roshannai</option>
<option>Armand Comas</option>
<option>Armin Mustafa</option>
<option>Arnab Ghosh</option>
<option>Arnaud Autef</option>
<option>Arnaud Doucet</option>
<option>Arne Schneuing</option>
<option>Arno Solin</option>
<option>Arpit Bansal</option>
<option>Artem Babenko</option>
<option>Arthur Mensch</option>
<option>Artsiom Sanakoyeu</option>
<option>Asad Aali</option>
<option>Ashwini Pokle</option>
<option>Asja Fischer</option>
<option>Auke Wiggers</option>
<option>Austin Wright</option>
<option>Ava P. Amini</option>
<option>Avideep Mukherjee</option>
<option>Axel Elaldi</option>
<option>Axel Levy</option>
<option>Axi Niu</option>
<option>Ayan Das</option>
<option>Ayush Chopra</option>
<option>Badour AlBahar</option>
<option>Bahjat Kawar</option>
<option>Baining Guo</option>
<option>Baishakhi Ray</option>
<option>Balaji Krishnamurthy</option>
<option>Bang Zhang</option>
<option>Baolin Liu</option>
<option>Baoquan Chen</option>
<option>Baoxiong Jia</option>
<option>Baoyuan Wang</option>
<option>Baptiste Chopin</option>
<option>Bardia Khosravi</option>
<option>Bastian Rieck</option>
<option>Bastian Wandt</option>
<option>Bastien Doignies</option>
<option>Behjat Siddiquie</option>
<option>Ben Mildenhall</option>
<option>Ben Poole</option>
<option>Benedikt Wiestler</option>
<option>Benjamin A. Dunn</option>
<option>Benjamin Hoover</option>
<option>Benjamin J. Holzschuh</option>
<option>Benjamin Nachman</option>
<option>Benjamin Remy</option>
<option>Benlin Liu</option>
<option>Bennett A. Landman</option>
<option>Beomsu Kim</option>
<option>Berk Tinaz</option>
<option>Bernard Ghanem</option>
<option>Bernd Bischl</option>
<option>Bernhard Schölkopf</option>
<option>Berthy T. Feng</option>
<option>Bettina Baessler</option>
<option>Bharat Singh</option>
<option>Bharath Hariharan</option>
<option>Bharath Ramsundar</option>
<option>Biao Jiang</option>
<option>Biao Zhang</option>
<option>Bihan Wen</option>
<option>Bin Cui</option>
<option>Bin Dong</option>
<option>Bin Fu</option>
<option>Bin Huang</option>
<option>Bin Xia</option>
<option>Bin Xie</option>
<option>Bing Cao</option>
<option>Bing Zeng</option>
<option>Bingchen Liu</option>
<option>Binghao Zhao</option>
<option>Bingliang Zhang</option>
<option>Bingzhe Wu</option>
<option>Binxin Yang</option>
<option>Binxu Wang</option>
<option>Björn Eskofier</option>
<option>Björn Ommer</option>
<option>Björn Schuller</option>
<option>Blága Kincső</option>
<option>Bo Dai</option>
<option>Bo Du</option>
<option>Bo Li</option>
<option>Bo Pang</option>
<option>Bo Zhang</option>
<option>Bo-Kyeong Kim</option>
<option>Bo-Wun Cheng</option>
<option>Boah Kim</option>
<option>Bob McGrew</option>
<option>Bohan Wang</option>
<option>Bonnie Berger</option>
<option>Borja Balle</option>
<option>Bosong Huang</option>
<option>Bowen Du</option>
<option>Bowen Jing</option>
<option>Boyang Yu</option>
<option>Boyu Lin</option>
<option>Bradley Brown</option>
<option>Bradley J. Erickson</option>
<option>Bradly Stadie</option>
<option>Bram Wallace</option>
<option>Bram de Wilde</option>
<option>Brandon B. May</option>
<option>Brandon Guo</option>
<option>Brandon Trabucco</option>
<option>Branislav Kveton</option>
<option>Brett Levac</option>
<option>Brian Curless</option>
<option>Brian Gordon</option>
<option>Brian L. Trippe</option>
<option>Bruno Correia</option>
<option>Bryan Catanzaro</option>
<option>Bunlong Lay</option>
<option>Burcu Karagol Ayan</option>
<option>Byeonghu Na</option>
<option>Byeongsu Sim</option>
<option>Bálint Gyepesi</option>
<option>Caiming Xiong</option>
<option>Calvin Hoi-Kwan Mak</option>
<option>Calvin Luo</option>
<option>Calvin Seward</option>
<option>Can Qin</option>
<option>Can Yang</option>
<option>Caner Hazirbas</option>
<option>Carla Gomes</option>
<option>Carola-Bibiane Schönlieb</option>
<option>Carsten Marr</option>
<option>Casey Chu</option>
<option>Ce Zheng</option>
<option>Cecilia Clementi</option>
<option>Cees G. M. Snoek</option>
<option>Cen Chen</option>
<option>Cesare M. Dalbagno</option>
<option>Ceyuan Yang</option>
<option>Chaehun Shin</option>
<option>Chaejeong Lee</option>
<option>Chang Li</option>
<option>Chang Liu</option>
<option>Chang Xu</option>
<option>Changde Du</option>
<option>Changhao Shi</option>
<option>Changhua Meng</option>
<option>Changsheng Xu</option>
<option>Changwen Zheng</option>
<option>Changyou Chen</option>
<option>Chao Du</option>
<option>Chao Li</option>
<option>Chao Wan</option>
<option>Chao Wang</option>
<option>Chao Weng</option>
<option>Chao Xu</option>
<option>Chaofan Ma</option>
<option>Chaoning Zhang</option>
<option>Chaowei Xiao</option>
<option>Chaoyue Wang</option>
<option>Charles Godfrey</option>
<option>Charles Harris</option>
<option>Charles Ollion</option>
<option>Charles R. Qi</option>
<option>Charlotte Bunne</option>
<option>Chen Change Loy</option>
<option>Chen Chen</option>
<option>Chen Henry Wu</option>
<option>Chen Ju</option>
<option>Chen Li</option>
<option>Chen Lin</option>
<option>Chen Liu</option>
<option>Chen Wei</option>
<option>Chen Zhang</option>
<option>Chen Zhao</option>
<option>Chen-Hao Chao</option>
<option>Chen-Hsuan Lin</option>
<option>Chence Shi</option>
<option>Chendong Xiang</option>
<option>Chenfei Wu</option>
<option>Chenfei Ye</option>
<option>Cheng Chen</option>
<option>Cheng Lu</option>
<option>Cheng Peng</option>
<option>Cheng Tan</option>
<option>Cheng Yang</option>
<option>Cheng Yu</option>
<option>Chengqi Duan</option>
<option>Chenguang Zhu</option>
<option>Chengxi Li</option>
<option>Chengyi Liu</option>
<option>Chengyu Wang</option>
<option>Chengyue Gong</option>
<option>Chengyue Wu</option>
<option>Chengze Li</option>
<option>Chenhao Niu</option>
<option>Chenhui Wang</option>
<option>Chenlin Meng</option>
<option>Chenpeng Du</option>
<option>Chenshuang Zhang</option>
<option>Chentao Cao</option>
<option>Chenxiao Yang</option>
<option>Chenxin Xu</option>
<option>Chi-Keung Tang</option>
<option>Chia-Che Chang</option>
<option>Chia-Jung Hsu</option>
<option>Chia-Ping Chen</option>
<option>Chieh Hubert Lin</option>
<option>Chieh-Hsin Lai</option>
<option>Chih-Wei Chang</option>
<option>Chin-Wei Huang</option>
<option>Chin-Yi Cheng</option>
<option>Chin-Yun Yu</option>
<option>Chitwan Saharia</option>
<option>Chiyu "Max'' Jiang</option>
<option>Chong Mou</option>
<option>Chong Wang</option>
<option>Chongxuan Li</option>
<option>Chongyang Ma</option>
<option>Chris A. Lee</option>
<option>Chris Dyer</option>
<option>Chris G. Willcocks</option>
<option>Chris Paxton</option>
<option>Chris Russel</option>
<option>Chris Russell</option>
<option>Christian Etmann</option>
<option>Christian Frank</option>
<option>Christian Rupprecht</option>
<option>Christian Theobalt</option>
<option>Christian Weilbach</option>
<option>Christiane Kuhl</option>
<option>Christoph Feichtenhofer</option>
<option>Christoph Haarburger</option>
<option>Christopher A. Metzler</option>
<option>Christopher Beckham</option>
<option>Christopher Pal</option>
<option>Christopher Ré</option>
<option>Christos Sakaridis</option>
<option>Chuan Li</option>
<option>Chuan Wen</option>
<option>Chuanchuan Yang</option>
<option>Chuang Gan</option>
<option>Chuang Niu</option>
<option>Chun-Hao Paul Huang</option>
<option>Chun-Yi Lee</option>
<option>Chunghsin Yeh</option>
<option>Chunyu Lin</option>
<option>Chuo-Ling Chang</option>
<option>Cihang Xie</option>
<option>Cindy M. Nguyen</option>
<option>Clayton Toste</option>
<option>Clement Vignac</option>
<option>Clément Vignac</option>
<option>Cong Fu</option>
<option>Congyue Deng</option>
<option>Connor Stone</option>
<option>Conor Durkan</option>
<option>Constantine Caramanis</option>
<option>Constantinos Daskalakis</option>
<option>Corentin Tallec</option>
<option>Cristian Canton Ferrer</option>
<option>Cristian Sbrolli</option>
<option>Cristina Granziera</option>
<option>Cristina Palmero</option>
<option>Curtis Hawthorne</option>
<option>Curtis McDonald</option>
<option>Dacheng Tao</option>
<option>DaeHan Ahn</option>
<option>Daesik Kim</option>
<option>Dahua Lin</option>
<option>Daichi Horita</option>
<option>Daiqing Li</option>
<option>Dale Schuurmans</option>
<option>Dan Bigioi</option>
<option>Dan Garrette</option>
<option>Dan Rosenbaum</option>
<option>Dan Ruta</option>
<option>Dan Su</option>
<option>Danfei Xu</option>
<option>Dani Lischinski</option>
<option>Dani Valevski</option>
<option>Daniel Bolya</option>
<option>Daniel Cohen-Or</option>
<option>Daniel D. Johnson</option>
<option>Daniel Kang</option>
<option>Daniel Korth</option>
<option>Daniel Paleka</option>
<option>Daniel Rueckert</option>
<option>Daniel S. Park</option>
<option>Daniel Severo</option>
<option>Daniel Sýkora</option>
<option>Daniel Tarlow</option>
<option>Daniel Truhn</option>
<option>Daniel Watson</option>
<option>Daniel Wesego</option>
<option>Daniel Zheng</option>
<option>Daniel Zuegner</option>
<option>Daniele Nerini</option>
<option>Daniil Gavrilov</option>
<option>Danilo Mandic</option>
<option>Danilo Rezende</option>
<option>Daniyar Turmukhambetov</option>
<option>Daochang Liu</option>
<option>Daphne Ippolito</option>
<option>Daqing Liu</option>
<option>Daquan Zhou</option>
<option>Dario Rossi</option>
<option>Dave Bignell</option>
<option>Dave Zhenyu Chen</option>
<option>David A. Klindt</option>
<option>David Baker</option>
<option>David Bau</option>
<option>David Berthelot</option>
<option>David Coeurjolly</option>
<option>David Dobre</option>
<option>David Fleet</option>
<option>David Futschik</option>
<option>David I. Inouye</option>
<option>David J Fleet</option>
<option>David J. Fleet</option>
<option>David Jacobs</option>
<option>David Lindner</option>
<option>David McAllester</option>
<option>David Munechika</option>
<option>David Rügamer</option>
<option>David Schinz</option>
<option>David Svitov</option>
<option>David W Zhang</option>
<option>David Werring</option>
<option>David Wipf</option>
<option>Davide Cozzolino</option>
<option>Davide Scaini</option>
<option>Davin Hill</option>
<option>Davis Brown</option>
<option>Davis Rempe</option>
<option>Dawn Song</option>
<option>Dayou Chen</option>
<option>De-An Huang</option>
<option>Debajyoti Sengupta</option>
<option>Debayan Bhattacharya</option>
<option>Deepak Pathak</option>
<option>Dejia Xu</option>
<option>Dejing Dou</option>
<option>Dekker Slade</option>
<option>Deli Zhao</option>
<option>Deng-Ping Fan</option>
<option>Dennis Eschweiler</option>
<option>Dequan Wang</option>
<option>Derek Nowrouzezahrai</option>
<option>Devi Parikh</option>
<option>Dewei Hu</option>
<option>Deyu Meng</option>
<option>Deyu Zhou</option>
<option>Dhairya Gandhi</option>
<option>Di Huang</option>
<option>Di Luo</option>
<option>Dian Ang Yap</option>
<option>Diana Wofk</option>
<option>Dianpeng Wang</option>
<option>Didrik Nielsen</option>
<option>Diederik P. Kingma</option>
<option>Diego Valsesia</option>
<option>Dimitris Metaxas</option>
<option>Dimitris Samaras</option>
<option>Ding Liu</option>
<option>Dinggang Shen</option>
<option>Dinghuai Zhang</option>
<option>Dmitrii Gudkov</option>
<option>Dmitry Baranchuk</option>
<option>Dmitry Molchanov</option>
<option>Dmitry Vetrov</option>
<option>Dohoon Ryu</option>
<option>Dohyun Kwon</option>
<option>Dominic Rampas</option>
<option>Dominik Hintersdorf</option>
<option>Dominik J. E. Waibel</option>
<option>Dominik Janzing</option>
<option>Dominik Lorenz</option>
<option>Dominik Narnhofer</option>
<option>Dominik Zietlow</option>
<option>Dong Chen</option>
<option>Dong Huk Park</option>
<option>Dong Liang</option>
<option>Dong Xu</option>
<option>Dong Yu</option>
<option>Dongchan Min</option>
<option>Dongchao Yang</option>
<option>Dongdong Chen</option>
<option>Dongheui Lee</option>
<option>Donghoon Ahn</option>
<option>Donghyeon Cho</option>
<option>Dongjun Kim</option>
<option>Dongki Kim</option>
<option>Dongmei Zhang</option>
<option>Dongnan Gui</option>
<option>Dongsoo Lee</option>
<option>Dorien Herremans</option>
<option>Dorit Merhof</option>
<option>Dorsa Sadigh</option>
<option>Doug Tischer</option>
<option>Douglas P. Finkbeiner</option>
<option>Dragomir Anguelov</option>
<option>Dudley Pennell</option>
<option>Duen Horng Chau</option>
<option>Duo Xu</option>
<option>Duolin Wang</option>
<option>Duomin Wang</option>
<option>Duygu Ceylan</option>
<option>Dwaraknath Gnaneshwar</option>
<option>Dylan Walker</option>
<option>Edgar Simo-Serra</option>
<option>Edmund J. C. Findlay</option>
<option>Edward Johns</option>
<option>Edwin Zhang</option>
<option>Ehsan Hajiramezanali</option>
<option>Ehsan Khodapanah Aghdam</option>
<option>Eitan Borgnia</option>
<option>Elad Richardson</option>
<option>Eli Shechtman</option>
<option>Eli Shlizerman</option>
<option>Elia Peruzzo</option>
<option>Eliahu Horwitz</option>
<option>Elies Gherbi</option>
<option>Eliot Shekhtman</option>
<option>Elisa Barney Smith</option>
<option>Eliya Nachmani</option>
<option>Elizabeth Mouchet</option>
<option>Ella Tamir</option>
<option>Ellen Zhong</option>
<option>Ellis Brown</option>
<option>Eloi Moliner</option>
<option>Emanuele Aiello</option>
<option>Emanuele Rodolà</option>
<option>Emiel Hoogeboom</option>
<option>Emile Mathieu</option>
<option>Emilian Postolache</option>
<option>Emilien Dupont</option>
<option>Emily Delaney</option>
<option>Emily Denton</option>
<option>Emmanuel de Bézenac</option>
<option>Enrico Magli</option>
<option>Enver Sangineto</option>
<option>Enze Xie</option>
<option>Erdun Gao</option>
<option>Eric A. Weiss</option>
<option>Eric Gu</option>
<option>Eric Luhman</option>
<option>Eric Moulines</option>
<option>Eric Price</option>
<option>Eric R. Chan</option>
<option>Eric Ryan Chan</option>
<option>Eric Vanden-Eijnden</option>
<option>Eric Wallace</option>
<option>Eric Zhang</option>
<option>Erich Elsen</option>
<option>Erich Kobler</option>
<option>Ernst Röell</option>
<option>Estelle Aflalo</option>
<option>Esteve Valls Mascaro</option>
<option>Ethan Manilow</option>
<option>Eun Sun Lee</option>
<option>Evan Montoya</option>
<option>Evan Shelhamer</option>
<option>Evangelos A. Theodorou</option>
<option>Evgeny Burnaev</option>
<option>Eyal Molad</option>
<option>Ezra Erives</option>
<option>Fabian Mentzer</option>
<option>Fabien Maldonado</option>
<option>Fabio Nonato</option>
<option>Fadi Boutros</option>
<option>Faez Ahmed</option>
<option>Fahad Shahbaz Khan</option>
<option>Faik C. Meral</option>
<option>Fan Bao</option>
<option>Fan Cheng</option>
<option>Fan Tang</option>
<option>Fan Yang</option>
<option>Fan Zhang</option>
<option>Fang Wen</option>
<option>Fang Wu</option>
<option>Fang Zhang</option>
<option>Fangchang Ma</option>
<option>Fangzhou Hong</option>
<option>Fei Deng</option>
<option>Fei Kong</option>
<option>Fei Ni</option>
<option>Fei Yin</option>
<option>Fei Zhang</option>
<option>Feiyang Chen</option>
<option>Felix Friedrich</option>
<option>Felix Heide</option>
<option>Feng Wang</option>
<option>Feng Yang</option>
<option>Fengpei Li</option>
<option>Fengying Yan</option>
<option>Fereshteh Forghani</option>
<option>Fergus Simpson</option>
<option>Ferhan Ture</option>
<option>Fernando De la Torre</option>
<option>Filip Ilic</option>
<option>Finn Behrendt</option>
<option>Finn Wong</option>
<option>Firas Khader</option>
<option>Firat Ozgenel</option>
<option>Fisher Yu</option>
<option>Flavio Schneider</option>
<option>Florence Regol</option>
<option>Florent Altché</option>
<option>Florentin Bieder</option>
<option>Florentin Guth</option>
<option>Florian M. Thieringer</option>
<option>Florian Schaefer</option>
<option>Florian Shkurti</option>
<option>Florian Thamm</option>
<option>Florian Tramer</option>
<option>Florian Tramèr</option>
<option>Florian Wenzel</option>
<option>Florinel-Alin Croitoru</option>
<option>Foivos Paraperas Papantoniou</option>
<option>Forrest Huang</option>
<option>Francesco Croce</option>
<option>Francesco Fumagalli</option>
<option>Francesco Giuliari</option>
<option>Francesco Locatello</option>
<option>Francesco Pedrotti</option>
<option>Francis Williams</option>
<option>Francisco Vargas</option>
<option>Francois Fleuret</option>
<option>Francois Lanusse</option>
<option>Frank Noé</option>
<option>Frank Wood</option>
<option>François G. Germain</option>
<option>François Mazé</option>
<option>Frederic Koehler</option>
<option>Fredrik K. Gustafsson</option>
<option>Frédéric Jurie</option>
<option>Fu Lee Wang</option>
<option>Furkan Ozcelik</option>
<option>Furong Huang</option>
<option>Furu Wei</option>
<option>Fuxin Fan</option>
<option>Fuxing Gao</option>
<option>Gabriela Ben Melech Stan</option>
<option>Gabriele Corso</option>
<option>Gabriele Franch</option>
<option>Gabriele Scalia</option>
<option>Gabriele Steidl</option>
<option>Gal Metzer</option>
<option>Gang Chen</option>
<option>Gang Li</option>
<option>Gang Yu</option>
<option>Gang Yue</option>
<option>Ganna Pogrebna</option>
<option>Gao Huang</option>
<option>Gaoang Wang</option>
<option>Gautam Bhattacharya</option>
<option>Gautam Mittal</option>
<option>Gautam Singh</option>
<option>Gauthier Gidel</option>
<option>Gaëtan Hadjeres</option>
<option>Ge Wang</option>
<option>Gefan Yang</option>
<option>Gefei Wang</option>
<option>Gefei Yang</option>
<option>Gemma Canet Tarrés</option>
<option>Gene Chou</option>
<option>Geoff Galgon</option>
<option>Geoffrey Hinton</option>
<option>Geonmo Gu</option>
<option>George Deligiannidis</option>
<option>George Papamakarios</option>
<option>George Retsinas</option>
<option>Georges El Fakhri</option>
<option>Georgia Chalvatzaki</option>
<option>Georgios Batzolis</option>
<option>Geraint Rees</option>
<option>Gerhard Widmer</option>
<option>German Barquero</option>
<option>Gertjan J. Burghouts</option>
<option>Giada Zingarini</option>
<option>Gianluca Scarpellini</option>
<option>Giannis Daras</option>
<option>Gihyun Kwon</option>
<option>Gilles Louppe</option>
<option>Giorgio Giannone</option>
<option>Giorgio Mariani</option>
<option>Giorgos Sfikas</option>
<option>Giovanni Poggi</option>
<option>Girish Chowdhary</option>
<option>Giulio Franzese</option>
<option>Giulio Tosato</option>
<option>Giuseppe Torri</option>
<option>Gleb Ryzhakov</option>
<option>Gokberk Elmas</option>
<option>Gong Ming</option>
<option>Gong Zhang</option>
<option>Gongfan Fang</option>
<option>Gongye Liu</option>
<option>Gordon Wetzstein</option>
<option>Gordon Wichern</option>
<option>Gowthami Somepalli</option>
<option>Gozde Unal</option>
<option>Grace Luo</option>
<option>Graham Healy</option>
<option>Graham Neubig</option>
<option>Greg Shakhnarovich</option>
<option>Greg Ver Steeg</option>
<option>Gregory Vaksman</option>
<option>Grigory Bartosh</option>
<option>Guan-Horng Liu</option>
<option>Guanchun Wang</option>
<option>Guandao Yang</option>
<option>Guang Cheng</option>
<option>Guang Yang</option>
<option>Guangcong Zheng</option>
<option>Guangrun Wang</option>
<option>Guangxing Liu</option>
<option>Guanhua Zhang</option>
<option>Guibin Wu</option>
<option>Guido Gerig</option>
<option>Guilin Liu</option>
<option>Guillaume Bal</option>
<option>Guillaume Huguet</option>
<option>Guillaume Jeanneret</option>
<option>Guillaume Leclerc</option>
<option>Guillaume Quispe</option>
<option>Guillaume Quétant</option>
<option>Guillaume Sautière</option>
<option>Gulcin Baykal</option>
<option>Guo-Jun Qi</option>
<option>Guohao Li</option>
<option>Guoqiang Zhang</option>
<option>Gustav Eje Henter</option>
<option>Gustav Mueller-Franzes</option>
<option>Gustav Müller-Franzes</option>
<option>Guy Tevet</option>
<option>Guy Williams</option>
<option>Guy Wolf</option>
<option>Guy Yariv</option>
<option>Gwanghyun Kim</option>
<option>Gyuseong Lee</option>
<option>Gyutaek Oh</option>
<option>György Fazekas</option>
<option>Hadar Averbuch-Elor</option>
<option>Hadi Salman</option>
<option>Haekyu Park</option>
<option>Hai-Tao Zheng</option>
<option>Haibin He</option>
<option>Haibin Huang</option>
<option>Hailong Hu</option>
<option>Haipeng Zhou</option>
<option>Hairong Zheng</option>
<option>Haisong Ding</option>
<option>Hakan Bilen</option>
<option>Haksoo Lim</option>
<option>Halil Faruk Karagoz</option>
<option>Hamam Mokayed</option>
<option>Hamid Kazemi</option>
<option>Hamish Ivison</option>
<option>Han Chen</option>
<option>Han Hu</option>
<option>Han Huang</option>
<option>Han Li</option>
<option>Han Liang</option>
<option>Han Shi</option>
<option>Han Zhang</option>
<option>Hang Li</option>
<option>Hang Su</option>
<option>Hang Ye</option>
<option>Hanjun Dai</option>
<option>Hannes Stärk</option>
<option>Hans van Gorp</option>
<option>Hanseok Ko</option>
<option>Hansheng Chen</option>
<option>Hanshu Yan</option>
<option>Hanspeter Pfister</option>
<option>Hanyu Wei</option>
<option>Hanyuan Liu</option>
<option>Hao Feng</option>
<option>Hao Li</option>
<option>Hao Phung</option>
<option>Hao Shi</option>
<option>Hao Su</option>
<option>Hao Tang</option>
<option>Hao Tian</option>
<option>Hao Yen</option>
<option>Hao Zhang</option>
<option>Haochen Wang</option>
<option>Haohe Liu</option>
<option>Haomiao Ni</option>
<option>Haomin Wen</option>
<option>Haonan Lu</option>
<option>Haoqi Fan</option>
<option>Haoqiang Fan</option>
<option>Haoran Sun</option>
<option>Haoran Xie</option>
<option>Haoxing Chen</option>
<option>Haoyang Yang</option>
<option>Haoyi Xiong</option>
<option>Haoze Sun</option>
<option>Haozheng Zhang</option>
<option>Hareesh Ravi</option>
<option>Harish Kamath</option>
<option>Harry Yang</option>
<option>Harry Zhang</option>
<option>Hasan A Bedel</option>
<option>Hatem Hajri</option>
<option>Hayeon Kim</option>
<option>Hazrat Ali</option>
<option>He Cao</option>
<option>He Zhang</option>
<option>HeeJae Jun</option>
<option>Heeseung Kim</option>
<option>Heewoo Jun</option>
<option>Hefeng Wu</option>
<option>Heiga Zen</option>
<option>Heiko Strathmann</option>
<option>Helen Meng</option>
<option>Heli Ben-Hamu</option>
<option>Heliang Zheng</option>
<option>Hendrik Strobelt</option>
<option>Hengtong Zhang</option>
<option>Hengyuan Ma</option>
<option>Henkjan Huisman</option>
<option>Henry Kvinge</option>
<option>Henry N. Chapman</option>
<option>Herman Kamper</option>
<option>Hervé Delingette</option>
<option>Hexiang Hu</option>
<option>Hezhen Hu</option>
<option>Hideki Tsunashima</option>
<option>Hideyuki Tachibana</option>
<option>Hila Chefer</option>
<option>Hirokazu Kameoka</option>
<option>Hiroshi Sasaki</option>
<option>Hisham Cholakkal</option>
<option>Hoigi Seo</option>
<option>Holden Lee</option>
<option>Hong Chen</option>
<option>Hong-Min Chu</option>
<option>Hongbin Li</option>
<option>Hongfei Fu</option>
<option>Hongfei Wang</option>
<option>Hongkai Zheng</option>
<option>Hongming Shan</option>
<option>Hongwei Zhang</option>
<option>Hongwen Zhang</option>
<option>Hongxia Jin</option>
<option>Hongyi Yuan</option>
<option>Hongyu Yang</option>
<option>Hossein Rahmani</option>
<option>Hossein Talebi</option>
<option>Houqiang Li</option>
<option>Hshmat Sahak</option>
<option>Hsiang-Chun Wang</option>
<option>Hsin-Min Wang</option>
<option>Hsin-Ying Lee</option>
<option>Hu Xu</option>
<option>Hua-Chieh Shao</option>
<option>Huadai Liu</option>
<option>Huaiyu Wan</option>
<option>Huan He</option>
<option>Huan Ling</option>
<option>Huan Wang</option>
<option>Huan Yang</option>
<option>Huangjie Zheng</option>
<option>Huanran Chen</option>
<option>Huanrui Yang</option>
<option>Huaqing He</option>
<option>Huazhu Fu</option>
<option>Hubert P. H. Shum</option>
<option>Hugh Jordan</option>
<option>Hui Liu</option>
<option>Hui Mao</option>
<option>Hui Tian</option>
<option>Hui Zhang</option>
<option>Huiguang He</option>
<option>Huihui Fang</option>
<option>Huijia Zhu</option>
<option>Huirong Li</option>
<option>Huiwen Chang</option>
<option>Huiying Liu</option>
<option>Huiyu Wang</option>
<option>Huiyu Zhou</option>
<option>Humphrey Shi</option>
<option>Hung-Yu Tseng</option>
<option>Huyên Pham</option>
<option>Hyemin Ahn</option>
<option>Hyeongrae Ihm</option>
<option>Hyeonho Jeong</option>
<option>Hyewon Seo</option>
<option>Hyojin Kim</option>
<option>Hyosoon Jang</option>
<option>Hyoung-Kyu Song</option>
<option>Hyungjin Chung</option>
<option>Hyunjik Kim</option>
<option>Hyunwoo Kim</option>
<option>Iain Murray</option>
<option>Ian Simon</option>
<option>Ibrahim Malik</option>
<option>Ida Momennejad</option>
<option>Idan Schwartz</option>
<option>Igor Gilitschenski</option>
<option>Igor Krawczuk</option>
<option>Il-Chul Moon</option>
<option>Ilia Igashov</option>
<option>Ilia Sucholutsky</option>
<option>Ilija Radosavovic</option>
<option>Ilker Hacihaliloglu</option>
<option>Ilya Kostrikov</option>
<option>Ilya Sutskever</option>
<option>In So Kweon</option>
<option>Inbal Leibovitch</option>
<option>Inbar Huberman-Spiegelglas</option>
<option>Inbar Mosseri</option>
<option>Ingmar Schuster</option>
<option>Inhwa Han</option>
<option>Innfarn Yoo</option>
<option>Ioannis Mitliagkas</option>
<option>Ipek Oguz</option>
<option>Ira Kemelmacher-Shlizerman</option>
<option>Ira Ktena</option>
<option>Irem Arikan Eksi</option>
<option>Irene Tallini</option>
<option>Irfan Essa</option>
<option>Irina Blok</option>
<option>Isaac Corley</option>
<option>Ishaan Gulrajani</option>
<option>István Fazekas</option>
<option>Itai Gat</option>
<option>Ivan Kapelyukh</option>
<option>Ivan Oseledets</option>
<option>Ivan Rubachev</option>
<option>Ivan Vovk</option>
<option>Iz Beltagy</option>
<option>J Webster Stayman</option>
<option>J. P. Lewis</option>
<option>J. Ryan Shue</option>
<option>J. Zico Kolter</option>
<option>Jaakko Lehtinen</option>
<option>Jack Simons</option>
<option>Jacob Austin</option>
<option>Jacob Deasy</option>
<option>Jacopo Teneggi</option>
<option>Jaden Fiotto-Kaufman</option>
<option>Jae Hyun Lim</option>
<option>Jaegul Choo</option>
<option>Jaehyeon Kim</option>
<option>Jaehyeong Jo</option>
<option>Jaemoo Choi</option>
<option>Jaeseok Jeong</option>
<option>Jaesung Tae</option>
<option>Jaewoong Choi</option>
<option>Jakiw Pidstrigach</option>
<option>Jakob Nikolas Kather</option>
<option>Jakub M. Tomczak</option>
<option>James A. Diao</option>
<option>James E. Geach</option>
<option>James Henderson</option>
<option>James Seale Smith</option>
<option>James T. Teo</option>
<option>James Thornton</option>
<option>James Vecore</option>
<option>James Y. Zou</option>
<option>Jamie Hayes</option>
<option>Jamie Smith</option>
<option>Jamie Wren-Jarvis</option>
<option>Jamie Wynn</option>
<option>Jan D. Wegner</option>
<option>Jan Kautz</option>
<option>Jan Maas</option>
<option>Jan Moritz Niehues</option>
<option>Jan Niklas Kolf</option>
<option>Jan Oscar Cross-Zamirski</option>
<option>Jan Peters</option>
<option>Jan S. Kirschke</option>
<option>Jan Stanczuk</option>
<option>Jarrid Rector-Brooks</option>
<option>Jascha Sohl-Dickstein</option>
<option>Jaskirat Singh</option>
<option>Jasmijn Bastings</option>
<option>Jason J. Yu</option>
<option>Jason Yim</option>
<option>Jason Yu</option>
<option>Javier E Santos</option>
<option>Javier Rando</option>
<option>Jay Whang</option>
<option>Jayaraman J. Thiagarajan</option>
<option>Jayoung Kim</option>
<option>Jean Kossaifi</option>
<option>Jean Yu</option>
<option>Jean-Claude Iehl</option>
<option>Jean-Luc Robert</option>
<option>Jean-Luc Starck</option>
<option>Jean-Marie Lemercier</option>
<option>Jean-Yves Franceschi</option>
<option>Jeanette Miriam Lorenz</option>
<option>Jeffrey Chang</option>
<option>Jennifer Dy</option>
<option>Jens Petersen</option>
<option>Jens Sjölund</option>
<option>Jeong Eun Lee</option>
<option>Jeong Joon Park</option>
<option>Jeongsol Kim</option>
<option>Jeremias Sulam</option>
<option>Jeremias Traub</option>
<option>Jeremy Heng</option>
<option>Jeremy Kawahara</option>
<option>Jerry Li</option>
<option>Jesse Engel</option>
<option>Jessica Dafflon</option>
<option>Jeya Maria Jose Valanarasu</option>
<option>Ji Lin</option>
<option>Ji-Rong Wen</option>
<option>Jia Gong</option>
<option>Jia Liu</option>
<option>Jia-Bin Huang</option>
<option>Jiabao Ji</option>
<option>Jiaben Chen</option>
<option>Jiachen Sun</option>
<option>Jiacheng Sun</option>
<option>Jiahan Zhang</option>
<option>Jiahao Li</option>
<option>Jiahui Yu</option>
<option>Jiaji Huang</option>
<option>Jiajun Wu</option>
<option>Jiale Xu</option>
<option>Jialing Zhang</option>
<option>Jiaming Liu</option>
<option>Jiaming Song</option>
<option>Jian Guo</option>
<option>Jian Jiao</option>
<option>Jian Liu</option>
<option>Jian Ma</option>
<option>Jian Peng</option>
<option>Jian Ren</option>
<option>Jian Tang</option>
<option>Jian Yang</option>
<option>Jian Zhang</option>
<option>Jian-Guang Lou</option>
<option>Jian-Yun Nie</option>
<option>Jianan Wang</option>
<option>Jiancheng Huang</option>
<option>Jianfei Chen</option>
<option>Jianfeng Feng</option>
<option>Jianfeng Lu</option>
<option>Jianfeng Wang</option>
<option>Jiang Bian</option>
<option>Jiang Wang</option>
<option>Jiangchao Yao</option>
<option>Jiangning Zhang</option>
<option>Jiangtao Feng</option>
<option>Jianhua Jiang</option>
<option>Jiankang Deng</option>
<option>Jianlin Cheng</option>
<option>Jianlong Fu</option>
<option>Jianmin Bao</option>
<option>Jiansheng Wei</option>
<option>Jianxin Sun</option>
<option>Jianyi Wang</option>
<option>Jianzhu Ma</option>
<option>Jianzhuang Liu</option>
<option>Jiaolong Yang</option>
<option>Jiapeng Tang</option>
<option>Jiaqi Guan</option>
<option>Jiaqi Guo</option>
<option>Jiarui Sun</option>
<option>Jiarun Liu</option>
<option>Jiasheng Ye</option>
<option>Jiashi Feng</option>
<option>Jiashi Li</option>
<option>Jiatao Gu</option>
<option>Jiatong Li</option>
<option>Jiawei Chen</option>
<option>Jiawei Huang</option>
<option>Jiawei Li</option>
<option>Jiaxi Lv</option>
<option>Jiaxin Cheng</option>
<option>Jiaxing He</option>
<option>Jiayi Guo</option>
<option>Jiayi Li</option>
<option>Jiayi Ma</option>
<option>Jiaying Liu</option>
<option>Jiayu Zou</option>
<option>Jie An</option>
<option>Jie Chen</option>
<option>Jie Ma</option>
<option>Jie S. Li</option>
<option>Jie Wu</option>
<option>Jie Yang</option>
<option>Jie Zhang</option>
<option>Jie Zhou</option>
<option>Jierui Lin</option>
<option>Jiezhang Cao</option>
<option>Jihoon Cho</option>
<option>Jiliang Tang</option>
<option>Jill Abrigo</option>
<option>Jimmy Lin</option>
<option>Jin Chen</option>
<option>Jin Gao</option>
<option>Jin Huang</option>
<option>Jinbo Xing</option>
<option>Jindong Jiang</option>
<option>Jing Cheng</option>
<option>Jing Qin</option>
<option>Jing Wang</option>
<option>Jing Xiao</option>
<option>Jing Zhao</option>
<option>Jingcai Guo</option>
<option>Jingfeng Zhang</option>
<option>Jinglin Liu</option>
<option>Jinglin Zhang</option>
<option>Jingren Zhou</option>
<option>Jingwan Lu</option>
<option>Jingxiang Sun</option>
<option>Jingye Chen</option>
<option>Jingyi Yu</option>
<option>Jingyu Hu</option>
<option>Jingyun Liang</option>
<option>Jinhao Duan</option>
<option>Jinhui Xu</option>
<option>Jinjin Gu</option>
<option>Jinkyu Kim</option>
<option>Jinmiao Huang</option>
<option>Jinqiu Sun</option>
<option>Jinsung Jeon</option>
<option>Jinwoo Shin</option>
<option>Jinxiang Liu</option>
<option>Jinyang Liu</option>
<option>Jinyi Hu</option>
<option>Jinyi Wang</option>
<option>Jinzheng He</option>
<option>Jiongxiao Wang</option>
<option>Jiqiang Feng</option>
<option>Jisu Nam</option>
<option>Jitendra Malik</option>
<option>Jiuqiang Tang</option>
<option>Jiwen Lu</option>
<option>Jiwen Yu</option>
<option>Jiyoung Lee</option>
<option>Joan Serrà</option>
<option>Joanna Materzynska</option>
<option>Joe Benton</option>
<option>Joey Bose</option>
<option>Johann Brehmer</option>
<option>Johanna Karras</option>
<option>Johannes Ackermann</option>
<option>Johannes Stegmaier</option>
<option>John Andrew Raine</option>
<option>John Collomosse</option>
<option>John J. Vastola</option>
<option>John Thickstun</option>
<option>Johnathan Chiu</option>
<option>Jon Rischewski</option>
<option>Jonas Adler</option>
<option>Jonas Beskow</option>
<option>Jonas Geiping</option>
<option>Jonas Müller</option>
<option>Jonas Ricker</option>
<option>Jonas Rothfuss</option>
<option>Jonathan C. Tan</option>
<option>Jonathan Granskog</option>
<option>Jonathan Heek</option>
<option>Jonathan Ho</option>
<option>Jonathan I. Tamir</option>
<option>Jonathan J Hunt</option>
<option>Jonathan Le Roux</option>
<option>Jonathan T. Barron</option>
<option>Jong Chul Ye</option>
<option>Jongmin Yoon</option>
<option>Joon Son Chung</option>
<option>Jooyoung Choi</option>
<option>Jordi Pons</option>
<option>Jorma Laaksonen</option>
<option>Josh Gardner</option>
<option>Josh Susskind</option>
<option>Joshua B. Tenenbaum</option>
<option>Joshua M. Susskind</option>
<option>Josue Casco-Rodriguez</option>
<option>Joyce C Ho</option>
<option>Juan Carlos Niebles</option>
<option>Juan Miguel Lopez Alcaraz</option>
<option>Judy Hoffman</option>
<option>Juergen Weissinger</option>
<option>Juho Lee</option>
<option>Juhua Liu</option>
<option>Juhyun Lee</option>
<option>Julen Urain</option>
<option>Julia Krüger</option>
<option>Julia Wolleb</option>
<option>Julie Digne</option>
<option>Julius Berner</option>
<option>Julius Richter</option>
<option>Jumin Lee</option>
<option>Jun Gao</option>
<option>Jun Huang</option>
<option>Jun Lan</option>
<option>Jun Liu</option>
<option>Jun Pang</option>
<option>Jun Seob Shin</option>
<option>Jun Wang</option>
<option>Jun Yu</option>
<option>Jun Yue</option>
<option>Jun Zhu</option>
<option>Jun-Yan Zhu</option>
<option>JunMing Hou</option>
<option>Junbo Peng</option>
<option>Junchi Yan</option>
<option>Junde Wu</option>
<option>Jungbeom Lee</option>
<option>Junho Kim</option>
<option>Junhyeok Lee</option>
<option>Junier Oliva</option>
<option>Junjie Zhou</option>
<option>Junliang Guo</option>
<option>Junshi Huang</option>
<option>Junsoo Lee</option>
<option>Juntao Li</option>
<option>Junwei Zhu</option>
<option>Junyi Li</option>
<option>Junyi Zhang</option>
<option>Junyoung Chung</option>
<option>Junyoung Seo</option>
<option>Jussi Leinonen</option>
<option>Justin Domke</option>
<option>Justin Lovelace</option>
<option>Justin N. M. Pinkney</option>
<option>Justin Roper</option>
<option>Justus Thies</option>
<option>K. Aditya Mohan</option>
<option>Kaan Oktay</option>
<option>Kai Chen</option>
<option>Kai Han</option>
<option>Kai Li</option>
<option>Kai Packhäuser</option>
<option>Kai Shang</option>
<option>Kai Wang</option>
<option>Kai Yu</option>
<option>Kai Zhang</option>
<option>Kai-En Lin</option>
<option>Kaidi Xu</option>
<option>Kaiduo Zhang</option>
<option>Kaifeng Zou</option>
<option>Kaiqi Huang</option>
<option>Kaiwen Xu</option>
<option>Kaiwen Xue</option>
<option>Kaiwen Zheng</option>
<option>Kaizhi Qian</option>
<option>Kamil Deja</option>
<option>Kamyar Azizzadenesheli</option>
<option>Kanchana Vaishnavi Gandikota</option>
<option>Kang Liao</option>
<option>Kang Zhang</option>
<option>Kangfu Mei</option>
<option>Kangwook Lee</option>
<option>Kangxue Yin</option>
<option>Kangyeol Kim</option>
<option>Kanji Uchino</option>
<option>Kannan Ramchandran</option>
<option>Karen Ullrich</option>
<option>Karl Holmquist</option>
<option>Karlis Freivalds</option>
<option>Karsten Kreis</option>
<option>Karsten Roscher</option>
<option>Karttikeya Mangalam</option>
<option>Karun Kumar</option>
<option>Kashif Rasul</option>
<option>Katherine L. Bouman</option>
<option>Katja Hofmann</option>
<option>Katja Schwarz</option>
<option>Katrina Cirone</option>
<option>Kawin Setsompop</option>
<option>Kaylee Yingxi Yang</option>
<option>Kayvon Fatahalian</option>
<option>Kazuki Shimada</option>
<option>Kazusato Oko</option>
<option>Ke Li</option>
<option>Ke Wang</option>
<option>Kehan Li</option>
<option>Keisuke Kawano</option>
<option>Keitaro Tanaka</option>
<option>Keith A. Johnson</option>
<option>Kejun Zhang</option>
<option>Kelvin C. K. Chan</option>
<option>Kelvin Ritland</option>
<option>Ken Osato</option>
<option>Keqiang Yan</option>
<option>Ketul Shah</option>
<option>Kevin Black</option>
<option>Kevin Clark</option>
<option>Kevin E. Wu</option>
<option>Kevin Eloff</option>
<option>Kevin Ferreira</option>
<option>Kevin K. Yang</option>
<option>Kevin Li</option>
<option>Kevin Zhou</option>
<option>Kexin Zhao</option>
<option>Kexun Zhang</option>
<option>Kfir Aberman</option>
<option>Ki-Ung Song</option>
<option>Kibeom Hong</option>
<option>Kihong Kim</option>
<option>Kihyuk Sohn</option>
<option>Kilian Fatras</option>
<option>Kilian Weinberger</option>
<option>Kim L. Sandler</option>
<option>Kin Wai Cheuk</option>
<option>Kirill Neklyudov</option>
<option>Kiyoharu Aizawa</option>
<option>Knut Zoch</option>
<option>Kohei Yatabe</option>
<option>Koichi Saito</option>
<option>Koji Maruhashi</option>
<option>Koki Nagano</option>
<option>Konpat Preechakul</option>
<option>Konrad Schindler</option>
<option>Konstantina Nikolaidou</option>
<option>Konstantinos G. Derpanis</option>
<option>Konstantinos Vougioukas</option>
<option>Korbinian Abstreiter</option>
<option>Kota Yamaguchi</option>
<option>Kotaro Kikuchi</option>
<option>Kou Tanaka</option>
<option>Koutilya Pnvr</option>
<option>Kris Kitani</option>
<option>Krishna Kumar Singh</option>
<option>Krishnamurthy Dj Dvijotham</option>
<option>Kristian Kersting</option>
<option>Kristy Choi</option>
<option>Krunoslav Lehman Pavasovic</option>
<option>Kuang Gong</option>
<option>Kuanning Wang</option>
<option>Kun Su</option>
<option>Kun Xu</option>
<option>Kunbo Zhang</option>
<option>Kunpeng Song</option>
<option>Kurt Keutzer</option>
<option>Kushagra Pandey</option>
<option>Kwan-Yee K. Wong</option>
<option>KwangHee Lee</option>
<option>Kwanyoung Kim</option>
<option>Kychul Lee</option>
<option>Kyle Doherty</option>
<option>Kyle Olszewski</option>
<option>Kyungwoo Song</option>
<option>Lala Li</option>
<option>Lan Jiang</option>
<option>Lan Xu</option>
<option>Lan Yi</option>
<option>Lanqing Guo</option>
<option>Lanqing Hong</option>
<option>Lantao Yu</option>
<option>Lanya T. Cai</option>
<option>Lars Doorenbos</option>
<option>Lars Jebe</option>
<option>Lars Lien Ankile</option>
<option>Lars Ruthotto</option>
<option>Latif Abid</option>
<option>Laura Bravo-Sánchez</option>
<option>Laurent Sartran</option>
<option>Laurent Sifre</option>
<option>Le Kang</option>
<option>Lea Bogensperger</option>
<option>Lefei Zhang</option>
<option>Lei Bai</option>
<option>Lei Cui</option>
<option>Lei He</option>
<option>Lei Li</option>
<option>Lei Xing</option>
<option>Lei Yang</option>
<option>Lei Zhang</option>
<option>Lei Zhu</option>
<option>Leila Mozaffari</option>
<option>Leilei Sun</option>
<option>Lemeng Wu</option>
<option>Leming Guo</option>
<option>Lennart Heim</option>
<option>Leo Klarner</option>
<option>Leo Schwinn</option>
<option>Leonard Berrada</option>
<option>Leonidas Guibas</option>
<option>Leslie Rice</option>
<option>Lev Markhasin</option>
<option>Levon Khachatryan</option>
<option>Lewei Yao</option>
<option>Leyuan Fang</option>
<option>Li Kevin Wenliang</option>
<option>Li Lin</option>
<option>Li Yuan</option>
<option>Li Zhang</option>
<option>Li Zhu</option>
<option>Li'an Zhuo</option>
<option>Li-Ping Liu</option>
<option>Li-ping Liu</option>
<option>Liam Paull</option>
<option>Liang Lin</option>
<option>Liang Pan</option>
<option>Liang Wan</option>
<option>Liang Wang</option>
<option>Liang Zheng</option>
<option>Liang-Jian Deng</option>
<option>Liangbin Xie</option>
<option>Lianghua Huang</option>
<option>Liangpei Zhang</option>
<option>Liansheng Zhuang</option>
<option>Libin Liu</option>
<option>Licheng Jiao</option>
<option>Liefeng Bo</option>
<option>Ligong Han</option>
<option>Lihua Qian</option>
<option>Lijie Liu</option>
<option>Lijing Liang</option>
<option>Lijuan Wang</option>
<option>Lijun Li</option>
<option>Lijun Yu</option>
<option>Lilac Atassi</option>
<option>Limei Wang</option>
<option>Lin Geng Foo</option>
<option>Linden Li</option>
<option>Ling Yang</option>
<option>Linghe Kong</option>
<option>Lingjie Liu</option>
<option>Lingpeng Kong</option>
<option>Lingyu Zhang</option>
<option>Linjie Li</option>
<option>Linli Xu</option>
<option>Linqi Zhou</option>
<option>Linqian Fan</option>
<option>Linqing Liu</option>
<option>Lior Wolf</option>
<option>Lior Yariv</option>
<option>Lisa Dunlap</option>
<option>Litu Rout</option>
<option>Liu Zhang</option>
<option>Lixuan Yang</option>
<option>Liyan Xie</option>
<option>Liyuan Chen</option>
<option>Liyue Shen</option>
<option>Long Lan</option>
<option>Long Lian</option>
<option>Lorenz Richter</option>
<option>Lorenzo Luzi</option>
<option>Lorin Sweeney</option>
<option>Louis Sharrock</option>
<option>Loïc Simon</option>
<option>Loïs Paulin</option>
<option>Lu Yuan</option>
<option>Luc Van Gool</option>
<option>Luca Cosmo</option>
<option>Lucas Theis</option>
<option>Ludovic Dos Santos</option>
<option>Luisa Verdoliva</option>
<option>Lujia Bai</option>
<option>Lukas Folle</option>
<option>Lukas Herron</option>
<option>Lukas Schott</option>
<option>Lukas Struppek</option>
<option>Lukas Zbinden</option>
<option>Luke Melas-Kyriazi</option>
<option>Luke W. Sagers</option>
<option>Luming Tang</option>
<option>Luping Liu</option>
<option>Luzhe Sun</option>
<option>Lvmin Zhang</option>
<option>M. Jorge Cardoso</option>
<option>Machel Reid</option>
<option>Maciej Zięba</option>
<option>Maham Tanveer</option>
<option>Mahdi Soltanolkotabi</option>
<option>Mahmut Yurt</option>
<option>Maja Pantic</option>
<option>Maka Karalashvili</option>
<option>Malsha V. Perera</option>
<option>Mana Moassefi</option>
<option>Maneesh Agrawala</option>
<option>Mang Ning</option>
<option>Manuel Brack</option>
<option>Mao Ye</option>
<option>Maosong Sun</option>
<option>Maozong Zheng</option>
<option>Marc Aubreville</option>
<option>Marc L. Klasky</option>
<option>Marco Federici</option>
<option>Marco Huber</option>
<option>Marco Mondelli</option>
<option>Marco Pavone</option>
<option>Marcus A. Brubaker</option>
<option>Marcus Liwicki</option>
<option>Margarita Vinaroz</option>
<option>Maria Dwornik</option>
<option>Maria Escobar</option>
<option>Maria Rodriguez Martinez</option>
<option>Marin Biloš</option>
<option>Marius Arvinte</option>
<option>Mark Beaumont</option>
<option>Mark Chen</option>
<option>Mark Coates</option>
<option>Mark D. Butala</option>
<option>Mark D. Plumbley</option>
<option>Mark S. Graham</option>
<option>Markus Heinonen</option>
<option>Marloes Arts</option>
<option>Martin Andrews</option>
<option>Martin Danelljan</option>
<option>Martin Gonzalez</option>
<option>Martin Renqiang Min</option>
<option>Martin Styner</option>
<option>Martin Trapp</option>
<option>Martin Zach</option>
<option>Maryam Qamar</option>
<option>Masato Hirano</option>
<option>Masayoshi Tomizuka</option>
<option>Matan Atad</option>
<option>Matan Kalman</option>
<option>Matan Kleiner</option>
<option>Mathias Seuret</option>
<option>Matt Le</option>
<option>Matt Tivnan</option>
<option>Matteo Frosi</option>
<option>Matteo Matteucci</option>
<option>Matteo Pariset</option>
<option>Matthew A. Chan</option>
<option>Matthew Baas</option>
<option>Matthew D. Hoffman</option>
<option>Matthew E. Peters</option>
<option>Matthew Fisher</option>
<option>Matthew Groh</option>
<option>Matthew J. Clarkson</option>
<option>Matthew Jagielski</option>
<option>Matthew Leigh</option>
<option>Matthew Walter</option>
<option>Matthias Grundmann</option>
<option>Matthias Hein</option>
<option>Matthias Keicher</option>
<option>Matthias Muller</option>
<option>Matthias Niessner</option>
<option>Matthias Nießner</option>
<option>Matthias Plasser</option>
<option>Matthias Weigel</option>
<option>Matthäus Kleindessner</option>
<option>Mauricio Delbracio</option>
<option>Maurizio Filippone</option>
<option>Max Cohen</option>
<option>Max Daniels</option>
<option>Max F. Burg</option>
<option>Max Gurinas</option>
<option>Max Horn</option>
<option>Max Tegmark</option>
<option>Max W. Shen</option>
<option>Max W. Y. Lam</option>
<option>Max Welling</option>
<option>Maxence Noble</option>
<option>Maxime Sermesant</option>
<option>Maximilian Augustin</option>
<option>Maximilian Nickel</option>
<option>Maximilian Schulze-Hagen</option>
<option>Mayank Kumar</option>
<option>Mayu Otani</option>
<option>Md Selim</option>
<option>Meiling Fang</option>
<option>Mengchun Zhang</option>
<option>Mengkang Lu</option>
<option>Mengrui Chen</option>
<option>Mengze Xu</option>
<option>Miao Hua</option>
<option>Micah Goldblum</option>
<option>Michael A. Brooks</option>
<option>Michael A. Riegler</option>
<option>Michael Backes</option>
<option>Michael Bronstein</option>
<option>Michael Elad</option>
<option>Michael F. Liu</option>
<option>Michael Hutchinson</option>
<option>Michael Ingrisch</option>
<option>Michael J. Black</option>
<option>Michael J. Smith</option>
<option>Michael Janner</option>
<option>Michael McThrow</option>
<option>Michael Rubinstein</option>
<option>Michael S. Albergo</option>
<option>Michael T. McCann</option>
<option>Michael T. Mccann</option>
<option>Michael Tanzer</option>
<option>Michael Zeng</option>
<option>Michal Geyer</option>
<option>Michal Irani</option>
<option>Michał Stypułkowski</option>
<option>Michał Zając</option>
<option>Michele Mancusi</option>
<option>Michiel Bacchiani</option>
<option>Mickaël Chen</option>
<option>Midhun Harikumar</option>
<option>Miguel Angel Bautista</option>
<option>Miguel Ángel Bautista</option>
<option>Mihir Prabhudesai</option>
<option>Miika Aittala</option>
<option>Mijung Park</option>
<option>Mike Gartrell</option>
<option>Mikhail Kudinov</option>
<option>Mikolaj Binkowski</option>
<option>Milad Nasr</option>
<option>Min Lin</option>
<option>Min Zhao</option>
<option>Ming Zhang</option>
<option>Ming-Hsuan Yang</option>
<option>Ming-Yu Liu</option>
<option>Mingfei Sun</option>
<option>Mingi Kwon</option>
<option>Mingjie Sun</option>
<option>Mingqiang Wei</option>
<option>Mingxuan Wang</option>
<option>Mingyan Han</option>
<option>Mingyang Yi</option>
<option>Mingyu Ding</option>
<option>Mingyuan Fan</option>
<option>Mingyuan Zhang</option>
<option>Mingyuan Zhou</option>
<option>Mingze Li</option>
<option>Minh N. Do</option>
<option>Minh-Quan Le</option>
<option>Minh-Triet Tran</option>
<option>Minheng Ni</option>
<option>Minhyeok Lee</option>
<option>Minjun Li</option>
<option>Minjung Kim</option>
<option>Minkai Xu</option>
<option>Minki Kang</option>
<option>Minne Yuan</option>
<option>Minshan Xie</option>
<option>Mo Yu</option>
<option>Mo Zhou</option>
<option>Moab Arar</option>
<option>Mocho Go</option>
<option>Moein Heidari</option>
<option>Mohamad Shahbazi</option>
<option>Mohamed Akrout</option>
<option>Mohamed Daoudi</option>
<option>Mohamed Hamdouche</option>
<option>Mohammad Amin Shabani</option>
<option>Mohammad Norouzi</option>
<option>Mohammed El-Amine Azz</option>
<option>Mohit Bansal</option>
<option>Mohsen Fayyaz</option>
<option>Molei Tao</option>
<option>Morteza Mardani</option>
<option>Mu Li</option>
<option>Muah Kim</option>
<option>Mubarak Shah</option>
<option>Muhammad Hamza Mughal</option>
<option>Muhammad Huzaifa</option>
<option>Muhammad Usman Akbar</option>
<option>Muheng Li</option>
<option>Mukai Li</option>
<option>Muneyoshi Inahara</option>
<option>Muyang Li</option>
<option>Muyi Sun</option>
<option>Muzaffer Özbey</option>
<option>Myungjoo Kang</option>
<option>Máté Kovács</option>
<option>N. Joseph Tatro</option>
<option>Nader Masmoudi</option>
<option>Namhyuk Ahn</option>
<option>Nan Duan</option>
<option>Nan Liu</option>
<option>Nanning Zheng</option>
<option>Nanxin Chen</option>
<option>Naoki Murata</option>
<option>Naoto Inoue</option>
<option>Naoya Takahashi</option>
<option>Narek Tumanyan</option>
<option>Naser Damer</option>
<option>Nassir Navab</option>
<option>Nataniel Ruiz</option>
<option>Nate Gruver</option>
<option>Nathan Shnidman</option>
<option>Nathaniel Diamant</option>
<option>Nattanat Chatthee</option>
<option>Naveed Akhtar</option>
<option>Nayantara Mudur</option>
<option>Naye Ji</option>
<option>Nebojsa Jojic</option>
<option>Neel Dey</option>
<option>Neil Zeghidour</option>
<option>Nelson Fernandez</option>
<option>Nga Yan Chan</option>
<option>Ngai-Man Cheung</option>
<option>Ngoc Tran</option>
<option>Niall Jeffrey</option>
<option>Nic Fishman</option>
<option>Nicholas Carlini</option>
<option>Nicholas Lubbers</option>
<option>Nicholas M. Boffi</option>
<option>Nicholas T. Runcie</option>
<option>Nicola Franco</option>
<option>Nicolas Bonneel</option>
<option>Nicole Tianjiao Yang</option>
<option>Nicu Sebe</option>
<option>Nikhil Arora</option>
<option>Nikhil Naik</option>
<option>Nikita Balagansky</option>
<option>Nikita Drobyshev</option>
<option>Nikita Gushchin</option>
<option>Niklas Funk</option>
<option>Nikola B. Kovachki</option>
<option>Nikola Simidjievski</option>
<option>Nikolai Kalischek</option>
<option>Nikolaos N. Vlassis</option>
<option>Nikolay Malkin</option>
<option>Nikolay Savinov</option>
<option>Niloy J. Mitra</option>
<option>Nils Strodthoff</option>
<option>Nils Thuerey</option>
<option>Nima Anari</option>
<option>Ning Chen</option>
<option>Ning Yu</option>
<option>Niru Maheswaranathan</option>
<option>Nisha Chandramoorthy</option>
<option>Nisha Huang</option>
<option>Nithin Gopalakrishnan Nair</option>
<option>Niv Haim</option>
<option>Niwa Kenta</option>
<option>Nizhuan Wang</option>
<option>Noah Constant</option>
<option>Noah Snavely</option>
<option>Noam Elata</option>
<option>Nobukatsu Hojo</option>
<option>Nontawat Charoenphakdee</option>
<option>Nontawat Tritrong</option>
<option>Noor Fathima Ghouse</option>
<option>Nori Jacoby</option>
<option>Noriyuki Kojima</option>
<option>Noseong Park</option>
<option>Nupur Kumari</option>
<option>Octavia Camps</option>
<option>Octavian-Eugen Ganea</option>
<option>Ohad Fried</option>
<option>Ole Winther</option>
<option>Olivia Wiles</option>
<option>Omer Bar-Tal</option>
<option>Omer Tov</option>
<option>Omri Avrahami</option>
<option>Ona Wu</option>
<option>Onat Dalmaz</option>
<option>Or Litany</option>
<option>Or Patashnik</option>
<option>Oran Gafni</option>
<option>Oran Lang</option>
<option>Oron Ashual</option>
<option>Osama Makansi</option>
<option>Oswin So</option>
<option>P. Thomas Fletcher</option>
<option>P. Y. Mok</option>
<option>Pablo Arbeláez</option>
<option>Pablo Márquez-Neila</option>
<option>Pablo Pernias</option>
<option>Pallabi Ghosh</option>
<option>Pamela Mishkin</option>
<option>Pan Xie</option>
<option>Paolo Cudrano</option>
<option>Paramanand Chandramouli</option>
<option>Parashkev Nachev</option>
<option>Pareesa Ameneh Golnari</option>
<option>Parmida Atighehchian</option>
<option>Pascal Frossard</option>
<option>Pascal Peter</option>
<option>Patrick Blöbaum</option>
<option>Patrick Esser</option>
<option>Patrick Forré</option>
<option>Patrick Schramowski</option>
<option>Patrick Siebke</option>
<option>Paul Friedrich</option>
<option>Paul Guerrero</option>
<option>Paul Hagemann</option>
<option>Paul Hand</option>
<option>Paul Henderson</option>
<option>Paul M Mayer</option>
<option>Paul Rolland</option>
<option>Paul Wright</option>
<option>Pedro F Da Costa</option>
<option>Pedro F da Costa</option>
<option>Pedro Ferreira</option>
<option>Pedro Sanchez</option>
<option>Peiang Zhao</option>
<option>Peihao Wang</option>
<option>Peiran Dong</option>
<option>Peiran Ren</option>
<option>Peiye Zhuang</option>
<option>Peiyu Yu</option>
<option>Peize Sun</option>
<option>Peng Jin</option>
<option>Peng Xue</option>
<option>Pengcheng He</option>
<option>Pengfei Guo</option>
<option>Pengfei Zhu</option>
<option>Penghai Zhao</option>
<option>Percy Liang</option>
<option>Peter Corcoran</option>
<option>Peter Pao-Huang</option>
<option>Peter Sadowski</option>
<option>Peter Wonka</option>
<option>Petru-Daniel Tudosiu</option>
<option>Peyman Milanfar</option>
<option>Peyman Najafirad</option>
<option>Peyton Greenside</option>
<option>Pham Ngoc Huy</option>
<option>Philip H. S. Torr</option>
<option>Philipp Schad</option>
<option>Philippe C. Cattin</option>
<option>Philippe Ciuciu</option>
<option>Phillip Maffettone</option>
<option>Pierre H. Richemond</option>
<option>Pierre Henry-Labordere</option>
<option>Pieter Abbeel</option>
<option>Pietro Liò</option>
<option>Pietro Lió</option>
<option>Pietro Michiardi</option>
<option>Pim de Haan</option>
<option>Pin-Yu Chen</option>
<option>Ping Li</option>
<option>Ping Luo</option>
<option>Piotr Miłoś</option>
<option>Piyush Kumar</option>
<option>Piyush Rai</option>
<option>Po-Yao Huang</option>
<option>Pontus Stenetorp</option>
<option>Pouria Rouzrokh</option>
<option>Pradyumna Narayana</option>
<option>Prafulla Dhariwal</option>
<option>Prakhar Srivastava</option>
<option>Pranav Rajpurkar</option>
<option>Pranav Shyam</option>
<option>Pratik Mukherjee</option>
<option>Pratyush Tiwary</option>
<option>Praveen Anand</option>
<option>Princy Chahal</option>
<option>Priyank Jaini</option>
<option>Priyatham Kattakinda</option>
<option>Puhao Li</option>
<option>Puijin Cheng</option>
<option>Puntawat Ponglertnapakorn</option>
<option>Pål Halvorsen</option>
<option>Péter Holló</option>
<option>Qi Dou</option>
<option>Qi Meng</option>
<option>Qi Shan</option>
<option>Qi Tian</option>
<option>Qi Wu</option>
<option>Qi Zhu</option>
<option>Qian He</option>
<option>Qian Xu</option>
<option>Qiang Huo</option>
<option>Qiang Liu</option>
<option>Qiang Qiu</option>
<option>Qiegen Liu</option>
<option>Qifeng Chen</option>
<option>Qihua Zhou</option>
<option>Qing Guo</option>
<option>Qing Li</option>
<option>Qing Lyu</option>
<option>Qinghe Wang</option>
<option>Qinghua Hu</option>
<option>Qingqing Huang</option>
<option>Qingzhe Gao</option>
<option>Qinsheng Zhang</option>
<option>Qinye Zhou</option>
<option>Qiong Luo</option>
<option>Qipeng Zhang</option>
<option>Qitian Wu</option>
<option>Qiuhong Ke</option>
<option>Qiyuan Hu</option>
<option>Qiyue Li</option>
<option>Quan Bai</option>
<option>Quan Dao</option>
<option>Quanlin Wu</option>
<option>Quanzheng Li</option>
<option>Quoc V. Le</option>
<option>R. Oguz Araz</option>
<option>RJ Mical</option>
<option>Rabeeh Karimi Mahabadi</option>
<option>Rachel Kurchin</option>
<option>Rachel McDonnell</option>
<option>Radu Timofte</option>
<option>Radu Tudor Ionescu</option>
<option>Rafael F. Schaefer</option>
<option>Raja Giryes</option>
<option>Raja Jurdak</option>
<option>Raja Marjieh</option>
<option>Rajmund Nagy</option>
<option>Raluca Georgescu</option>
<option>Rama Chellappa</option>
<option>Raman Sarokin</option>
<option>Ran Ran</option>
<option>Ran Xu</option>
<option>Rao Fu</option>
<option>Rao Muhammad Anwer</option>
<option>Rapha Gontijo Lopes</option>
<option>Raphael Sznitman</option>
<option>Raphael Tang</option>
<option>Ravi Ramamoorthi</option>
<option>Raymond A. Yeh</option>
<option>Raza Imam</option>
<option>Regina Barzilay</option>
<option>Renat Bashirov</option>
<option>Renato Berlinghieri</option>
<option>Renjie Song</option>
<option>Reza Azad</option>
<option>Rianne van den Berg</option>
<option>Ricardo Baptista</option>
<option>Ricardo Martin-Brualla</option>
<option>Riccardo Corvi</option>
<option>Richard Baraniuk</option>
<option>Richard L. J. Qiu</option>
<option>Richard P. G. ten Broek</option>
<option>Richard Zhang</option>
<option>Rick Fritschek</option>
<option>Ricky T. Q. Chen</option>
<option>Rishabh Dabral</option>
<option>Rita Cucchiara</option>
<option>Rob Brekelmans</option>
<option>Robby T. Tan</option>
<option>Robert Gray</option>
<option>Robert Pinsler</option>
<option>Robert Stanforth</option>
<option>Roberto Henschel</option>
<option>Robin Rombach</option>
<option>Robin San Roman</option>
<option>Robin San-Roman</option>
<option>Robin Sandkühler</option>
<option>Robin Strudel</option>
<option>Robin Zbinden</option>
<option>Rodrigo Mira</option>
<option>Roger Zimmermann</option>
<option>Rohit Gandikota</option>
<option>Rohit Pandey</option>
<option>Roland Opfer</option>
<option>Roland S. Zimmermann</option>
<option>Roland Vollgraf</option>
<option>Rolf Jager</option>
<option>Roman Macháček</option>
<option>Ron J. Weiss</option>
<option>Ron Mokady</option>
<option>Rongjie Huang</option>
<option>Rosanne Liu</option>
<option>Roy Ganz</option>
<option>Roy Kapon</option>
<option>Rufin VanRullen</option>
<option>Ruibin Li</option>
<option>Ruichen Wang</option>
<option>Ruihan Yang</option>
<option>Ruijun Li</option>
<option>Ruili Feng</option>
<option>Ruiqi Gao</option>
<option>Ruiqi Li</option>
<option>Ruixiang Zhang</option>
<option>Ruiyang Jin</option>
<option>Ruizhi Shao</option>
<option>Runnan Li</option>
<option>Runsheng Xu</option>
<option>Runyi Li</option>
<option>Runyi Yu</option>
<option>Ruojin Cai</option>
<option>Ruoqi Wang</option>
<option>Rushil Anirudh</option>
<option>Ruslan Salakhutdinov</option>
<option>Ruud J. G. van Sloun</option>
<option>Ruzhong Xie</option>
<option>Ryan A. Jackson</option>
<option>Ryan Cotterell</option>
<option>Ryan Po</option>
<option>Ryan Webster</option>
<option>Ryoko Tokuhisa</option>
<option>Ryosuke Sawata</option>
<option>Rémi Leblond</option>
<option>Rémi Piché-Taillefer</option>
<option>Rémi Tachet des Combes</option>
<option>S. Kevin Zhou</option>
<option>S. M. Ali Eslami</option>
<option>S. Sara Mahdavi</option>
<option>Sachin Kelkar</option>
<option>Sachin Kumar</option>
<option>Saeid Naderiparizi</option>
<option>Sagie Benaim</option>
<option>Sahra Ghalebikesabi</option>
<option>Saining Xie</option>
<option>Saiyue Lyu</option>
<option>Salman Khan</option>
<option>Salman UH Dar</option>
<option>Sam Bond-Taylor</option>
<option>Sam Devlin</option>
<option>Sam Witteveen</option>
<option>Samaneh Azadi</option>
<option>Samira Abnar</option>
<option>Samuel Denton</option>
<option>Samuel L. Smith</option>
<option>Samuel Stanton</option>
<option>Samuli Laine</option>
<option>Sanaz Vahdati</option>
<option>Sander Dieleman</option>
<option>Sandesh Ghimire</option>
<option>Sandy Engelhardt</option>
<option>Sang-gil Lee</option>
<option>Sangdoo Yun</option>
<option>Sanghyuk Chun</option>
<option>Sangjun Han</option>
<option>Sangpil Kim</option>
<option>Sangwoo Mo</option>
<option>Sangyun Lee</option>
<option>Sanja Fidler</option>
<option>Sanjay Shakkottai</option>
<option>Santiago Pascual</option>
<option>Sarah C. Foreman</option>
<option>Sarthak Mittal</option>
<option>Sasha Luccioni</option>
<option>Saurabh Saxena</option>
<option>Sauradip Nag</option>
<option>Savvas Panagiotou</option>
<option>Savvas Zannettou</option>
<option>Scottie Fox</option>
<option>Se Jung Kwon</option>
<option>Se Young Chun</option>
<option>Sean Fanello</option>
<option>Sean I. Young</option>
<option>Sebastian Foersch</option>
<option>Sebastian Reich</option>
<option>Sebastian Weisshaar</option>
<option>Sebastien Ourselin</option>
<option>Sebin Lee</option>
<option>Sehui Kim</option>
<option>Sen He</option>
<option>Sen Jia</option>
<option>Sen Zhang</option>
<option>Seokju Cho</option>
<option>Seongmin Lee</option>
<option>Sepidehsadat Hosseini</option>
<option>Serena Yeung</option>
<option>Serge Belongie</option>
<option>Sergejs Kozlovics</option>
<option>Sergey Levine</option>
<option>Sergey Tulyakov</option>
<option>Sergio Escalera</option>
<option>Sergio Orts-Escolano</option>
<option>Sergio Valcarcel Macua</option>
<option>Seul Lee</option>
<option>Seung Hyun Lee</option>
<option>Seung Wook Kim</option>
<option>Seungjae Shin</option>
<option>Seungjun Nah</option>
<option>Seungryong Kim</option>
<option>Seungu Han</option>
<option>Severi Rissanen</option>
<option>Sewon Park</option>
<option>Seyed Kamyar Seyed Ghasemipour</option>
<option>Shady Abu-Hussein</option>
<option>Shafaq Murad</option>
<option>Shahar Lutati</option>
<option>Shahar Yadin</option>
<option>Shaheer U. Saeed</option>
<option>Shahriar Faghani</option>
<option>Shai Bagon</option>
<option>Shaked Dovrat</option>
<option>Shalini De Mello</option>
<option>Shan Tan</option>
<option>Shan Zheng Tan</option>
<option>Shancheng Fang</option>
<option>Shanchuan Lin</option>
<option>Shandian Zhe</option>
<option>Shang Chai</option>
<option>Shang Lu</option>
<option>Shang-Fu Chen</option>
<option>Shangchen Zhou</option>
<option>Shanghang Zhang</option>
<option>Shangrong Yang</option>
<option>Shangyuan Tong</option>
<option>Shansan Gong</option>
<option>Shanshe Wang</option>
<option>Shant Navasardyan</option>
<option>Shao-Hua Sun</option>
<option>Shao-Yen Tseng</option>
<option>Shaobo Xia</option>
<option>Shaoting Zhu</option>
<option>Shaoyan Pan</option>
<option>Shaozhe Hao</option>
<option>Sharan Narang</option>
<option>Sharon X. Huang</option>
<option>Shekoofeh Azizi</option>
<option>Shelly Sheynin</option>
<option>Shen Nie</option>
<option>Shenda Hong</option>
<option>Sheng Yu</option>
<option>Sheng Zhao</option>
<option>Sheng Zheng</option>
<option>Sheng-Min Shih</option>
<option>Sheng-Yen Chou</option>
<option>Sheng-Yu Wang</option>
<option>ShengYun Peng</option>
<option>Shenghua Gao</option>
<option>Shengjia Zhao</option>
<option>Shengmeng Li</option>
<option>Shengming Yin</option>
<option>Shengqu Cai</option>
<option>Shengyong Chen</option>
<option>Shi Pu</option>
<option>ShiQi Cao</option>
<option>Shifan Zhao</option>
<option>Shifeng Chen</option>
<option>Shifeng Pan</option>
<option>Shigeo Morishima</option>
<option>Shihao Ji</option>
<option>Shihao Zhao</option>
<option>Shikai Fang</option>
<option>Shinei Arakawa</option>
<option>Shinji Watanabe</option>
<option>Shinkook Choi</option>
<option>Shiqi Sun</option>
<option>Shiqi Wang</option>
<option>Shiran Zada</option>
<option>Shirin Shoushtari</option>
<option>Shitong Luo</option>
<option>Shitong Shao</option>
<option>Shitong Xu</option>
<option>Shiu-hong Kao</option>
<option>Shiva Prasad Kasiviswanathan</option>
<option>Shivam Duggal</option>
<option>Shiyin Wang</option>
<option>Shiyu Chang</option>
<option>Shiyu Lu</option>
<option>Shizhan Gong</option>
<option>Shizhao Sun</option>
<option>Shogo Seki</option>
<option>Shoufa Chen</option>
<option>Shu Zhang</option>
<option>Shuai Shen</option>
<option>Shuai Wang</option>
<option>Shuaicheng Liu</option>
<option>Shuang Li</option>
<option>Shuangfei Zhai</option>
<option>Shubhajit Basak</option>
<option>Shubham Tulsiani</option>
<option>Shuguang Liu</option>
<option>Shuhan Zheng</option>
<option>Shuicheng Yan</option>
<option>Shuiwang Ji</option>
<option>Shunli Tian</option>
<option>Shunta Akiyama</option>
<option>Shuo Wang</option>
<option>Shusuke Takahashi</option>
<option>Shutong Wu</option>
<option>Shuyang Gu</option>
<option>Shuyao Shang</option>
<option>Siddharth Ramesh</option>
<option>Siddique Latif</option>
<option>Sidharth Kumar</option>
<option>Sieun Kim</option>
<option>Sifei Liu</option>
<option>Sigal Raab</option>
<option>Siheng Chen</option>
<option>Sihyun Yu</option>
<option>Silvan Peter</option>
<option>Silvio Savarese</option>
<option>Simon Alexanderson</option>
<option>Simon Coste</option>
<option>Simon Damm</option>
<option>Simon Kornblith</option>
<option>Simon Rouard</option>
<option>Simon Welker</option>
<option>Simona Vegetti</option>
<option>Simone Calderara</option>
<option>Simone Rossi</option>
<option>Singh</option>
<option>Sinho Chewi</option>
<option>Sinno Jialin Pan</option>
<option>Sirui Xie</option>
<option>Sitan Chen</option>
<option>Sitian Shen</option>
<option>Siwei Ma</option>
<option>Siyu Huang</option>
<option>Siyuan Hu</option>
<option>Siyuan Huang</option>
<option>Siyuan Mei</option>
<option>Soham De</option>
<option>Soheil Feizi</option>
<option>Sonal Gupta</option>
<option>Song Guo</option>
<option>Song Han</option>
<option>Song Liu</option>
<option>Song-Chun Zhu</option>
<option>Songchi Zhou</option>
<option>Songhui Diao</option>
<option>Songwei Ge</option>
<option>Songxiang Liu</option>
<option>Songyang Zhang</option>
<option>Songyou Peng</option>
<option>Sonia Chernova</option>
<option>Sonia Nielles-Vallespin</option>
<option>Soo-Whan Chung</option>
<option>Soon Yau Cheong</option>
<option>Sophia S. Goller</option>
<option>Sophie Mildenberger</option>
<option>Soroosh Tayebi Arasteh</option>
<option>Sotirios A. Tsaftaris</option>
<option>Sravanthi Parasa</option>
<option>Stan Birchfield</option>
<option>Stan Z. Li</option>
<option>Stavros Petridis</option>
<option>Stefan Bauer</option>
<option>Stefan Sommer</option>
<option>Stefano Ermon</option>
<option>Stefano Peluchetti</option>
<option>Stefanos Zafeiriou</option>
<option>Stephan Guennemann</option>
<option>Stephan Günnemann</option>
<option>Stephan Mandt</option>
<option>Stephane Mallat</option>
<option>Stephen Gould</option>
<option>Stephen Solis</option>
<option>Steve Jiang</option>
<option>Stewart He</option>
<option>Stuart James</option>
<option>Stylianos Moschoglou</option>
<option>Stéphane Courteau</option>
<option>Subin Kim</option>
<option>Sugato Basu</option>
<option>Suhyeon Lee</option>
<option>Suneel Belkhale</option>
<option>Sung Ju Hwang</option>
<option>Sung-Eui Yoon</option>
<option>Sung-Ho Bae</option>
<option>Sung-Lin Yeh</option>
<option>Sunghyun Park</option>
<option>Sungjin Ahn</option>
<option>Sungnyun Kim</option>
<option>Sungroh Yoon</option>
<option>Sungsoo Ahn</option>
<option>Sungwon Kim</option>
<option>Supasorn Suwajanakorn</option>
<option>Suraj Srinivas</option>
<option>Surgan Jandial</option>
<option>Surya Ganguli</option>
<option>Sushant Veer</option>
<option>Susung Hong</option>
<option>Suttisak Wizadwongsa</option>
<option>Sven Gowal</option>
<option>Sven Nebelung</option>
<option>Sven Wang</option>
<option>Sylvain Faisan</option>
<option>Sylvain Le Corff</option>
<option>Sébastien Valette</option>
<option>Tabish Rashid</option>
<option>Taco Cohen</option>
<option>Taegoo Kang</option>
<option>Taesun Yeom</option>
<option>Taesung Kwon</option>
<option>Taiji Suzuki</option>
<option>Takao Fukui</option>
<option>Takashi Shibuya</option>
<option>Takuhiro Kaneko</option>
<option>Takuma Yoneda</option>
<option>Takuro Kutsuna</option>
<option>Tal Kachman</option>
<option>Tal Peer</option>
<option>Tal Shaharbany</option>
<option>Tali Dekel</option>
<option>Tam V. Nguyen</option>
<option>Tamara Broderick</option>
<option>Tan Lee</option>
<option>Tao Chen</option>
<option>Tao Komikado</option>
<option>Tao Qin</option>
<option>Tao Wang</option>
<option>Tao Xiang</option>
<option>Tao Yang</option>
<option>Tasnima Sadekova</option>
<option>Tatsunori B. Hashimoto</option>
<option>Tatsuya Kawahara</option>
<option>Tengchao Lv</option>
<option>Tenglong Ao</option>
<option>Tengyu Liu</option>
<option>Teo Deveney</option>
<option>Teodora Reu</option>
<option>Tero Karras</option>
<option>Thanh Van Le</option>
<option>Thanh-Toan Do</option>
<option>Theodoros Pissas</option>
<option>Thibault Castells</option>
<option>Thibaut Issenhuth</option>
<option>Thomas A. Langlois</option>
<option>Thomas Altstidl</option>
<option>Thomas B. Schön</option>
<option>Thomas Hayes</option>
<option>Thomas L. Griffiths</option>
<option>Thomas Leung</option>
<option>Thomas Pock</option>
<option>Thomas Seror</option>
<option>Thomas Z. Li</option>
<option>Thorsten Holz</option>
<option>Thuan Hoang Nguyen</option>
<option>Thuy Tran</option>
<option>Tian Liu</option>
<option>Tian Xie</option>
<option>Tianci Wang</option>
<option>Tiange Xiang</option>
<option>Tianhe Ren</option>
<option>Tianjun Xiao</option>
<option>Tiankai Hang</option>
<option>Tianlin Xu</option>
<option>Tianrong Chen</option>
<option>Tianshi Cao</option>
<option>Tianshu Chu</option>
<option>Tiantian Yuan</option>
<option>Tianxiang Sun</option>
<option>Tianxin Huang</option>
<option>Tianyu Han</option>
<option>Tianyu Pang</option>
<option>Tianze Luo</option>
<option>Tie-Yan Liu</option>
<option>Tien-Tsin Wong</option>
<option>Tieniu Tan</option>
<option>Tijin Yan</option>
<option>Tim Brooks</option>
<option>Tim Dockhorn</option>
<option>Tim Pearce</option>
<option>Tim Salimans</option>
<option>Tim Schrabback</option>
<option>Tim Sinnecker</option>
<option>Timo Aila</option>
<option>Timo Gerkmann</option>
<option>Timo I. Denk</option>
<option>Timothy Hospedales</option>
<option>Ting Chen</option>
<option>Ting Hua</option>
<option>Ting Jiang</option>
<option>Ting Ma</option>
<option>Ting Zhang</option>
<option>Ting-Chun Wang</option>
<option>Ting-Hsuan Liao</option>
<option>Tingting Jiang</option>
<option>Tingyang Xu</option>
<option>Tinsu Pan</option>
<option>Titas Anciukevicius</option>
<option>Tobias Golling</option>
<option>Tobias Höppe</option>
<option>Tobias Weber</option>
<option>Toby P. Breckon</option>
<option>Tolga Çukur</option>
<option>Tom Blundell</option>
<option>Tom Goldstein</option>
<option>Tom Rainforth</option>
<option>Tomas Geffner</option>
<option>Tomasz Trzcinski</option>
<option>Tomasz Trzciński</option>
<option>Tomer Amit</option>
<option>Tomer Michaeli</option>
<option>Tommaso Biancalani</option>
<option>Tommi Jaakkola</option>
<option>Tong Che</option>
<option>Tong He</option>
<option>Tong Lu</option>
<option>Tong Wu</option>
<option>Tong Zhou</option>
<option>Tong-Yee Lee</option>
<option>Tonghe Wang</option>
<option>Torben Peters</option>
<option>Toshimitsu Uesaka</option>
<option>Towaki Takikawa</option>
<option>Tran Minh Quan</option>
<option>Trevor Darrell</option>
<option>Tristan S. W. Stevens</option>
<option>Troy Luhman</option>
<option>Trung X. Pham</option>
<option>Trung-Nghia Le</option>
<option>Tsachi Blau</option>
<option>Tsai-Shien Chen</option>
<option>Tsu-Jui Fu</option>
<option>Tsung-Yi Ho</option>
<option>Tsung-Yi Lin</option>
<option>Tu Bui</option>
<option>Tucker Hermans</option>
<option>Tyler Maunu</option>
<option>Tyler Poon</option>
<option>Ulrich Hamann</option>
<option>Ulugbek S. Kamilov</option>
<option>Uriel Singer</option>
<option>Urs Germann</option>
<option>Vaden Masrani</option>
<option>Vadim Popov</option>
<option>Vahram Tadevosyan</option>
<option>Vajira Thambawita</option>
<option>Valentin De Bortoli</option>
<option>Valentin Khrulkov</option>
<option>Valentyn Boreiko</option>
<option>Varsha Kishore</option>
<option>Varun Jampani</option>
<option>Vasu Singla</option>
<option>Vasudev Lal</option>
<option>Vedant Singh</option>
<option>Venkatasubramanian Viswanathan</option>
<option>Vesa Välimäki</option>
<option>Victor Garcia Satorras</option>
<option>Victor Lempitsky</option>
<option>Victor Ostromoukhov</option>
<option>Vidit Goel</option>
<option>Vignesh Ram Somnath</option>
<option>Vikash Sehwag</option>
<option>Vikram Voleti</option>
<option>Vincent Christlein</option>
<option>Vincent Dutordoir</option>
<option>Vincent J. Hellendoorn</option>
<option>Vincent Tao Hu</option>
<option>Vineeth N. Balasubramanian</option>
<option>Vinh Khuc</option>
<option>Vinicius Mikuni</option>
<option>Virginia Fernandez</option>
<option>Vishal M Patel</option>
<option>Vishal M. Patel</option>
<option>Vishal Patel</option>
<option>Vishnu Sarukkai</option>
<option>Vitalis Vosylius</option>
<option>Vivek Jayaram</option>
<option>Vlad Hondru</option>
<option>Vladimir Arkhipkin</option>
<option>Vladimir Gogoryan</option>
<option>Vladimir Kulikov</option>
<option>Vladislav Golyanik</option>
<option>Volkan Cevher</option>
<option>Vésteinn Snæbjarnarson</option>
<option>W. Bastiaan Kleijn</option>
<option>WaiChing Sun</option>
<option>Walter H. L. Pinaya</option>
<option>Walter Talbott</option>
<option>Wan-Cyuan Fan</option>
<option>Wan-Duo Kurt Ma</option>
<option>Wanhua Li</option>
<option>Wanli Xue</option>
<option>Wanmo Kang</option>
<option>Wayne Xin Zhao</option>
<option>Wei Chen</option>
<option>Wei Deng</option>
<option>Wei Han</option>
<option>Wei Hu</option>
<option>Wei Liang</option>
<option>Wei Liu</option>
<option>Wei Ping</option>
<option>Wei Tian</option>
<option>Wei Ye</option>
<option>Wei-Fang Sun</option>
<option>Weibo Mao</option>
<option>Weidi Xie</option>
<option>Weifeng Chen</option>
<option>Weifeng Lv</option>
<option>Weihao Cheng</option>
<option>Weihao Yu</option>
<option>Weihua Li</option>
<option>Weilai Xiang</option>
<option>Weili Nie</option>
<option>Weilun Wang</option>
<option>Weimin Wang</option>
<option>Weiming Dong</option>
<option>Weining Qian</option>
<option>Weiqiang Wang</option>
<option>Weitao Du</option>
<option>Weiwei Liu</option>
<option>Weiwei Lv</option>
<option>Weiwen Wu</option>
<option>Weixi Feng</option>
<option>Weixin Chen</option>
<option>Weiyang Liu</option>
<option>Weiyu Liu</option>
<option>Weizhu Chen</option>
<option>Wele Gedara Chaminda Bandara</option>
<option>Wen Gao</option>
<option>Wen Liu</option>
<option>Wenao Ma</option>
<option>Wenbo Gong</option>
<option>Wengang Zhou</option>
<option>Wenhan Luo</option>
<option>Wenhan Yang</option>
<option>Wenhao Li</option>
<option>Wenhu Chen</option>
<option>Wenjing Wang</option>
<option>Wenjing Yang</option>
<option>Wenjun Xia</option>
<option>Wenkang Shan</option>
<option>Wenliang Zhao</option>
<option>Wenming Yang</option>
<option>Wenqi Fan</option>
<option>Wenqian Zhang</option>
<option>Wenqiang Xu</option>
<option>Wentao Hu</option>
<option>Wentao Zhang</option>
<option>Wentao Zhao</option>
<option>Wenwu Wang</option>
<option>Wenxiang Cong</option>
<option>Wenxuan Li</option>
<option>Wenzhe Li</option>
<option>Wesley Maddox</option>
<option>Wesley Wei Qian</option>
<option>Will Grathwohl</option>
<option>Will Saxton</option>
<option>William Chan</option>
<option>William Harvey</option>
<option>William Peebles</option>
<option>William T. Freeman</option>
<option>William W. Cohen</option>
<option>William Wang</option>
<option>William Yang Wang</option>
<option>Wing Yee Au</option>
<option>Wonjae Kim</option>
<option>Wonwoong Cho</option>
<option>Woobin Im</option>
<option>Woohyung Lim</option>
<option>Wooseok Jang</option>
<option>Wuhao Wang</option>
<option>Wuyue Lu</option>
<option>Xi Chen</option>
<option>Xi Li</option>
<option>Xi Yin</option>
<option>Xianbiao Qi</option>
<option>Xianchao Wu</option>
<option>Xianda Guo</option>
<option>Xianfan Gu</option>
<option>Xiang Fu</option>
<option>Xiang Li</option>
<option>Xiang Lisa Li</option>
<option>Xiang Yin</option>
<option>Xiangfeng Wang</option>
<option>Xianghao Kong</option>
<option>Xiangming Meng</option>
<option>Xiangqiao Meng</option>
<option>Xiangrong Zhang</option>
<option>Xiangxi Meng</option>
<option>Xiangyang Ji</option>
<option>Xiangyong Cao</option>
<option>Xiangyu Rui</option>
<option>Xianjun Yang</option>
<option>Xianpan Zhou</option>
<option>Xiao Liang</option>
<option>Xiao Liu</option>
<option>Xiao Wang</option>
<option>Xiao Wu</option>
<option>Xiao Yang</option>
<option>Xiaochuang Han</option>
<option>Xiaodan Du</option>
<option>Xiaodong Lin</option>
<option>Xiaodong Wang</option>
<option>Xiaofeng Yang</option>
<option>Xiaohan Yuan</option>
<option>Xiaohu Qie</option>
<option>Xiaohui Chen</option>
<option>Xiaohui Hu</option>
<option>Xiaohui Zeng</option>
<option>Xiaojian Ma</option>
<option>Xiaojun Tang</option>
<option>Xiaojun Yang</option>
<option>Xiaokun Liang</option>
<option>Xiaolin Wei</option>
<option>Xiaoming Wei</option>
<option>Xiaoning Qian</option>
<option>Xiaoshuang Shi</option>
<option>Xiaoting Zhao</option>
<option>Xiaoyan Sun</option>
<option>Xiaoying Tang</option>
<option>Xiaoyu Yang</option>
<option>Xiaoyuan Yi</option>
<option>Xiaoyue Li</option>
<option>Xiaoyun Zhang</option>
<option>Xiatian Zhu</option>
<option>Xie Chen</option>
<option>Xihui Liu</option>
<option>Xin Chen</option>
<option>Xin Eric Wang</option>
<option>Xin Geng</option>
<option>Xin Lin</option>
<option>Xin Xia</option>
<option>Xin Yuan</option>
<option>Xin Zhou</option>
<option>Xinchao Wang</option>
<option>Xinchen Yan</option>
<option>Xinfeng Zhang</option>
<option>Xing Zhang</option>
<option>Xing Zheng</option>
<option>Xingang Peng</option>
<option>Xingang Wang</option>
<option>Xingchao Liu</option>
<option>Xingjian Shi</option>
<option>Xinglong Wu</option>
<option>Xingqian Xu</option>
<option>Xingyi Yang</option>
<option>Xinhang Liu</option>
<option>Xinhao Mei</option>
<option>Xinjiang Lu</option>
<option>Xinlei He</option>
<option>Xinlei Pan</option>
<option>Xintao Wang</option>
<option>Xinwei Zhang</option>
<option>Xinxiao Wu</option>
<option>Xinyi Yang</option>
<option>Xinyin Ma</option>
<option>Xinying Guo</option>
<option>Xinyuan Chen</option>
<option>Xinyue Shen</option>
<option>Xipeng Qiu</option>
<option>Xiulong Yang</option>
<option>Xiuming Zhang</option>
<option>Xiurong Jiang</option>
<option>Xiuyu Li</option>
<option>Xize Cheng</option>
<option>Xizewen Han</option>
<option>Xu Han</option>
<option>Xu Tan</option>
<option>Xuan Lin</option>
<option>Xuan Liu</option>
<option>Xuan Su</option>
<option>Xuaner Zhang</option>
<option>Xuanjing Huang</option>
<option>Xuansong xie</option>
<option>Xubo Liu</option>
<option>Xudong XU</option>
<option>Xudong Xu</option>
<option>Xue Bin Peng</option>
<option>Xue-Jing Luo</option>
<option>Xuefeng Xiao</option>
<option>Xuehai He</option>
<option>Xuejin Chen</option>
<option>Xueting Li</option>
<option>Xueting Liu</option>
<option>Xuewei Li</option>
<option>Xuming He</option>
<option>Xun Huang</option>
<option>Xutao Guo</option>
<option>Ya Zhang</option>
<option>Ya-Ping Hsieh</option>
<option>Yadong Lu</option>
<option>Yael Pritch</option>
<option>Yael Vinker</option>
<option>Yan Huang</option>
<option>Yan Li</option>
<option>Yan Lv</option>
<option>Yan Yan</option>
<option>Yan-Pei Cao</option>
<option>Yanfeng Wang</option>
<option>Yang Cui</option>
<option>Yang Gao</option>
<option>Yang Li</option>
<option>Yang Song</option>
<option>Yang Wang</option>
<option>Yang Xiang</option>
<option>Yang Zhang</option>
<option>Yanghao Li</option>
<option>Yaniv Benny</option>
<option>Yaniv Leviathan</option>
<option>Yaniv Nikankin</option>
<option>Yaniv Taigman</option>
<option>Yanjie Zhu</option>
<option>Yanlei Zhang</option>
<option>Yanli Wang</option>
<option>Yannik Glaser</option>
<option>Yanning Zhang</option>
<option>Yanwu Xu</option>
<option>Yanwu Yang</option>
<option>Yanze Wu</option>
<option>Yao Du</option>
<option>Yao Feng</option>
<option>Yao Mu</option>
<option>Yao Zhao</option>
<option>Yao-Chih Lee</option>
<option>Yaohui Li</option>
<option>Yaohui Wang</option>
<option>Yaole Wang</option>
<option>Yaoqin Xie</option>
<option>Yapeng Tian</option>
<option>Yaqing Wang</option>
<option>Yaron Lipman</option>
<option>Yaroslav Ganin</option>
<option>Yasiru Ranasinghe</option>
<option>Yasushi Esaki</option>
<option>Yasutaka Furukawa</option>
<option>Yawar Siddiqui</option>
<option>Ye Mao</option>
<option>Ye Yuan</option>
<option>Ye Zhu</option>
<option>Yebin Liu</option>
<option>Yedid Hoshen</option>
<option>Yee H. Mah</option>
<option>Yee Whye Teh</option>
<option>Yehjin Shin</option>
<option>Yehui Yang</option>
<option>Yelong Shen</option>
<option>Yen Ting Lin</option>
<option>Yen-Chang Hsu</option>
<option>Yen-Chun Chen</option>
<option>Yen-Ju Lu</option>
<option>Yeongmin Kim</option>
<option>Yesom Park</option>
<option>Yeying Jin</option>
<option>Yeyun Gong</option>
<option>Yi Huang</option>
<option>Yi Liu</option>
<option>Yi Ren</option>
<option>Yi Yang</option>
<option>Yi Yuan</option>
<option>Yi-Chen Lo</option>
<option>Yi-Wen Liu</option>
<option>Yi-Zhe Song</option>
<option>Yibing Song</option>
<option>Yichong Leng</option>
<option>Yidong Ouyang</option>
<option>Yifan Jiang</option>
<option>Yifei Zhang</option>
<option>Yifeng Qin</option>
<option>Yihan Wu</option>
<option>Yihang Wang</option>
<option>Yihao Chen</option>
<option>Yihao Feng</option>
<option>Yijiang Liu</option>
<option>Yijin Huang</option>
<option>Yijun Yang</option>
<option>Yikai Wang</option>
<option>Yikai Zhang</option>
<option>Yilin Shen</option>
<option>Yilmaz Korkmaz</option>
<option>Yilun Du</option>
<option>Yilun Xu</option>
<option>Yiming Qin</option>
<option>Yiming Wang</option>
<option>Yin Zhou</option>
<option>Yin-Ping Cho</option>
<option>Ying Fan</option>
<option>Ying Nian Wu</option>
<option>Ying Shan</option>
<option>Ying Tai</option>
<option>Yingbo Zhou</option>
<option>Yingya Zhang</option>
<option>Yingyu Yang</option>
<option>Yingzhen Li</option>
<option>Yinhai Wang</option>
<option>Yinhuai Wang</option>
<option>Yinlin Fu</option>
<option>Yinpeng Dong</option>
<option>Yinxiao Li</option>
<option>Yinyu Nie</option>
<option>Yipeng Hu</option>
<option>Yiqiang Zhan</option>
<option>Yiqun Duan</option>
<option>Yiran Xu</option>
<option>Yiting Qu</option>
<option>Yitong Wang</option>
<option>Yiwei Guo</option>
<option>Yiwen Li</option>
<option>Yixin Tan</option>
<option>Yixin Zhu</option>
<option>Yixuan He</option>
<option>Yiyang Ma</option>
<option>Yiyang Shen</option>
<option>Yizhe Zhang</option>
<option>Yizhe Zhu</option>
<option>Yizhi Wang</option>
<option>Yizhuo Lu</option>
<option>Yogesh Balaji</option>
<option>Yonatan Shafir</option>
<option>Yong Liu</option>
<option>Yong Zhong</option>
<option>Yonghyun Jeong</option>
<option>Yongkang Li</option>
<option>Yonglong Tian</option>
<option>Yongming Rao</option>
<option>Yongping Li</option>
<option>Yongping Xiong</option>
<option>Yongsheng Pan</option>
<option>Yongxin Chen</option>
<option>Yongxin Yang</option>
<option>Yongxin Zhu</option>
<option>Yongzhen Wang</option>
<option>Yoni Choukroun</option>
<option>Yoohoon Kang</option>
<option>Yoshiyuki Kabashima</option>
<option>Yoshua Bengio</option>
<option>Yossi Adi</option>
<option>Yossi Matias</option>
<option>Yotaro Katayama</option>
<option>Yotaro Watanabe</option>
<option>You Zhang</option>
<option>Youfang Lin</option>
<option>Youngjune Gwon</option>
<option>Youngjung Uh</option>
<option>Youngseo Kim</option>
<option>Youssef Marzouk</option>
<option>Yu Bao</option>
<option>Yu Cao</option>
<option>Yu Chen</option>
<option>Yu Cheng</option>
<option>Yu Dong</option>
<option>Yu Liu</option>
<option>Yu Qiao</option>
<option>Yu Tsao</option>
<option>Yu Wang</option>
<option>Yu Wu</option>
<option>Yu Zhang</option>
<option>Yu Zhu</option>
<option>Yu-Chiang Frank Wang</option>
<option>Yu-Gang Jiang</option>
<option>Yu-Guan Hsieh</option>
<option>Yu-Hui Chen</option>
<option>Yu-Lin Chang</option>
<option>Yu-Lun Liu</option>
<option>Yu-Wing Tai</option>
<option>Yuan Yao</option>
<option>Yuan Yuan</option>
<option>Yuanfeng Ji</option>
<option>Yuankai Huo</option>
<option>Yuankai K. Tao</option>
<option>Yuanqi Du</option>
<option>Yuanqing Xia</option>
<option>Yuanyuan Zhu</option>
<option>Yuanzhe Xi</option>
<option>Yuanzhen Li</option>
<option>Yuanzhi Li</option>
<option>Yuanzhi Zhu</option>
<option>Yubin Lu</option>
<option>Yuchun Miao</option>
<option>Yue Cao</option>
<option>Yue Deng</option>
<option>Yue Zhao</option>
<option>Yueqi Duan</option>
<option>Yueqin Yin</option>
<option>Yuewen Cao</option>
<option>Yufan Zhou</option>
<option>Yufei Wang</option>
<option>Yufei Ye</option>
<option>Yufeng Su</option>
<option>Yufeng Zhan</option>
<option>Yuhao Zhou</option>
<option>Yuhta Takida</option>
<option>Yuhuan Yang</option>
<option>Yuichiro Koyama</option>
<option>Yujia Huang</option>
<option>Yujie Lu</option>
<option>Yujin Oh</option>
<option>Yujiu Yang</option>
<option>Yujun Shen</option>
<option>Yukai Shi</option>
<option>Yukang Cao</option>
<option>Yuki Koyama</option>
<option>Yuki M. Asano</option>
<option>Yuki Mitsufuji</option>
<option>Yukun Li</option>
<option>Yulia Tsvetkov</option>
<option>Yuling Jiao</option>
<option>Yulun Zhang</option>
<option>Yuma Koizumi</option>
<option>Yuming Jiang</option>
<option>Yun Cheng</option>
<option>Yun Fu</option>
<option>Yun Ye</option>
<option>Yuna Jung</option>
<option>Yunguan Fu</option>
<option>Yunho Kim</option>
<option>Yunhong Wang</option>
<option>Yuning Gu</option>
<option>Yunjey Choi</option>
<option>Yunji Kim</option>
<option>Yunqing Liu</option>
<option>Yunqing Zhao</option>
<option>Yuntian Deng</option>
<option>Yuntian Gu</option>
<option>Yunxiang Li</option>
<option>Yupan Huang</option>
<option>Yupei Lin</option>
<option>Yuqi Gong</option>
<option>Yuqi Yang</option>
<option>Yuriy Nevmyvaka</option>
<option>Yusheng Tian</option>
<option>Yusuke Hatanaka</option>
<option>Yusuke Tashiro</option>
<option>Yutong Chen</option>
<option>Yutong He</option>
<option>Yutong Xia</option>
<option>Yutong Xie</option>
<option>Yuval Alaluf</option>
<option>Yuval Bahat</option>
<option>Yuval Dagan</option>
<option>Yuwang Wang</option>
<option>Yuxi Zhou</option>
<option>Yuxiao Chen</option>
<option>Yuxiao Hu</option>
<option>Yuxin Zhang</option>
<option>Yuxiong He</option>
<option>Yuxuan Li</option>
<option>Yuxuan Liang</option>
<option>Yuyang Shi</option>
<option>Yuyin Zhou</option>
<option>Yuzhang Shang</option>
<option>Z. Morley Mao</option>
<option>Zaccharie Ramzi</option>
<option>Zachary Ankner</option>
<option>Zachary R. Fox</option>
<option>Zahra Sepasdar</option>
<option>Zaixiang Zheng</option>
<option>Zalan Fabian</option>
<option>Zan Gojcic</option>
<option>Zan Wang</option>
<option>Zander Blasingame</option>
<option>Ze Wang</option>
<option>Zebin You</option>
<option>Zehua Chen</option>
<option>Zekai Wang</option>
<option>Zekang Chen</option>
<option>Zekun Hao</option>
<option>Zenghao Chai</option>
<option>Zerong Zheng</option>
<option>Zesen Cheng</option>
<option>Zexian Li</option>
<option>Zexiang Xu</option>
<option>Zeyang Sha</option>
<option>Zeyi Zhang</option>
<option>Zeyu Lu</option>
<option>Zeyu Zhu</option>
<option>Zezhou Zhang</option>
<option>Zhanfeng Mo</option>
<option>Zhangxuan Gu</option>
<option>Zhangyang Gao</option>
<option>Zhangyang Wang</option>
<option>Zhantao Yang</option>
<option>Zhao Wang</option>
<option>Zhaohu Xing</option>
<option>Zhaoqiang Liu</option>
<option>Zhaoyang Lyu</option>
<option>Zhe Chen</option>
<option>Zhen Dong</option>
<option>Zhen Huang</option>
<option>Zhen Liu</option>
<option>Zhenan Sun</option>
<option>Zhenda Xu</option>
<option>Zhendong Wang</option>
<option>Zheng Chen</option>
<option>Zheng Ding</option>
<option>Zheng Li</option>
<option>Zheng Wang</option>
<option>Zheng Zhao</option>
<option>Zheng Zhu</option>
<option>Zhengcong Fei</option>
<option>Zhengdong Zhang</option>
<option>Zhengfu He</option>
<option>Zhenghao Lin</option>
<option>Zhenguo Li</option>
<option>Zhengxiong Luo</option>
<option>Zhengyang Geng</option>
<option>Zhengyang Shan</option>
<option>Zhengyi Luo</option>
<option>Zhengyi Wang</option>
<option>Zhengyuan Yang</option>
<option>Zhenhua Liu</option>
<option>Zhenhui Ye</option>
<option>Zhentao Yu</option>
<option>Zhenzhen Weng</option>
<option>Zhewei Yao</option>
<option>Zhi Zhong</option>
<option>Zhiding Yu</option>
<option>Zhifeng Chen</option>
<option>Zhifeng Kong</option>
<option>Zhihang Yuan</option>
<option>Zhihao Fan</option>
<option>Zhihao Xia</option>
<option>Zhihong Pan</option>
<option>Zhijian Ou</option>
<option>Zhijie Deng</option>
<option>Zhijie Lin</option>
<option>Zhijing Jin</option>
<option>Zhijun Liu</option>
<option>Zhili Liu</option>
<option>Zhilin Huang</option>
<option>Zhilong Zhang</option>
<option>Zhipeng Fan</option>
<option>Zhiqiang Tan</option>
<option>Zhisheng Xiao</option>
<option>Zhishuai Zhang</option>
<option>Zhixin Wang</option>
<option>Zhixing Zhang</option>
<option>Zhixuan Liang</option>
<option>Zhixun Su</option>
<option>Zhiye Guo</option>
<option>Zhiying Jiang</option>
<option>Zhiyong Wu</option>
<option>Zhiyuan Liu</option>
<option>Zhiyuan Ren</option>
<option>Zhizhuo Zhou</option>
<option>Zhong-Qiu Wang</option>
<option>Zhongang Cai</option>
<option>Zhongang Qi</option>
<option>Zhongjian Wang</option>
<option>Zhongjie Duan</option>
<option>Zhongyu Wei</option>
<option>Zhou Zhao</option>
<option>Zhujin Gao</option>
<option>Zhuo-Xu Cui</option>
<option>Zhuoer Xu</option>
<option>Zhuowen Tu</option>
<option>Zhuoyang Chen</option>
<option>ZiHan Cao</option>
<option>Zibin Meng</option>
<option>Zicheng Liu</option>
<option>Zico Kolter</option>
<option>Zihao Li</option>
<option>Zihao Wang</option>
<option>Zijian Zhang</option>
<option>Zijie J. Wang</option>
<option>Zilin Zhu</option>
<option>Zilong Huang</option>
<option>Ziming Liu</option>
<option>Ziming Qiu</option>
<option>Zineng Tang</option>
<option>Ziqi Huang</option>
<option>Ziqi Liu</option>
<option>Ziwei Liu</option>
<option>Ziwei Luo</option>
<option>Zixin Yin</option>
<option>Ziya Erkoç</option>
<option>Ziyi Chang</option>
<option>Ziyi Li</option>
<option>Ziyi Wu</option>
<option>Ziyi Yang</option>
<option>Ziying Zhang</option>
<option>Ziyuan Zhong</option>
<option>Zohya Khalique</option>
<option>Zongsheng Yue</option>
<option>Zongwei Wu</option>
<option>Zongyuan Yang</option>
<option>Zoubin Ghahramani</option>
<option>Zsolt Kira</option>
<option>Zubair Shah</option>
<option>Zuopeng Yang</option>
<option>Zuxuan Wu</option>
<option>Zuyan Liu</option>
<option>and Ge Yang</option>
<option>Álvaro Barbero Jiménez</option>
<option>Özgür Yaldizli</option>
<option>Şaban Özturk</option>
<option>Şaban Öztürk</option>
    </select>
  </span>
  <div class="selector">
    <span>Select Year: </span>
    <select id="yearselection" class="selectpicker" multiple data-live-search="true">
      <option>2015</option>
<option>2019</option>
<option>2020</option>
<option>2021</option>
<option>2022</option>
<option>2023</option>
    </select>
  </div>
</div>

{% assign id = 0 %}
 <hr/>
 

<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 26, 2023 </span>    
         <span class="authors"> Gongye Liu, Haoze Sun, Jiayi Li, Fei Yin, Yujiu Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16965" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models have demonstrated a remarkable ability to solve
inverse problems in an unsupervised manner. Existing methods mainly focus on
modifying the posterior sampling process while neglecting the potential of the
forward process. In this work, we propose Shortcut Sampling for Diffusion
(SSD), a novel pipeline for solving inverse problems. Instead of initiating
from random noise, the key concept of SSD is to find the "Embryo", a
transitional state that bridges the measurement image y and the restored image
x. By utilizing the "shortcut" path of "input-Embryo-output", SSD can achieve
precise and fast restoration. To obtain the Embryo in the forward process, We
propose Distortion Adaptive Inversion (DA Inversion). Moreover, we apply back
projection and attention injection as additional consistency constraints during
the generation process. Experimentally, we demonstrate the effectiveness of SSD
on several representative tasks, including super-resolution, deblurring, and
colorization. Compared to state-of-the-art zero-shot methods, our method
achieves competitive results with only 30 NFEs. Moreover, SSD with 100 NFEs can
outperform state-of-the-art zero-shot methods in certain tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Error Bounds for Flow Matching Methods
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 26, 2023 </span>    
         <span class="authors"> Joe Benton, George Deligiannidis, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16860" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models are a popular class of generative modelling
techniques relying on stochastic differential equations (SDE). From their
inception, it was realized that it was also possible to perform generation
using ordinary differential equations (ODE) rather than SDE. This led to the
introduction of the probability flow ODE approach and denoising diffusion
implicit models. Flow matching methods have recently further extended these
ODE-based approaches and approximate a flow between two arbitrary probability
distributions. Previous work derived bounds on the approximation error of
diffusion models under the stochastic sampling regime, given assumptions on the
$L^2$ loss. We present error bounds for the flow matching procedure using fully
deterministic sampling, assuming an $L^2$ bound on the approximation error and
a certain regularity condition on the data distributions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 26, 2023 </span>    
         <span class="authors"> Xiang Li, Songxiang Liu, Max W. Y. Lam, Zhiyong Wu, Chao Weng, Helen Meng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16749" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Expressive human speech generally abounds with rich and flexible speech
prosody variations. The speech prosody predictors in existing expressive speech
synthesis methods mostly produce deterministic predictions, which are learned
by directly minimizing the norm of prosody prediction error. Its unimodal
nature leads to a mismatch with ground truth distribution and harms the model's
ability in making diverse predictions. Thus, we propose a novel prosody
predictor based on the denoising diffusion probabilistic model to take
advantage of its high-quality generative modeling and training stability.
Experiment results confirm that the proposed prosody predictor outperforms the
deterministic baseline on both the expressiveness and diversity of prediction
results with even fewer network parameters.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Tree-Based Diffusion Schrödinger Bridge with Applications to Wasserstein Barycenters
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 26, 2023 </span>    
         <span class="authors"> Maxence Noble, Valentin De Bortoli, Arnaud Doucet, Alain Durmus </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16557" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, math.PR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at
minimizing the integral of a cost function with respect to a distribution with
some prescribed marginals. In this paper, we consider an entropic version of
mOT with a tree-structured quadratic cost, i.e., a function that can be written
as a sum of pairwise cost functions between the nodes of a tree. To address
this problem, we develop Tree-based Diffusion Schr\"odinger Bridge (TreeDSB),
an extension of the Diffusion Schr\"odinger Bridge (DSB) algorithm. TreeDSB
corresponds to a dynamic and continuous state-space counterpart of the
multimarginal Sinkhorn algorithm. A notable use case of our methodology is to
compute Wasserstein barycenters which can be recast as the solution of a mOT
problem on a star-shaped tree. We demonstrate that our methodology can be
applied in high-dimensional settings such as image interpolation and Bayesian
fusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Diffusion Models for Bayesian Image Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Michael T. McCann, Hyungjin Chung, Jong Chul Ye, Marc L. Klasky </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16482" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper explores the use of score-based diffusion models for Bayesian
image reconstruction. Diffusion models are an efficient tool for generative
modeling. Diffusion models can also be used for solving image reconstruction
problems. We present a simple and flexible algorithm for training a diffusion
model and using it for maximum a posteriori reconstruction, minimum mean square
error reconstruction, and posterior sampling. We present experiments on both a
linear and a nonlinear reconstruction problem that highlight the strengths and
limitations of the approach.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, Kwan-Yee K. Wong </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16322" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-Image diffusion models have made tremendous progress over the past
two years, enabling the generation of highly realistic images based on
open-domain text descriptions. However, despite their success, text
descriptions often struggle to adequately convey detailed controls, even when
composed of long and complex texts. Moreover, recent studies have also shown
that these models face challenges in understanding such complex texts and
generating the corresponding images. Therefore, there is a growing need to
enable more control modes beyond text description. In this paper, we introduce
Uni-ControlNet, a novel approach that allows for the simultaneous utilization
of different local controls (e.g., edge maps, depth map, segmentation masks)
and global controls (e.g., CLIP image embeddings) in a flexible and composable
manner within one model. Unlike existing methods, Uni-ControlNet only requires
the fine-tuning of two additional adapters upon frozen pre-trained
text-to-image diffusion models, eliminating the huge cost of training from
scratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNet
only necessitates a constant number (i.e., 2) of adapters, regardless of the
number of local or global controls used. This not only reduces the fine-tuning
costs and model size, making it more suitable for real-world deployment, but
also facilitate composability of different conditions. Through both
quantitative and qualitative comparisons, Uni-ControlNet demonstrates its
superiority over existing methods in terms of controllability, generation
quality and composability. Code is available at
\url{https://github.com/ShihaoZhaoZSH/Uni-ControlNet}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Parallel Sampling of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16317" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are powerful generative models but suffer from slow
sampling, often taking 1000 sequential denoising steps for one sample. As a
result, considerable efforts have been directed toward reducing the number of
denoising steps, but these methods hurt sample quality. Instead of reducing the
number of denoising steps (trading quality for speed), in this paper we explore
an orthogonal approach: can we run the denoising steps in parallel (trading
compute for speed)? In spite of the sequential nature of the denoising steps,
we show that surprisingly it is possible to parallelize sampling via Picard
iterations, by guessing the solution of future denoising steps and iteratively
refining until convergence. With this insight, we present ParaDiGMS, a novel
method to accelerate the sampling of pretrained diffusion models by denoising
multiple steps in parallel. ParaDiGMS is the first diffusion sampling method
that enables trading compute for speed and is even compatible with existing
fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we
improve sampling speed by 2-4x across a range of robotics and image generation
models, giving state-of-the-art sampling speeds of 0.2s on 100-step
DiffusionPolicy and 16s on 1000-step StableDiffusion-v2 with no measurable
degradation of task reward, FID score, or CLIP score.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## UDPM: Upsampling Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Shady Abu-Hussein, Raja Giryes </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16269" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In recent years, Denoising Diffusion Probabilistic Models (DDPM) have caught
significant attention. By composing a Markovian process that starts in the data
domain and then gradually adds noise until reaching pure white noise, they
achieve superior performance in learning data distributions. Yet, these models
require a large number of diffusion steps to produce aesthetically pleasing
samples, which is inefficient. In addition, unlike common generative
adversarial networks, the latent space of diffusion models is not
interpretable. In this work, we propose to generalize the denoising diffusion
process into an Upsampling Diffusion Probabilistic Model (UDPM), in which we
reduce the latent variable dimension in addition to the traditional noise level
addition. As a result, we are able to sample images of size $256\times 256$
with only 7 diffusion steps, which is less than two orders of magnitude
compared to standard DDPMs. We formally develop the Markovian diffusion
processes of the UDPM, and demonstrate its generation capabilities on the
popular FFHQ, LSUN horses, ImageNet, and AFHQv2 datasets. Another favorable
property of UDPM is that it is very easy to interpolate its latent space, which
is not the case with standard diffusion models. Our code is available online
\url{https://github.com/shadyabh/UDPM}
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Trans-Dimensional Generative Modeling via Jump Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Andrew Campbell, William Harvey, Christian Weilbach, Valentin De Bortoli, Tom Rainforth, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16261" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a new class of generative models that naturally handle data of
varying dimensionality by jointly modeling the state and dimension of each
datapoint. The generative process is formulated as a jump diffusion process
that makes jumps between different dimensional spaces. We first define a
dimension destroying forward noising process, before deriving the dimension
creating time-reversed generative process along with a novel evidence lower
bound training objective for learning to approximate it. Simulating our learned
approximation to the time-reversed generative process then provides an
effective way of sampling data of varying dimensionality by jointly generating
state values and dimensions. We demonstrate our approach on molecular and video
datasets of varying dimensionality, reporting better compatibility with
test-time diffusion guidance imputation tasks and improved interpolation
capabilities versus fixed dimensional models that generate state values and
dimensions separately.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, Humphrey Shi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16223" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-image (T2I) research has grown explosively in the past year, owing to
the large-scale pre-trained diffusion models and many emerging personalization
and editing approaches. Yet, one pain point persists: the text prompt
engineering, and searching high-quality text prompts for customized results is
more art than science. Moreover, as commonly argued: "an image is worth a
thousand words" - the attempt to describe a desired image with texts often ends
up being ambiguous and cannot comprehensively cover delicate visual details,
hence necessitating more additional controls from the visual domain. In this
paper, we take a bold step forward: taking "Text" out of a pre-trained T2I
diffusion model, to reduce the burdensome prompt engineering efforts for users.
Our proposed framework, Prompt-Free Diffusion, relies on only visual inputs to
generate new images: it takes a reference image as "context", an optional image
structural conditioning, and an initial noise, with absolutely no text prompt.
The core architecture behind the scene is Semantic Context Encoder (SeeCoder),
substituting the commonly used CLIP-based or LLM-based text encoder. The
reusability of SeeCoder also makes it a convenient drop-in component: one can
also pre-train a SeeCoder in one T2I model and reuse it for another. Through
extensive experiments, Prompt-Free Diffusion is experimentally found to (i)
outperform prior exemplar-based image synthesis approaches; (ii) perform on par
with state-of-the-art T2I models using prompts following the best practice; and
(iii) be naturally extensible to other downstream applications such as anime
figure generation and virtual try-on, with promising quality. Our code and
models are open-sourced at https://github.com/SHI-Labs/Prompt-Free-Diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16213" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score distillation sampling (SDS) has shown great promise in text-to-3D
generation by distilling pretrained large-scale text-to-image diffusion models,
but suffers from over-saturation, over-smoothing, and low-diversity problems.
In this work, we propose to model the 3D parameter as a random variable instead
of a constant as in SDS and present variational score distillation (VSD), a
principled particle-based variational framework to explain and address the
aforementioned issues in text-to-3D generation. We show that SDS is a special
case of VSD and leads to poor samples with both small and large CFG weights. In
comparison, VSD works well with various CFG weights as ancestral sampling from
diffusion models and simultaneously improves the diversity and sample quality
with a common CFG weight (i.e., $7.5$). We further present various improvements
in the design space for text-to-3D such as distillation time schedule and
density initialization, which are orthogonal to the distillation algorithm yet
not well explored. Our overall approach, dubbed ProlificDreamer, can generate
high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with
rich structure and complex effects (e.g., smoke and drops). Further,
initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and
photo-realistic. Project page: https://ml.cs.tsinghua.edu.cn/prolificdreamer/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unifying GANs and Score-Based Diffusion as Generative Particle Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bézenac, Mickaël Chen, Alain Rakotomamonjy </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.16150" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.NE, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Particle-based deep generative models, such as gradient flows and score-based
diffusion models, have recently gained traction thanks to their striking
performance. Their principle of displacing particle distributions by
differential equations is conventionally seen as opposed to the previously
widespread generative adversarial networks (GANs), which involve training a
pushforward generator network. In this paper, we challenge this interpretation
and propose a novel framework that unifies particle and adversarial generative
models by framing generator training as a generalization of particle models.
This suggests that a generator is an optional addition to any such generative
model. Consequently, integrating a generator into a score-based diffusion model
and training a GAN without a generator naturally emerge from our framework. We
empirically test the viability of these original models as proofs of concepts
of potential applications of our framework.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15957" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large pre-trained models have had a significant impact on computer vision by
enabling multi-modal learning, where the CLIP model has achieved impressive
results in image classification, object detection, and semantic segmentation.
However, the model's performance on 3D point cloud processing tasks is limited
due to the domain gap between depth maps from 3D projection and training images
of CLIP. This paper proposes DiffCLIP, a new pre-training framework that
incorporates stable diffusion with ControlNet to minimize the domain gap in the
visual branch. Additionally, a style-prompt generation module is introduced for
few-shot tasks in the textual branch. Extensive experiments on the ModelNet10,
ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilities
for 3D understanding. By using stable diffusion and style-prompt generation,
DiffCLIP achieves an accuracy of 43.2\% for zero-shot classification on OBJ\_BG
of ScanObjectNN, which is state-of-the-art performance, and an accuracy of
80.6\% for zero-shot classification on ModelNet10, which is comparable to
state-of-the-art performance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Diffusion Probabilistic Prior for Low-Dose CT Image Denoising
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15887" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Low-dose computed tomography (CT) image denoising is crucial in medical image
computing. Recent years have been remarkable improvement in deep learning-based
methods for this task. However, training deep denoising neural networks
requires low-dose and normal-dose CT image pairs, which are difficult to obtain
in the clinic settings. To address this challenge, we propose a novel fully
unsupervised method for low-dose CT image denoising, which is based on
denoising diffusion probabilistic model -- a powerful generative model. First,
we train an unconditional denoising diffusion probabilistic model capable of
generating high-quality normal-dose CT images from random noise. Subsequently,
the probabilistic priors of the pre-trained diffusion model are incorporated
into a Maximum A Posteriori (MAP) estimation framework for iteratively solving
the image denoising problem. Our method ensures the diffusion model produces
high-quality normal-dose CT images while keeping the image content consistent
with the input low-dose CT images. We evaluate our method on a widely used
low-dose CT image denoising benchmark, and it outperforms several supervised
low-dose CT image denoising methods in terms of both quantitative and visual
performance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On Architectural Compression of Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, Shinkook Choi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15798" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Exceptional text-to-image (T2I) generation results of Stable Diffusion models
(SDMs) come with substantial computational demands. To resolve this issue,
recent research on efficient SDMs has prioritized reducing the number of
sampling steps and utilizing network quantization. Orthogonal to these
directions, this study highlights the power of classical architectural
compression for general-purpose T2I synthesis by introducing block-removed
knowledge-distilled SDMs (BK-SDMs). We eliminate several residual and attention
blocks from the U-Net of SDMs, obtaining over a 30% reduction in the number of
parameters, MACs per sampling step, and latency. We conduct distillation-based
pretraining with only 0.22M LAION pairs (fewer than 0.1% of the full training
pairs) on a single A100 GPU. Despite being trained with limited resources, our
compact models can imitate the original SDM by benefiting from transferred
knowledge and achieve competitive results against larger multi-billion
parameter models on the zero-shot MS-COCO benchmark. Moreover, we demonstrate
the applicability of our lightweight pretrained models in personalized
generation with DreamBooth finetuning.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Jooyoung Choi, Yunjey Choi, Yunji Kim, Junho Kim, Sungroh Yoon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15779" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-image diffusion models can generate diverse, high-fidelity images
based on user-provided text prompts. Recent research has extended these models
to support text-guided image editing. While text guidance is an intuitive
editing interface for users, it often fails to ensure the precise concept
conveyed by users. To address this issue, we propose Custom-Edit, in which we
(i) customize a diffusion model with a few reference images and then (ii)
perform text-guided editing. Our key discovery is that customizing only
language-relevant parameters with augmented prompts improves reference
similarity significantly while maintaining source similarity. Moreover, we
provide our recipe for each customization and editing process. We compare
popular customization methods and validate our findings on two editing methods
using various datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Differentially Private Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Saiyue Lyu, Margarita Vinaroz, Michael F. Liu, Mijung Park </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15759" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models (DMs) are widely used for generating high-quality image
datasets. However, since they operate directly in the high-dimensional pixel
space, optimization of DMs is computationally expensive, requiring long
training times. This contributes to large amounts of noise being injected into
the differentially private learning process, due to the composability property
of differential privacy. To address this challenge, we propose training Latent
Diffusion Models (LDMs) with differential privacy. LDMs use powerful
pre-trained autoencoders to reduce the high-dimensional pixel space to a much
lower-dimensional latent space, making training DMs more efficient and fast.
Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then
fine-tunes them with private data, we fine-tune only the attention modules of
LDMs at varying layers with privacy-sensitive data, reducing the number of
trainable parameters by approximately 96% compared to fine-tuning the entire
DM. We test our algorithm on several public-private data pairs, such as
ImageNet as public data and CIFAR10 and CelebA as private data, and SVHN as
public data and MNIST as private data. Our approach provides a promising
direction for training more powerful, yet training-efficient differentially
private DMs that can produce high-quality synthetic images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Multimodal Autoencoders
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Daniel Wesego, Amirmohammad Rooshenas </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15708" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Multimodal Variational Autoencoders (VAEs) represent a promising group of
generative models that facilitate the construction of a tractable posterior
within the latent space, given multiple modalities. Daunhawer et al. (2022)
demonstrate that as the number of modalities increases, the generative quality
of each modality declines. In this study, we explore an alternative approach to
enhance the generative performance of multimodal VAEs by jointly modeling the
latent space of unimodal VAEs using score-based models (SBMs). The role of the
SBM is to enforce multimodal coherence by learning the correlation among the
latent variables. Consequently, our model combines the superior generative
quality of unimodal VAEs with coherent integration across different modalities.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2023 </span>    
         <span class="authors"> Dongnan Gui, Kai Chen, Haisong Ding, Qiang Huo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15660" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 There are more than 80,000 character categories in Chinese while most of them
are rarely used. To build a high performance handwritten Chinese character
recognition (HCCR) system supporting the full character set with a traditional
approach, many training samples need be collected for each character category,
which is both time-consuming and expensive. In this paper, we propose a novel
approach to transforming Chinese character glyph images generated from font
libraries to handwritten ones with a denoising diffusion probabilistic model
(DDPM). Training from handwritten samples of a small character set, the DDPM is
capable of mapping printed strokes to handwritten ones, which makes it possible
to generate photo-realistic and diverse style handwritten samples of unseen
character categories. Combining DDPM-synthesized samples of unseen categories
with real samples of other categories, we can build an HCCR system to support
the full character set. Experimental results on CASIA-HWDB dataset with 3,755
character categories show that the HCCR systems trained with synthetic samples
perform similarly with the one trained with real samples in terms of
recognition accuracy. The proposed method has the potential to address HCCR
with a larger vocabulary.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Manifold Diffusion Fields
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Ahmed A. Elhag, Joshua M. Susskind, Miguel Angel Bautista </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15586" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Manifold Diffusion Fields (MDF), an approach to learn generative
models of continuous functions defined over Riemannian manifolds. Leveraging
insights from spectral geometry analysis, we define an intrinsic coordinate
system on the manifold via the eigen-functions of the Laplace-Beltrami
Operator. MDF represents functions using an explicit parametrization formed by
a set of multiple input-output pairs. Our approach allows to sample continuous
functions on manifolds and is invariant with respect to rigid and isometric
transformations of the manifold. Empirical results on several datasets and
manifolds show that MDF can capture distributions of such functions with better
diversity and fidelity than previous approaches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15357" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models, as a kind of powerful generative model, have given
impressive results on image super-resolution (SR) tasks. However, due to the
randomness introduced in the reverse process of diffusion models, the
performances of diffusion-based SR models are fluctuating at every time of
sampling, especially for samplers with few resampled steps. This inherent
randomness of diffusion models results in ineffectiveness and instability,
making it challenging for users to guarantee the quality of SR results.
However, our work takes this randomness as an opportunity: fully analyzing and
leveraging it leads to the construction of an effective plug-and-play sampling
method that owns the potential to benefit a series of diffusion-based SR
methods. More in detail, we propose to steadily sample high-quality SR images
from pretrained diffusion-based SR models by solving diffusion ordinary
differential equations (diffusion ODEs) with optimal boundary conditions (BCs)
and analyze the characteristics between the choices of BCs and their
corresponding SR results. Our analysis shows the route to obtain an
approximately optimal BC via an efficient exploration in the whole space. The
quality of SR results sampled by the proposed method with fewer steps
outperforms the quality of results sampled by current methods with randomness
from the same pretrained diffusion-based SR model, which means that our
sampling method ``boosts'' current diffusion-based SR models without any
additional training.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Training Energy-Based Normalizing Flow with Score-Matching Objectives
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Chen-Hao Chao, Wei-Fang Sun, Yen-Chang Hsu, Zsolt Kira, Chun-Yi Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15267" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we establish a connection between the parameterization of
flow-based and energy-based generative models, and present a new flow-based
modeling approach called energy-based normalizing flow (EBFlow). We demonstrate
that by optimizing EBFlow with score-matching objectives, the computation of
Jacobian determinants for linear transformations can be entirely bypassed. This
feature enables the use of arbitrary linear layers in the construction of
flow-based models without increasing the computational time complexity of each
training iteration from $\mathcal{O}(D^2L)$ to $\mathcal{O}(D^3L)$ for an
$L$-layered model that accepts $D$-dimensional inputs. This makes the training
of EBFlow more efficient than the commonly-adopted maximum likelihood training
method. In addition to the reduction in runtime, we enhance the training
stability and empirical performance of EBFlow through a number of techniques
developed based on our analysis on the score-matching methods. The experimental
results demonstrate that our approach achieves a significant speedup compared
to maximum likelihood estimation, while outperforming prior efficient training
techniques with a noticeable margin in terms of negative log-likelihood (NLL).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-Based Audio Inpainting
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Eloi Moliner, Vesa Välimäki </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15266" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Audio inpainting aims to reconstruct missing segments in corrupted
recordings. Previous methods produce plausible reconstructions when the gap
length is shorter than about 100\;ms, but the quality decreases for longer
gaps. This paper explores recent advancements in deep learning and,
particularly, diffusion models, for the task of audio inpainting. The proposed
method uses an unconditionally trained generative model, which can be
conditioned in a zero-shot fashion for audio inpainting, offering high
flexibility to regenerate gaps of arbitrary length. An improved deep neural
network architecture based on the constant-Q transform, which allows the model
to exploit pitch-equivariant symmetries in audio, is also presented. The
performance of the proposed algorithm is evaluated through objective and
subjective metrics for the task of reconstructing short to mid-sized gaps. The
results of a formal listening test show that the proposed method delivers a
comparable performance against state-of-the-art for short gaps, while retaining
a good audio quality and outperforming the baselines for the longest gap
lengths tested, 150\;ms and 200\;ms. This work helps improve the restoration of
sound recordings having fairly long local disturbances or dropouts, which must
be reconstructed.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Robust Classification via a Single Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15241" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models have been successfully applied to improving
adversarial robustness of image classifiers by purifying the adversarial noises
or generating realistic data for adversarial training. However, the
diffusion-based purification can be evaded by stronger adaptive attacks while
adversarial training does not perform well under unseen threats, exhibiting
inevitable limitations of these methods. To better harness the expressive power
of diffusion models, in this paper we propose Robust Diffusion Classifier
(RDC), a generative classifier that is constructed from a pre-trained diffusion
model to be adversarially robust. Our method first maximizes the data
likelihood of a given input and then predicts the class probabilities of the
optimized input using the conditional likelihood of the diffusion model through
Bayes' theorem. Since our method does not require training on particular
adversarial attacks, we demonstrate that it is more generalizable to defend
against multiple unseen threats. In particular, RDC achieves $73.24\%$ robust
accuracy against $\ell_\infty$ norm-bounded perturbations with
$\epsilon_\infty=8/255$ on CIFAR-10, surpassing the previous state-of-the-art
adversarial training models by $+2.34\%$. The findings highlight the potential
of generative classifiers by employing diffusion models for adversarial
robustness compared with the commonly studied discriminative classifiers.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik Kim, Namhyuk Ahn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15194" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The recent progress in diffusion-based text-to-image generation models has
significantly expanded generative capabilities via conditioning the text
descriptions. However, since relying solely on text prompts is still
restrictive for fine-grained customization, we aim to extend the boundaries of
conditional generation to incorporate diverse types of modalities, e.g.,
sketch, box, and style embedding, simultaneously. We thus design a multimodal
text-to-image diffusion model, coined as DiffBlender, that achieves the
aforementioned goal in a single model by training only a few small
hypernetworks. DiffBlender facilitates a convenient scaling of input
modalities, without altering the parameters of an existing large-scale
generative model to retain its well-established knowledge. Furthermore, our
study sets new standards for multimodal generation by conducting quantitative
and qualitative comparisons with existing approaches. By diversifying the
channels of conditioning modalities, DiffBlender faithfully reflects the
provided information or, in its absence, creates imaginative generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai, Chi-Keung Tang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15171" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper introduces Deceptive-NeRF, a new method for enhancing the quality
of reconstructed NeRF models using synthetically generated pseudo-observations,
capable of handling sparse input and removing floater artifacts. Our proposed
method involves three key steps: 1) reconstruct a coarse NeRF model from sparse
inputs; 2) generate pseudo-observations based on the coarse model; 3) refine
the NeRF model using pseudo-observations to produce a high-quality
reconstruction. To generate photo-realistic pseudo-observations that faithfully
preserve the identity of the reconstructed scene while remaining consistent
with the sparse inputs, we develop a rectification latent diffusion model that
generates images conditional on a coarse RGB image and depth map, which are
derived from the coarse NeRF and latent text embedding from input images.
Extensive experiments show that our method is effective and can generate
perceptually high-quality NeRF even with very sparse inputs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unpaired Image-to-Image Translation via Neural Schrödinger Bridge
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.15086" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are a powerful class of generative models which simulate
stochastic differential equations (SDEs) to generate data from noise. Although
diffusion models have achieved remarkable progress in recent years, they have
limitations in the unpaired image-to-image translation tasks due to the
Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to
translate between two arbitrary distributions, have risen as an attractive
solution to this problem. However, none of SB models so far have been
successful at unpaired translation between high-resolution images. In this
work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which
combines SB with adversarial training and regularization to learn a SB between
unpaired data. We demonstrate that UNSB is scalable, and that it successfully
solves various unpaired image-to-image translation tasks. Code:
\url{https://github.com/cyclomon/UNSB}
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DuDGAN: Improving Class-Conditional GANs via Dual-Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Taesun Yeom, Minhyeok Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14849" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Class-conditional image generation using generative adversarial networks
(GANs) has been investigated through various techniques; however, it continues
to face challenges such as mode collapse, training instability, and low-quality
output in cases of datasets with high intra-class variation. Furthermore, most
GANs often converge in larger iterations, resulting in poor iteration efficacy
in training procedures. While Diffusion-GAN has shown potential in generating
realistic samples, it has a critical limitation in generating class-conditional
samples. To overcome these limitations, we propose a novel approach for
class-conditional image generation using GANs called DuDGAN, which incorporates
a dual diffusion-based noise injection process. Our method consists of three
unique networks: a discriminator, a generator, and a classifier. During the
training process, Gaussian-mixture noises are injected into the two noise-aware
networks, the discriminator and the classifier, in distinct ways. This noisy
data helps to prevent overfitting by gradually introducing more challenging
tasks, leading to improved model performance. As a result, our method
outperforms state-of-the-art conditional GAN models for image generation in
terms of performance. We evaluated our method using the AFHQ, Food-101, and
CIFAR-10 datasets and observed superior results across metrics such as FID,
KID, Precision, and Recall score compared with comparison models, highlighting
the effectiveness of our approach.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Jaemoo Choi, Jaewoong Choi, Myungjoo Kang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14777" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Optimal Transport (OT) problem investigates a transport map that bridges two
distributions while minimizing a given cost function. In this regard, OT
between tractable prior distribution and data has been utilized for generative
modeling tasks. However, OT-based methods are susceptible to outliers and face
optimization challenges during training. In this paper, we propose a novel
generative model based on the semi-dual formulation of Unbalanced Optimal
Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution
matching. This approach provides better robustness against outliers, stability
during training, and faster convergence. We validate these properties
empirically through experiments. Moreover, we study the theoretical upper-bound
of divergence between distributions in UOT. Our model outperforms existing
OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80
on CelebA-HQ-256.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On the Generalization of Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Mingyang Yi, Jiacheng Sun, Zhenguo Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14712" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The diffusion probabilistic generative models are widely used to generate
high-quality data. Though they can synthetic data that does not exist in the
training set, the rationale behind such generalization is still unexplored. In
this paper, we formally define the generalization of the generative model,
which is measured by the mutual information between the generated data and the
training set. The definition originates from the intuition that the model which
generates data with less correlation to the training set exhibits better
generalization ability. Meanwhile, we show that for the empirical optimal
diffusion model, the data generated by a deterministic sampler are all highly
related to the training set, thus poor generalization. This result contradicts
the observation of the trained diffusion model's (approximating empirical
optima) extrapolation ability (generating unseen data). To understand this
contradiction, we empirically verify the difference between the sufficiently
trained diffusion model and the empirical optima. We found, though obtained
through sufficient training, there still exists a slight difference between
them, which is critical to making the diffusion model generalizable. Moreover,
we propose another training objective whose empirical optimal solution has no
potential generalization problem. We empirically show that the proposed
training objective returns a similar model to the original one, which further
verifies the generalization ability of the trained diffusion model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Optimal Linear Subspace Search: Learning to Construct Fast and High-Quality Schedulers for Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Zhongjie Duan, Chengyu Wang, Cen Chen, Jun Huang, Weining Qian </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14677" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In recent years, diffusion models have become the most popular and powerful
methods in the field of image synthesis, even rivaling human artists in
artistic creativity. However, the key issue currently limiting the application
of diffusion models is its extremely slow generation process. Although several
methods were proposed to speed up the generation process, there still exists a
trade-off between efficiency and quality. In this paper, we first provide a
detailed theoretical and empirical analysis of the generation process of the
diffusion models based on schedulers. We transform the designing problem of
schedulers into the determination of several parameters, and further transform
the accelerated generation process into an expansion process of the linear
subspace. Based on these analyses, we consequently propose a novel method
called Optimal Linear Subspace Search (OLSS), which accelerates the generation
process by searching for the optimal approximation process of the complete
generation process in the linear subspaces spanned by latent variables. OLSS is
able to generate high-quality images with a very small number of steps. To
demonstrate the effectiveness of our method, we conduct extensive comparative
experiments on open-source diffusion models. Experimental results show that
with a given number of steps, OLSS can significantly improve the quality of
generated images. Using an NVIDIA A100 GPU, we make it possible to generate a
high-quality image by Stable Diffusion within only one second without other
optimization techniques.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 24, 2023 </span>    
         <span class="authors"> Kangfu Mei, Mo Zhou, Vishal M. Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14674" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Probabilistic Field (DPF) models the distribution of continuous
functions defined over metric spaces. While DPF shows great potential for
unifying data generation of various modalities including images, videos, and 3D
geometry, it does not scale to a higher data resolution. This can be attributed
to the ``scaling property'', where it is difficult for the model to capture
local structures through uniform sampling. To this end, we propose a new model
comprising of a view-wise sampling algorithm to focus on local structure
learning, and incorporating additional guidance, e.g., text description, to
complement the global geometry. The model can be scaled to generate
high-resolution data while unifying multiple modalities. Experimental results
on data generation in various modalities demonstrate the effectiveness of our
model, as well as its potential as a foundation framework for scalable
modality-unified visual content generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, Trevor Darrell </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14334" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have been shown to be capable of generating high-quality
images, suggesting that they could contain meaningful internal representations.
Unfortunately, the feature maps that encode a diffusion model's internal
information are spread not only over layers of the network, but also over
diffusion timesteps, making it challenging to extract useful descriptors. We
propose Diffusion Hyperfeatures, a framework for consolidating multi-scale and
multi-timestep feature maps into per-pixel feature descriptors that can be used
for downstream tasks. These descriptors can be extracted for both synthetic and
real images using the generation and inversion processes. We evaluate the
utility of our Diffusion Hyperfeatures on the task of semantic keypoint
correspondence: our method achieves superior performance on the SPair-71k real
image benchmark. We also demonstrate that our method is flexible and
transferable: our feature aggregation network trained on the inversion features
of real image pairs can be used on the generation features of synthetic image
pairs with unseen objects and compositions. Our code is available at
\url{https://diffusion-hyperfeatures.github.io}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, Nader Masmoudi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14267" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.NA, math.NA, I.2.6
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 A potent class of generative models known as Diffusion Probabilistic Models
(DPMs) has become prominent. A forward diffusion process adds gradually noise
to data, while a model learns to gradually denoise. Sampling from pre-trained
DPMs is obtained by solving differential equations (DE) defined by the learnt
model, a process which has shown to be prohibitively slow. Numerous efforts on
speeding-up this process have consisted on crafting powerful ODE solvers.
Despite being quick, such solvers do not usually reach the optimal quality
achieved by available slow SDE solvers. Our goal is to propose SDE solvers that
reach optimal quality without requiring several hundreds or thousands of NFEs
to achieve that goal. In this work, we propose Stochastic Exponential
Derivative-free Solvers (SEEDS), improving and generalizing Exponential
Integrator approaches to the stochastic case on several frameworks. After
carefully analyzing the formulation of exact solutions of diffusion SDEs, we
craft SEEDS to analytically compute the linear part of such solutions. Inspired
by the Exponential Time-Differencing method, SEEDS uses a novel treatment of
the stochastic components of solutions, enabling the analytical computation of
their variance, and contains high-order terms allowing to reach optimal quality
sampling $\sim3$-$5\times$ faster than previous SDE methods. We validate our
approach on several image generation benchmarks, showing that SEEDS outperforms
or is competitive with previous SDE solvers. Contrary to the latter, SEEDS are
derivative and training free, and we fully prove strong convergence guarantees
for them.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improved Convergence of Score-Based Diffusion Models via Prediction-Correction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Francesco Pedrotti, Jan Maas, Marco Mondelli </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14164" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.ST, stat.ML, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) are powerful tools to sample from
complex data distributions. Their underlying idea is to (i) run a forward
process for time $T_1$ by adding noise to the data, (ii) estimate its score
function, and (iii) use such estimate to run a reverse process. As the reverse
process is initialized with the stationary distribution of the forward one, the
existing analysis paradigm requires $T_1\to\infty$. This is however
problematic: from a theoretical viewpoint, for a given precision of the score
approximation, the convergence guarantee fails as $T_1$ diverges; from a
practical viewpoint, a large $T_1$ increases computational costs and leads to
error propagation. This paper addresses the issue by considering a version of
the popular predictor-corrector scheme: after running the forward process, we
first estimate the final distribution via an inexact Langevin dynamics and then
revert the process. Our key technical contribution is to provide convergence
guarantees in Wasserstein distance which require to run the forward process
only for a finite time $T_1$. Our bounds exhibit a mild logarithmic dependence
on the input dimension and the subgaussian norm of the target distribution,
have minimal assumptions on the data, and require only to control the $L^2$
loss on the score approximation, which is the quantity minimized in practice.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Realistic Noise Synthesis with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Qi Wu, Mingyan Han, Ting Jiang, Haoqiang Fan, Bing Zeng, Shuaicheng Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.14022" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep learning-based approaches have achieved remarkable performance in
single-image denoising. However, training denoising models typically requires a
large amount of data, which can be difficult to obtain in real-world scenarios.
Furthermore, synthetic noise used in the past has often produced significant
differences compared to real-world noise due to the complexity of the latter
and the poor modeling ability of noise distributions of Generative Adversarial
Network (GAN) models, resulting in residual noise and artifacts within
denoising models. To address these challenges, we propose a novel method for
synthesizing realistic noise using diffusion models. This approach enables us
to generate large amounts of high-quality data for training denoising models by
controlling camera settings to simulate different environmental conditions and
employing guided multi-scale content information to ensure that our method is
more capable of generating real noise with multi-frequency spatial
correlations. In particular, we design an inversion mechanism for the setting,
which extends our method to more public datasets without setting information.
Based on the noise dataset we synthesized, we have conducted sufficient
experiments on multiple benchmarks, and experimental results demonstrate that
our method outperforms state-of-the-art methods on multiple benchmarks and
metrics, demonstrating its effectiveness in synthesizing realistic noise for
training denoising models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, Xiaodong Lin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.13921" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent text-to-image (T2I) diffusion models show outstanding performance in
generating high-quality images conditioned on textual prompts. However, these
models fail to semantically align the generated images with the text
descriptions due to their limited compositional capabilities, leading to
attribute leakage, entity leakage, and missing entities. In this paper, we
propose a novel attention mask control strategy based on predicted object boxes
to address these three issues. In particular, we first train a BoxNet to
predict a box for each entity that possesses the attribute specified in the
prompt. Then, depending on the predicted boxes, unique mask control is applied
to the cross- and self-attention maps. Our approach produces a more
semantically accurate synthesis by constraining the attention regions of each
token in the prompt to the image. In addition, the proposed method is
straightforward and effective, and can be readily integrated into existing
cross-attention-diffusion-based T2I generators. We compare our approach to
competing methods and demonstrate that it not only faithfully conveys the
semantics of the original text to the generated content, but also achieves high
availability as a ready-to-use plugin.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, Yang Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.13873" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CR, cs.CY, cs.LG, cs.SI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 State-of-the-art Text-to-Image models like Stable Diffusion and DALLE$\cdot$2
are revolutionizing how people generate visual content. At the same time,
society has serious concerns about how adversaries can exploit such models to
generate unsafe images. In this work, we focus on demystifying the generation
of unsafe images and hateful memes from Text-to-Image models. We first
construct a typology of unsafe images consisting of five categories (sexually
explicit, violent, disturbing, hateful, and political). Then, we assess the
proportion of unsafe images generated by four advanced Text-to-Image models
using four prompt datasets. We find that these models can generate a
substantial percentage of unsafe images; across four models and four prompt
datasets, 14.56% of all generated images are unsafe. When comparing the four
models, we find different risk levels, with Stable Diffusion being the most
prone to generating unsafe content (18.92% of all generated images are unsafe).
Given Stable Diffusion's tendency to generate more unsafe content, we evaluate
its potential to generate hateful meme variants if exploited by an adversary to
attack a specific individual or community. We employ three image editing
methods, DreamBooth, Textual Inversion, and SDEdit, which are supported by
Stable Diffusion. Our evaluation result shows that 24% of the generated images
using DreamBooth are hateful meme variants that present the features of the
original hateful meme and the target individual/community; these generated
images are comparable to hateful meme variants collected from the real world.
Overall, our results demonstrate that the danger of large-scale generation of
unsafe images is imminent. We discuss several mitigating measures, such as
curating training data, regulating prompts, and implementing safety filters,
and encourage better safeguard tools to be developed to prevent unsafe
generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, Liang Lin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.13840" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, cs.MM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper presents a controllable text-to-video (T2V) diffusion model, named
Video-ControlNet, that generates videos conditioned on a sequence of control
signals, such as edge or depth maps. Video-ControlNet is built on a pre-trained
conditional text-to-image (T2I) diffusion model by incorporating a
spatial-temporal self-attention mechanism and trainable temporal layers for
efficient cross-frame modeling. A first-frame conditioning strategy is proposed
to facilitate the model to generate videos transferred from the image domain as
well as arbitrary-length videos in an auto-regressive manner. Moreover,
Video-ControlNet employs a novel residual-based noise initialization strategy
to introduce motion prior from an input video, producing more coherent videos.
With the proposed architecture and strategies, Video-ControlNet can achieve
resource-efficient convergence and generate superior quality and consistent
videos with fine-grained control. Extensive experiments demonstrate its success
in various video generative tasks such as video editing and video style
transfer, outperforming previous methods in terms of consistency and quality.
Project Page: https://controlavideo.github.io/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## WaveDM: Wavelet-Based Diffusion Models for Image Restoration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Yi Huang, Jiancheng Huang, Jianzhuang Liu, Yu Dong, Jiaxi Lv, Shifeng Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.13819" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Latest diffusion-based methods for many image restoration tasks outperform
traditional models, but they encounter the long-time inference problem. To
tackle it, this paper proposes a Wavelet-Based Diffusion Model (WaveDM) with an
Efficient Conditional Sampling (ECS) strategy. WaveDM learns the distribution
of clean images in the wavelet domain conditioned on the wavelet spectrum of
degraded images after wavelet transform, which is more time-saving in each step
of sampling than modeling in the spatial domain. In addition, ECS follows the
same procedure as the deterministic implicit sampling in the initial sampling
period and then stops to predict clean images directly, which reduces the
number of total sampling steps to around 5. Evaluations on four benchmark
datasets including image raindrop removal, defocus deblurring, demoir\'eing,
and denoising demonstrate that WaveDM achieves state-of-the-art performance
with the efficiency that is comparable to traditional one-pass methods and over
100 times faster than existing image restoration methods using vanilla
diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2023 </span>    
         <span class="authors"> Lijun Li, Li'an Zhuo, Bang Zhang, Liefeng Bo, Chen Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.13705" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Hand mesh reconstruction from the monocular image is a challenging task due
to its depth ambiguity and severe occlusion, there remains a non-unique mapping
between the monocular image and hand mesh. To address this, we develop
DiffHand, the first diffusion-based framework that approaches hand mesh
reconstruction as a denoising diffusion process. Our one-stage pipeline
utilizes noise to model the uncertainty distribution of the intermediate hand
mesh in a forward process. We reformulate the denoising diffusion process to
gradually refine noisy hand mesh and then select mesh with the highest
probability of being correct based on the image itself, rather than relying on
2D joints extracted beforehand. To better model the connectivity of hand
vertices, we design a novel network module called the cross-modality decoder.
Extensive experiments on the popular benchmarks demonstrate that our method
outperforms the state-of-the-art hand mesh reconstruction approaches by
achieving 5.8mm PA-MPJPE on the Freihand test set, 4.98mm PA-MPJPE on the
DexYCB test set.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Training Diffusion Models with Reinforcement Learning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 22, 2023 </span>    
         <span class="authors"> Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.13301" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are a class of flexible generative models trained with an
approximation to the log-likelihood objective. However, most use cases of
diffusion models are not concerned with likelihoods, but instead with
downstream objectives such as human-perceived image quality or drug
effectiveness. In this paper, we investigate reinforcement learning methods for
directly optimizing diffusion models for such objectives. We describe how
posing denoising as a multi-step decision-making problem enables a class of
policy gradient algorithms, which we refer to as denoising diffusion policy
optimization (DDPO), that are more effective than alternative reward-weighted
likelihood approaches. Empirically, DDPO is able to adapt text-to-image
diffusion models to objectives that are difficult to express via prompting,
such as image compressibility, and those derived from human feedback, such as
aesthetic quality. Finally, we show that DDPO can improve prompt-image
alignment using feedback from a vision-language model without the need for
additional data collection or human annotation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GSURE-Based Diffusion Model Training with Corrupted Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 22, 2023 </span>    
         <span class="authors"> Bahjat Kawar, Noam Elata, Tomer Michaeli, Michael Elad </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.13128" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have demonstrated impressive results in both data generation
and downstream tasks such as inverse problems, text-based editing,
classification, and more. However, training such models usually requires large
amounts of clean signals which are often difficult or impossible to obtain. In
this work, we propose a novel training technique for generative diffusion
models based only on corrupted data. We introduce a loss function based on the
Generalized Stein's Unbiased Risk Estimator (GSURE), and prove that under some
conditions, it is equivalent to the training objective used in fully supervised
diffusion models. We demonstrate our technique on face images as well as
Magnetic Resonance Imaging (MRI), where the use of undersampled data
significantly alleviates data collection costs. Our approach achieves
generative performance comparable to its fully supervised counterpart without
training on any clean signals. In addition, we deploy the resulting diffusion
model in various downstream tasks beyond the degradation present in the
training set, showcasing promising results.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 22, 2023 </span>    
         <span class="authors"> Guy Yariv, Itai Gat, Lior Wolf, Yossi Adi, Idan Schwartz </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.13050" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.CV, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In recent years, image generation has shown a great leap in performance,
where diffusion models play a central role. Although generating high-quality
images, such models are mainly conditioned on textual descriptions. This begs
the question: "how can we adopt such models to be conditioned on other
modalities?". In this paper, we propose a novel method utilizing latent
diffusion models trained for text-to-image-generation to generate images
conditioned on audio recordings. Using a pre-trained audio encoding model, the
proposed method encodes audio into a new token, which can be considered as an
adaptation layer between the audio and text representations. Such a modeling
paradigm requires a small number of trainable parameters, making the proposed
approach appealing for lightweight optimization. Results suggest the proposed
method is superior to the evaluated baseline methods, considering objective and
subjective metrics. Code and samples are available at:
https://pages.cs.huji.ac.il/adiyoss-lab/AudioToken.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Hierarchical Integration Diffusion Model for Realistic Image Deblurring
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 22, 2023 </span>    
         <span class="authors"> Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, Linghe Kong, Xin Yuan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.12966" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models (DMs) have recently been introduced in image deblurring and
exhibited promising performance, particularly in terms of details
reconstruction. However, the diffusion model requires a large number of
inference iterations to recover the clean image from pure Gaussian noise, which
consumes massive computational resources. Moreover, the distribution
synthesized by the diffusion model is often misaligned with the target results,
leading to restrictions in distortion-based metrics. To address the above
issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for
realistic image deblurring. Specifically, we perform the DM in a highly
compacted latent space to generate the prior feature for the deblurring
process. The deblurring process is implemented by a regression-based method to
obtain better distortion accuracy. Meanwhile, the highly compact latent space
ensures the efficiency of the DM. Furthermore, we design the hierarchical
integration module to fuse the prior into the regression-based model from
multiple scales, enabling better generalization in complex blurry scenarios.
Comprehensive experiments on synthetic and real-world blur datasets demonstrate
that our HI-Diff outperforms state-of-the-art methods. Code and trained models
are available at https://github.com/zhengchen1999/HI-Diff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 22, 2023 </span>    
         <span class="authors"> Zheng Li, Yuxuan Li, Penghai Zhao, Renjie Song, Xiang Li, Jian Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.12954" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently achieved astonishing performance in generating
high-fidelity photo-realistic images. Given their huge success, it is still
unclear whether synthetic images are applicable for knowledge distillation when
real images are unavailable. In this paper, we extensively study whether and
how synthetic images produced from state-of-the-art diffusion models can be
used for knowledge distillation without access to real images, and obtain three
key conclusions: (1) synthetic data from diffusion models can easily lead to
state-of-the-art performance among existing synthesis-based distillation
methods, (2) low-fidelity synthetic images are better teaching materials, and
(3) relatively weak classifiers are better teachers. Code is available at
https://github.com/zhengli97/DM-KD.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 22, 2023 </span>    
         <span class="authors"> Huadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, Zhou Zhao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.12708" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-speech(TTS) has undergone remarkable improvements in performance,
particularly with the advent of Denoising Diffusion Probabilistic Models
(DDPMs). However, the perceived quality of audio depends not solely on its
content, pitch, rhythm, and energy, but also on the physical environment. In
this work, we propose ViT-TTS, the first visual TTS model with scalable
diffusion transformers. ViT-TTS complement the phoneme sequence with the visual
information to generate high-perceived audio, opening up new avenues for
practical applications of AR and VR to allow a more immersive and realistic
audio experience. To mitigate the data scarcity in learning visual acoustic
information, we 1) introduce a self-supervised learning framework to enhance
both the visual-text encoder and denoiser decoder; 2) leverage the diffusion
transformer scalable in terms of parameters and capacity to learn visual scene
information. Experimental results demonstrate that ViT-TTS achieves new
state-of-the-art results, outperforming cascaded systems and other baselines
regardless of the visibility of the scene. With low-resource data (1h, 2h, 5h),
ViT-TTS achieves comparative results with rich-resource
baselines.~\footnote{Audio samples are available at
\url{https://ViT-TTS.github.io/.}}
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Towards Globally Consistent Stochastic Human Motion Prediction via Motion Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 21, 2023 </span>    
         <span class="authors"> Jiarui Sun, Girish Chowdhary </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.12554" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Stochastic human motion prediction aims to predict multiple possible upcoming
pose sequences based on past human motion trajectories. Prior works focused
heavily on generating diverse motion samples, leading to inconsistent, abnormal
predictions from the immediate past observations. To address this issue, in
this work, we propose DiffMotion, a diffusion-based stochastic human motion
prediction framework that considers both the kinematic structure of the human
body and the globally temporally consistent nature of motion. Specifically,
DiffMotion consists of two modules: 1) a transformer-based network for
generating an initial motion reconstruction from corrupted motion, and 2) a
multi-stage graph convolutional network to iteratively refine the generated
motion based on past observations. Facilitated by the proposed direct target
prediction objective and the variance scheduler, our method is capable of
predicting accurate, realistic and consistent motion with an appropriate level
of diversity. Our results on benchmark datasets demonstrate that DiffMotion
outperforms previous methods by large margins in terms of accuracy and fidelity
while demonstrating superior robustness.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffUCD:Unsupervised Hyperspectral Image Change Detection with Semantic Correlation Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 21, 2023 </span>    
         <span class="authors"> Xiangrong Zhang, Shunli Tian, Guanchun Wang, Huiyu Zhou, Licheng Jiao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.12410" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Hyperspectral image change detection (HSI-CD) has emerged as a crucial
research area in remote sensing due to its ability to detect subtle changes on
the earth's surface. Recently, diffusional denoising probabilistic models
(DDPM) have demonstrated remarkable performance in the generative domain. Apart
from their image generation capability, the denoising process in diffusion
models can comprehensively account for the semantic correlation of
spectral-spatial features in HSI, resulting in the retrieval of semantically
relevant features in the original image. In this work, we extend the diffusion
model's application to the HSI-CD field and propose a novel unsupervised HSI-CD
with semantic correlation diffusion model (DiffUCD). Specifically, the semantic
correlation diffusion model (SCDM) leverages abundant unlabeled samples and
fully accounts for the semantic correlation of spectral-spatial features, which
mitigates pseudo change between multi-temporal images arising from inconsistent
imaging conditions. Besides, objects with the same semantic concept at the same
spatial location may exhibit inconsistent spectral signatures at different
times, resulting in pseudo change. To address this problem, we propose a
cross-temporal contrastive learning (CTCL) mechanism that aligns the spectral
feature representations of unchanged samples. By doing so, the spectral
difference invariant features caused by environmental changes can be obtained.
Experiments conducted on three publicly available datasets demonstrate that the
proposed method outperforms the other state-of-the-art unsupervised methods in
terms of Overall Accuracy (OA), Kappa Coefficient (KC), and F1 scores,
achieving improvements of approximately 3.95%, 8.13%, and 4.45%, respectively.
Notably, our method can achieve comparable results to those fully supervised
methods requiring numerous annotated samples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Dual-Diffusion: Dual Conditional Denoising Diffusion Probabilistic Models for Blind Super-Resolution Reconstruction in RSIs
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 20, 2023 </span>    
         <span class="authors"> Mengze Xu, Jie Ma, Yuanyuan Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.12170" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Previous super-resolution reconstruction (SR) works are always designed on
the assumption that the degradation operation is fixed, such as bicubic
downsampling. However, as for remote sensing images, some unexpected factors
can cause the blurred visual performance, like weather factors, orbit altitude,
etc. Blind SR methods are proposed to deal with various degradations. There are
two main challenges of blind SR in RSIs: 1) the accu-rate estimation of
degradation kernels; 2) the realistic image generation in the ill-posed
problem. To rise to the challenge, we propose a novel blind SR framework based
on dual conditional denoising diffusion probabilistic models (DDSR). In our
work, we introduce conditional denoising diffusion probabilistic models (DDPM)
from two aspects: kernel estimation progress and re-construction progress,
named as the dual-diffusion. As for kernel estimation progress, conditioned on
low-resolution (LR) images, a new DDPM-based kernel predictor is constructed by
studying the invertible mapping between the kernel distribution and the latent
distribution. As for reconstruction progress, regarding the predicted
degradation kernels and LR images as conditional information, we construct a
DDPM-based reconstructor to learning the mapping from the LR images to HR
images. Com-prehensive experiments show the priority of our proposal com-pared
with SOTA blind SR methods. Source Code is available at
https://github.com/Lincoln20030413/DDSR
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Any-to-Any Generation via Composable Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 19, 2023 </span>    
         <span class="authors"> Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, Mohit Bansal </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.11846" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CL, cs.LG, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Composable Diffusion (CoDi), a novel generative model capable of
generating any combination of output modalities, such as language, image,
video, or audio, from any combination of input modalities. Unlike existing
generative AI systems, CoDi can generate multiple modalities in parallel and
its input is not limited to a subset of modalities like text or image. Despite
the absence of training datasets for many combinations of modalities, we
propose to align modalities in both the input and output space. This allows
CoDi to freely condition on any input combination and generate any group of
modalities, even if they are not present in the training data. CoDi employs a
novel composable generation strategy which involves building a shared
multimodal space by bridging alignment in the diffusion process, enabling the
synchronized generation of intertwined modalities, such as temporally aligned
video and audio. Highly customizable and flexible, CoDi achieves strong
joint-modality generation quality, and outperforms or is on par with the
unimodal state-of-the-art for single-modality synthesis. The project page with
demonstrations and code is at https://codi-gen.github.io
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 19, 2023 </span>    
         <span class="authors"> Jinyi Hu, Xu Han, Xiaoyuan Yi, Yutong Chen, Wenhao Li, Zhiyuan Liu, Maosong Sun </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.11540" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CL
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have made impressive progress in text-to-image synthesis.
However, training such large-scale models (e.g. Stable Diffusion), from scratch
requires high computational costs and massive high-quality text-image pairs,
which becomes unaffordable in other languages. To handle this challenge, we
propose IAP, a simple but effective method to transfer English Stable Diffusion
into Chinese. IAP optimizes only a separate Chinese text encoder with all other
parameters fixed to align Chinese semantics space to the English one in CLIP.
To achieve this, we innovatively treat images as pivots and minimize the
distance of attentive features produced from cross-attention between images and
each language respectively. In this way, IAP establishes connections of
Chinese, English and visual semantics in CLIP's embedding space efficiently,
advancing the quality of the generated image with direct Chinese prompts.
Experimental results show that our method outperforms several strong Chinese
diffusion models with only 5%~10% training data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 19, 2023 </span>    
         <span class="authors"> Ibrahim Malik, Siddique Latif, Raja Jurdak, Björn Schuller </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.11413" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we propose to utilise diffusion models for data augmentation
in speech emotion recognition (SER). In particular, we present an effective
approach to utilise improved denoising diffusion probabilistic models (IDDPM)
to generate synthetic emotional data. We condition the IDDPM with the textual
embedding from bidirectional encoder representations from transformers (BERT)
to generate high-quality synthetic emotional samples in different speakers'
voices\footnote{synthetic samples URL:
\url{https://emulationai.com/research/diffusion-ser.}}. We implement a series
of experiments and show that better quality synthetic data helps improve SER
performance. We compare results with generative adversarial networks (GANs) and
show that the proposed model generates better-quality synthetic samples that
can considerably improve the performance of SER when augmented with synthetic
data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, Animesh Garg </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.11281" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Object-centric learning aims to represent visual data with a set of object
entities (a.k.a. slots), providing structured representations that enable
systematic generalization. Leveraging advanced architectures like Transformers,
recent approaches have made significant progress in unsupervised object
discovery. In addition, slot-based representations hold great potential for
generative modeling, such as controllable image generation and object
manipulation in image editing. However, current slot-based methods often
produce blurry images and distorted objects, exhibiting poor generative
modeling capabilities. In this paper, we focus on improving slot-to-image
decoding, a crucial aspect for high-quality visual generation. We introduce
SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for
both image and video data. Thanks to the powerful modeling capacity of LDMs,
SlotDiffusion surpasses previous slot models in unsupervised object
segmentation and visual generation across six datasets. Furthermore, our
learned object features can be utilized by existing object-centric dynamics
models, improving video prediction quality and downstream temporal reasoning
tasks. Finally, we demonstrate the scalability of SlotDiffusion to
unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated
with self-supervised pre-trained image encoders.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.11147" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Achieving machine autonomy and human control often represent divergent
objectives in the design of interactive AI systems. Visual generative
foundation models such as Stable Diffusion show promise in navigating these
goals, especially when prompted with arbitrary languages. However, they often
fall short in generating images with spatial, structural, or geometric
controls. The integration of such controls, which can accommodate various
visual conditions in a single unified model, remains an unaddressed challenge.
In response, we introduce UniControl, a new generative foundation model that
consolidates a wide array of controllable condition-to-image (C2I) tasks within
a singular framework, while still allowing for arbitrary language prompts.
UniControl enables pixel-level-precise image generation, where visual
conditions primarily influence the generated structures and language prompts
guide the style and context. To equip UniControl with the capacity to handle
diverse visual conditions, we augment pretrained text-to-image diffusion models
and introduce a task-aware HyperNet to modulate the diffusion models, enabling
the adaptation to different C2I tasks simultaneously. Trained on nine unique
C2I tasks, UniControl demonstrates impressive zero-shot generation abilities
with unseen visual conditions. Experimental results show that UniControl often
surpasses the performance of single-task-controlled methods of comparable model
sizes. This control versatility positions UniControl as a significant
advancement in the realm of controllable visual generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Javier E Santos, Zachary R. Fox, Nicholas Lubbers, Yen Ting Lin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.11089" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Typical generative diffusion models rely on a Gaussian diffusion process for
training the backward transformations, which can then be used to generate
samples from Gaussian noise. However, real world data often takes place in
discrete-state spaces, including many scientific applications. Here, we develop
a theoretical formulation for arbitrary discrete-state Markov processes in the
forward diffusion process using exact (as opposed to variational) analysis. We
relate the theory to the existing continuous-state Gaussian diffusion as well
as other approaches to discrete diffusion, and identify the corresponding
reverse-time stochastic process and score function in the continuous-time
setting, and the reverse-time mapping in the discrete-time setting. As an
example of this framework, we introduce ``Blackout Diffusion'', which learns to
produce samples from an empty image instead of from noise. Numerical
experiments on the CIFAR-10, Binarized MNIST, and CelebA datasets confirm the
feasibility of our approach. Generalizing from specific (Gaussian) forward
processes to discrete-state processes without a variational approximation sheds
light on how to interpret diffusion models, which we discuss.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unsupervised Pansharpening via Low-rank Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Xiangyu Rui, Xiangyong Cao, Zeyu Zhu, Zongsheng Yue, Deyu Meng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10925" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Pansharpening is a process of merging a highresolution panchromatic (PAN)
image and a low-resolution multispectral (LRMS) image to create a single
high-resolution multispectral (HRMS) image. Most of the existing deep
learningbased pansharpening methods have poor generalization ability and the
traditional model-based pansharpening methods need careful manual exploration
for the image structure prior. To alleviate these issues, this paper proposes
an unsupervised pansharpening method by combining the diffusion model with the
low-rank matrix factorization technique. Specifically, we assume that the HRMS
image is decomposed into the product of two low-rank tensors, i.e., the base
tensor and the coefficient matrix. The base tensor lies on the image field and
has low spectral dimension, we can thus conveniently utilize a pre-trained
remote sensing diffusion model to capture its image structures. Additionally,
we derive a simple yet quite effective way to preestimate the coefficient
matrix from the observed LRMS image, which preserves the spectral information
of the HRMS. Extensive experimental results on some benchmark datasets
demonstrate that our proposed method performs better than traditional
model-based approaches and has better generalization ability than deep
learning-based techniques. The code is released in
https://github.com/xyrui/PLRDiff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Structural Pruning for Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Gongfan Fang, Xinyin Ma, Xinchao Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10924" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative modeling has recently undergone remarkable advancements, primarily
propelled by the transformative implications of Diffusion Probabilistic Models
(DPMs). The impressive capability of these models, however, often entails
significant computational overhead during both training and inference. To
tackle this challenge, we present Diff-Pruning, an efficient compression method
tailored for learning lightweight diffusion models from pre-existing ones,
without the need for extensive re-training. The essence of Diff-Pruning is
encapsulated in a Taylor expansion over pruned timesteps, a process that
disregards non-contributory diffusion steps and ensembles informative gradients
to identify important weights. Our empirical assessment, undertaken across four
diverse datasets highlights two primary benefits of our proposed method: 1)
Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to
20% of the original training expenditure; 2) Consistency: the pruned diffusion
models inherently preserve generative behavior congruent with their pre-trained
progenitors. Code is available at \url{https://github.com/VainF/Diff-Pruning}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-Based Mel-Spectrogram Enhancement for Personalized Speech Synthesis with Found Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Yusheng Tian, Wei Liu, Tan Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10891" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Creating synthetic voices with found data is challenging, as real-world
recordings often contain various types of audio degradation. One way to address
this problem is to pre-enhance the speech with an enhancement model and then
use the enhanced data for text-to-speech (TTS) model training. Ideally, the
enhancement model should be able to tackle multiple types of audio degradation
simultaneously. This paper investigates the use of conditional diffusion models
for generalized speech enhancement. The enhancement is performed on the log
Mel-spectrogram domain to align with the TTS training objective. Text
information is introduced as an additional condition to improve the model
robustness. Experiments on real-world recordings demonstrate that the synthetic
voice built on data enhanced by the proposed model produces higher-quality
synthetic speech, compared to those trained on data enhanced by strong
baselines. Audio samples are available at \url{https://dmse4tts.github.io/}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## TextDiffuser: Diffusion Models as Text Painters
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10855" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have gained increasing attention for their impressive
generation abilities but currently struggle with rendering accurate and
coherent text. To address this issue, we introduce TextDiffuser, focusing on
generating images with visually appealing text that is coherent with
backgrounds. TextDiffuser consists of two stages: first, a Transformer model
generates the layout of keywords extracted from text prompts, and then
diffusion models generate images conditioned on the text prompt and the
generated layout. Additionally, we contribute the first large-scale text images
dataset with OCR annotations, MARIO-10M, containing 10 million image-text pairs
with text recognition, detection, and character-level segmentation annotations.
We further collect the MARIO-Eval benchmark to serve as a comprehensive tool
for evaluating text rendering quality. Through experiments and user studies, we
show that TextDiffuser is flexible and controllable to create high-quality text
images using text prompts alone or together with text template images, and
conduct text inpainting to reconstruct incomplete images with text. The code,
model, and dataset will be available at \url{https://aka.ms/textdiffuser}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LDM3D: Latent Diffusion Model for 3D
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10853" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that
generates both image and depth map data from a given text prompt, allowing
users to generate RGBD images from text prompts. The LDM3D model is fine-tuned
on a dataset of tuples containing an RGB image, depth map and caption, and
validated through extensive experiments. We also develop an application called
DepthFusion, which uses the generated RGB images and depth maps to create
immersive and interactive 360-degree-view experiences using TouchDesigner. This
technology has the potential to transform a wide range of industries, from
entertainment and gaming to architecture and design. Overall, this paper
presents a significant contribution to the field of generative AI and computer
vision, and showcases the potential of LDM3D and DepthFusion to revolutionize
content creation and digital experiences. A short video summarizing the
approach can be found at https://t.ly/tdi2.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffUTE: Universal Text Editing Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Xing Zheng, Yaohui Li, Changhua Meng, Huijia Zhu, Weiqiang Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10825" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion model based language-guided image editing has achieved great
success recently. However, existing state-of-the-art diffusion models struggle
with rendering correct text and text style during generation. To tackle this
problem, we propose a universal self-supervised text editing diffusion model
(DiffUTE), which aims to replace or modify words in the source image with
another one while maintaining its realistic appearance. Specifically, we build
our model on a diffusion model and carefully modify the network structure to
enable the model for drawing multilingual characters with the help of glyph and
position information. Moreover, we design a self-supervised learning framework
to leverage large amounts of web data to improve the representation ability of
the model. Experimental results show that our method achieves an impressive
performance and enables controllable editing on in-the-wild images with high
fidelity. Our code will be avaliable in
\url{https://github.com/chenhaoxing/DiffUTE}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Democratized Diffusion Language Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Nikita Balagansky, Daniil Gavrilov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10818" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CL
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite the potential benefits of Diffusion Models for NLP applications,
publicly available implementations, trained models, or reproducible training
procedures currently need to be publicly available. We present the Democratized
Diffusion Language Model (DDLM), based on the Continuous Diffusion for
Categorical Data (CDCD) framework, to address these challenges. We propose a
simplified training procedure for DDLM using the C4 dataset and perform an
in-depth analysis of the trained model's behavior. Furthermore, we introduce a
novel early-exiting strategy for faster sampling with models trained with score
interpolation. Since no previous works aimed at solving downstream tasks with
pre-trained Diffusion LM (e.g., classification tasks), we experimented with
GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this
paper, we propose available training and evaluation pipelines to other
researchers and pre-trained DDLM models, which could be used in future research
with Diffusion LMs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Hao Shi, Kazuki Shimada, Masato Hirano, Takashi Shibuya, Yuichiro Koyama, Zhi Zhong, Shusuke Takahashi, Tatsuya Kawahara, Yuki Mitsufuji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10734" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.CL, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based speech enhancement (SE) has been investigated recently, but
its decoding is very time-consuming. One solution is to initialize the decoding
process with the enhanced feature estimated by a predictive SE system. However,
this two-stage method ignores the complementarity between predictive and
diffusion SE. In this paper, we propose a unified system that integrates these
two SE modules. The system encodes both generative and predictive information,
and then applies both generative and predictive decoders, whose outputs are
fused. Specifically, the two SE modules are fused in the first and final
diffusion steps: the first step fusion initializes the diffusion process with
the predictive SE for improving the convergence, and the final step fusion
combines the two complementary SE outputs to improve the SE performance.
Experiments on the Voice-Bank dataset show that the diffusion score estimation
can benefit from the predictive information and speed up the decoding.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Discriminative Diffusion Models as Few-shot Vision and Language Learners
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Xuehai He, Weixi Feng, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Eric Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10722" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models, such as Stable Diffusion, have shown incredible performance
on text-to-image generation. Since text-to-image generation often requires
models to generate visual concepts with fine-grained details and attributes
specified in text prompts, can we leverage the powerful representations learned
by pre-trained diffusion models for discriminative tasks such as image-text
matching? To answer this question, we propose a novel approach, Discriminative
Stable Diffusion (DSD), which turns pre-trained text-to-image diffusion models
into few-shot discriminative learners. Our approach uses the cross-attention
score of a Stable Diffusion model to capture the mutual influence between
visual and textual information and fine-tune the model via attention-based
prompt learning to perform image-text matching. By comparing DSD with
state-of-the-art methods on several benchmark datasets, we demonstrate the
potential of using pre-trained diffusion models for discriminative tasks with
superior results on few-shot image-text matching.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Sampling, Diffusions, and Stochastic Localization
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 18, 2023 </span>    
         <span class="authors"> Andrea Montanari </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10690" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusions are a successful technique to sample from high-dimensional
distributions can be either explicitly given or learnt from a collection of
samples. They implement a diffusion process whose endpoint is a sample from the
target distribution and whose drift is typically represented as a neural
network. Stochastic localization is a successful technique to prove mixing of
Markov Chains and other functional inequalities in high dimension. An
algorithmic version of stochastic localization was introduced in [EAMS2022], to
obtain an algorithm that samples from certain statistical mechanics models.
  This notes have three objectives: (i) Generalize the construction [EAMS2022]
to other stochastic localization processes; (ii) Clarify the connection between
diffusions and stochastic localization. In particular we show that standard
denoising diffusions are stochastic localizations but other examples that are
naturally suggested by the proposed viewpoint; (iii) Describe some insights
that follow from this viewpoint.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 17, 2023 </span>    
         <span class="authors"> Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, Yogesh Balaji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10474" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite tremendous progress in generating high-quality images using diffusion
models, synthesizing a sequence of animated frames that are both photorealistic
and temporally coherent is still in its infancy. While off-the-shelf
billion-scale datasets for image generation are available, collecting similar
video data of the same scale is still challenging. Also, training a video
diffusion model is computationally much more expensive than its image
counterpart. In this work, we explore finetuning a pretrained image diffusion
model with video data as a practical solution for the video synthesis task. We
find that naively extending the image noise prior to video noise prior in video
diffusion leads to sub-optimal performance. Our carefully designed video noise
prior leads to substantially better performance. Extensive experimental
validation shows that our model, Preserve Your Own Correlation (PYoCo), attains
SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It
also achieves SOTA video generation quality on the small-scale UCF-101
benchmark with a $10\times$ smaller model using significantly less computation
than the prior art.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Raising the Bar for Certified Adversarial Robustness with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 17, 2023 </span>    
         <span class="authors"> Thomas Altstidl, David Dobre, Björn Eskofier, Gauthier Gidel, Leo Schwinn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.10388" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Certified defenses against adversarial attacks offer formal guarantees on the
robustness of a model, making them more reliable than empirical methods such as
adversarial training, whose effectiveness is often later reduced by unseen
attacks. Still, the limited certified robustness that is currently achievable
has been a bottleneck for their practical adoption. Gowal et al. and Wang et
al. have shown that generating additional training data using state-of-the-art
diffusion models can considerably improve the robustness of adversarial
training. In this work, we demonstrate that a similar approach can
substantially improve deterministic certified defenses. In addition, we provide
a list of recommendations to scale the robustness of certified training
approaches. One of our main insights is that the generalization gap, i.e., the
difference between the training and test accuracy of the original model, is a
good predictor of the magnitude of the robustness improvement when using
additional generated data. Our approach achieves state-of-the-art deterministic
robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and
$\ell_\infty$ ($\epsilon = 8/255$) threat models, outperforming the previous
best results by $+3.95\%$ and $+1.39\%$, respectively. Furthermore, we report
similar improvements for CIFAR-100.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 16, 2023 </span>    
         <span class="authors"> Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.09847" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This study examines the impact of optimizing the Stable Diffusion (SD) guided
inference pipeline. We propose optimizing certain denoising steps by limiting
the noise computation to conditional noise and eliminating unconditional noise
computation, thereby reducing the complexity of the target iterations by 50%.
Additionally, we demonstrate that later iterations of the SD are less sensitive
to optimization, making them ideal candidates for applying the suggested
optimization. Our experiments show that optimizing the last 20% of the
denoising loop iterations results in an 8.2% reduction in inference time with
almost no perceivable changes to the human eye. Furthermore, we found that by
extending the optimization to 50% of the last iterations, we can reduce
inference time by approximately 20.3%, while still generating visually pleasing
images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A score-based operator Newton method for measure transport
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 16, 2023 </span>    
         <span class="authors"> Nisha Chandramoorthy, Florian Schaefer, Youssef Marzouk </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.09792" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  math.ST, cs.LG, cs.NA, math.NA, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Transportation of probability measures underlies many core tasks in
statistics and machine learning, from variational inference to generative
modeling. A typical goal is to represent a target probability measure of
interest as the push-forward of a tractable source measure through a learned
map. We present a new construction of such a transport map, given the ability
to evaluate the score of the target distribution. Specifically, we characterize
the map as a zero of an infinite-dimensional score-residual operator and derive
a Newton-type method for iteratively constructing such a zero. We prove
convergence of these iterations by invoking classical elliptic regularity
theory for partial differential equations (PDE) and show that this construction
enjoys rapid convergence, under smoothness assumptions on the target score. A
key element of our approach is a generalization of the elementary Newton method
to infinite-dimensional operators, other forms of which have appeared in
nonlinear PDE and in dynamical systems. Our Newton construction, while
developed in a functional setting, also suggests new iterative algorithms for
approximating transport maps.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Expressiveness Remarks for Denoising Diffusion Models and Samplers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 16, 2023 </span>    
         <span class="authors"> Francisco Vargas, Teodora Reu, Anna Kerekes </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.09605" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models are a class of generative models which have
recently achieved state-of-the-art results across many domains. Gradual noise
is added to the data using a diffusion process, which transforms the data
distribution into a Gaussian. Samples from the generative model are then
obtained by simulating an approximation of the time reversal of this diffusion
initialized by Gaussian samples. Recent research has explored adapting
diffusion models for sampling and inference tasks. In this paper, we leverage
known connections to stochastic control akin to the F\"ollmer drift to extend
established neural network approximation results for the F\"ollmer drift to
denoising diffusion models and samplers.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 16, 2023 </span>    
         <span class="authors"> Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, Weizhu Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.09515" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have gained significant attention in the realm of image
generation due to their exceptional performance. Their success has been
recently expanded to text generation via generating all tokens within a
sequence concurrently. However, natural language exhibits a far more pronounced
sequential dependency in comparison to images, and the majority of existing
language models are trained with a left-to-right auto-regressive approach. To
account for the inherent sequential characteristic of natural language, we
introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that
the generation of tokens on the right depends on the generated ones on the
left, a mechanism achieved through employing a dynamic number of denoising
steps that vary based on token position. This results in tokens on the left
undergoing fewer denoising steps than those on the right, thereby enabling them
to generate earlier and subsequently influence the generation of tokens on the
right. In a series of experiments on various text generation tasks, including
text summarization, machine translation, and common sense generation,
AR-Diffusion clearly demonstrated its superiority over existing diffusion
language models and that it can be $100\times\sim600\times$ faster when
achieving comparable results. Our code is available at
https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Discrete Diffusion Probabilistic Models for Symbolic Music Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 16, 2023 </span>    
         <span class="authors"> Matthias Plasser, Silvan Peter, Gerhard Widmer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.09489" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.AI, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Probabilistic Models (DDPMs) have made great strides in
generating high-quality samples in both discrete and continuous domains.
However, Discrete DDPMs (D3PMs) have yet to be applied to the domain of
Symbolic Music. This work presents the direct generation of Polyphonic Symbolic
Music using D3PMs. Our model exhibits state-of-the-art sample quality,
according to current quantitative evaluation metrics, and allows for flexible
infilling at the note level. We further show, that our models are accessible to
post-hoc classifier guidance, widening the scope of possible applications.
However, we also cast a critical view on quantitative evaluation of music
sample quality via statistical metrics, and present a simple algorithm that can
confound our metrics with completely spurious, non-musical samples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 16, 2023 </span>    
         <span class="authors"> Ruoqi Wang, Zhuoyang Chen, Qiong Luo, Feng Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.09121" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  astro-ph.IM, astro-ph.GA, cs.CV, cs.LG, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In radio astronomy, signals from radio telescopes are transformed into images
of observed celestial objects, or sources. However, these images, called dirty
images, contain real sources as well as artifacts due to signal sparsity and
other factors. Therefore, radio interferometric image reconstruction is
performed on dirty images, aiming to produce clean images in which artifacts
are reduced and real sources are recovered. So far, existing methods have
limited success on recovering faint sources, preserving detailed structures,
and eliminating artifacts. In this paper, we present VIC-DDPM, a Visibility and
Image Conditioned Denoising Diffusion Probabilistic Model. Our main idea is to
use both the original visibility data in the spectral domain and dirty images
in the spatial domain to guide the image generation process with DDPM. This
way, we can leverage DDPM to generate fine details and eliminate noise, while
utilizing visibility data to separate signals from noise and retaining spatial
information in dirty images. We have conducted experiments in comparison with
both traditional methods and recent deep learning based approaches. Our results
show that our method significantly improves the resulting images by reducing
artifacts, preserving fine details, and recovering dim sources. This
advancement further facilitates radio astronomical data analysis tasks on
celestial phenomena.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Models for Plug-and-Play Image Restoration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 15, 2023 </span>    
         <span class="authors"> Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, Luc Van Gool </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.08995" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Plug-and-play Image Restoration (IR) has been widely recognized as a flexible
and interpretable method for solving various inverse problems by utilizing any
off-the-shelf denoiser as the implicit image prior. However, most existing
methods focus on discriminative Gaussian denoisers. Although diffusion models
have shown impressive performance for high-quality image synthesis, their
potential to serve as a generative denoiser prior to the plug-and-play IR
methods remains to be further explored. While several other attempts have been
made to adopt diffusion models for image restoration, they either fail to
achieve satisfactory results or typically require an unacceptable number of
Neural Function Evaluations (NFEs) during inference. This paper proposes
DiffPIR, which integrates the traditional plug-and-play method into the
diffusion sampling framework. Compared to plug-and-play IR methods that rely on
discriminative Gaussian denoisers, DiffPIR is expected to inherit the
generative ability of diffusion models. Experimental results on three
representative IR tasks, including super-resolution, image deblurring, and
inpainting, demonstrate that DiffPIR achieves state-of-the-art performance on
both the FFHQ and ImageNet datasets in terms of reconstruction faithfulness and
perceptual quality with no more than 100 NFEs. The source code is available at
{\url{https://github.com/yuanzhi-zhu/DiffPIR}}
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 15, 2023 </span>    
         <span class="authors"> Antoni Bigata Casademunt, Rodrigo Mira, Nikita Drobyshev, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.08854" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Speech-driven animation has gained significant traction in recent years, with
current methods achieving near-photorealistic results. However, the field
remains underexplored regarding non-verbal communication despite evidence
demonstrating its importance in human interaction. In particular, generating
laughter sequences presents a unique challenge due to the intricacy and nuances
of this behaviour. This paper aims to bridge this gap by proposing a novel
model capable of generating realistic laughter sequences, given a still
portrait and an audio clip containing laughter. We highlight the failure cases
of traditional facial animation methods and leverage recent advances in
diffusion models to produce convincing laughter videos. We train our model on a
diverse set of laughter datasets and introduce an evaluation metric
specifically designed for laughter. When compared with previous speech-driven
approaches, our model achieves state-of-the-art performance across all metrics,
even when these are re-trained for laughter generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Reproducible Extraction of Training Images from Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 15, 2023 </span>    
         <span class="authors"> Ryan Webster </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.08694" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, Carlini et al. demonstrated the widely used model Stable Diffusion
can regurgitate real training samples, which is troublesome from a copyright
perspective. In this work, we provide an efficient extraction attack on par
with the recent attack, with several order of magnitudes less network
evaluations. In the process, we expose a new phenomena, which we dub template
verbatims, wherein a diffusion model will regurgitate a training sample largely
in tact. Template verbatims are harder to detect as they require retrieval and
masking to correctly label. Furthermore, they are still generated by newer
systems, even those which de-duplicate their training set, and we give insight
into why they still appear during generation. We extract training images from
several state of the art systems, including Stable Diffusion 2.0, Deep Image
Floyd, and finally Midjourney v4. We release code to verify our extraction
attack, perform the attack, as well as all extracted prompts at
\url{https://github.com/ryanwebster90/onestep-extraction}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Common Diffusion Noise Schedules and Sample Steps are Flawed
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 15, 2023 </span>    
         <span class="authors"> Shanchuan Lin, Bingchen Liu, Jiashi Li, Xiao Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.08891" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We discover that common diffusion noise schedules do not enforce the last
timestep to have zero signal-to-noise ratio (SNR), and some implementations of
diffusion samplers do not start from the last timestep. Such designs are flawed
and do not reflect the fact that the model is given pure Gaussian noise at
inference, creating a discrepancy between training and inference. We show that
the flawed design causes real problems in existing implementations. In Stable
Diffusion, it severely limits the model to only generate images with medium
brightness and prevents it from generating very bright and dark samples. We
propose a few simple fixes: (1) rescale the noise schedule to enforce zero
terminal SNR; (2) train the model with v prediction; (3) change the sampler to
always start from the last timestep; (4) rescale classifier-free guidance to
prevent over-exposure. These simple changes ensure the diffusion process is
congruent between training and inference and allow the model to generate
samples more faithful to the original data distribution.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## TESS: Text-to-Text Self-Conditioned Simplex Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 15, 2023 </span>    
         <span class="authors"> Rabeeh Karimi Mahabadi, Jaesung Tae, Hamish Ivison, James Henderson, Iz Beltagy, Matthew E. Peters, Arman Cohan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.08379" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have emerged as a powerful paradigm for generation,
obtaining strong performance in various domains with continuous-valued inputs.
Despite the promises of fully non-autoregressive text generation, applying
diffusion models to natural language remains challenging due to its discrete
nature. In this work, we propose Text-to-text Self-conditioned Simplex
Diffusion (TESS), a text diffusion model that is fully non-autoregressive,
employs a new form of self-conditioning, and applies the diffusion process on
the logit simplex space rather than the typical learned embedding space.
Through extensive experiments on natural language understanding and generation
tasks including summarization, text simplification, paraphrase generation, and
question generation, we demonstrate that TESS outperforms state-of-the-art
non-autoregressive models and is competitive with pretrained autoregressive
sequence-to-sequence models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Meta-DM: Applications of Diffusion Models on Few-Shot Learning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 14, 2023 </span>    
         <span class="authors"> Wentao Hu, Xiurong Jiang, Jiarun Liu, Yuqi Yang, Hui Tian </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.08092" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In the field of few-shot learning (FSL), extensive research has focused on
improving network structures and training strategies. However, the role of data
processing modules has not been fully explored. Therefore, in this paper, we
propose Meta-DM, a generalized data processing module for FSL problems based on
diffusion models. Meta-DM is a simple yet effective module that can be easily
integrated with existing FSL methods, leading to significant performance
improvements in both supervised and unsupervised settings. We provide a
theoretical analysis of Meta-DM and evaluate its performance on several
algorithms. Our experiments show that combining Meta-DM with certain methods
achieves state-of-the-art results.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On enhancing the robustness of Vision Transformers: Defensive Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 14, 2023 </span>    
         <span class="authors"> Raza Imam, Muhammad Huzaifa, Mohammed El-Amine Azz </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.08031" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Privacy and confidentiality of medical data are of utmost importance in
healthcare settings. ViTs, the SOTA vision model, rely on large amounts of
patient data for training, which raises concerns about data security and the
potential for unauthorized access. Adversaries may exploit vulnerabilities in
ViTs to extract sensitive patient information and compromising patient privacy.
This work address these vulnerabilities to ensure the trustworthiness and
reliability of ViTs in medical applications. In this work, we introduced a
defensive diffusion technique as an adversarial purifier to eliminate
adversarial noise introduced by attackers in the original image. By utilizing
the denoising capabilities of the diffusion model, we employ a reverse
diffusion process to effectively eliminate the adversarial noise from the
attack sample, resulting in a cleaner image that is then fed into the ViT
blocks. Our findings demonstrate the effectiveness of the diffusion model in
eliminating attack-agnostic adversarial noise from images. Additionally, we
propose combining knowledge distillation with our framework to obtain a
lightweight student model that is both computationally efficient and robust
against gray box attacks. Comparison of our method with a SOTA baseline method,
SEViT, shows that our work is able to outperform the baseline. Extensive
experiments conducted on a publicly available Tuberculosis X-ray dataset
validate the computational efficiency and improved robustness achieved by our
proposed architecture.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 12, 2023 </span>    
         <span class="authors"> Muhammad Usman Akbar, Wuhao Wang, Anders Eklund </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.07644" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models were initially developed for text-to-image generation and
are now being utilized to generate high quality synthetic images. Preceded by
GANs, diffusion models have shown impressive results using various evaluation
metrics. However, commonly used metrics such as FID and IS are not suitable for
determining whether diffusion models are simply reproducing the training
images. Here we train StyleGAN and diffusion models, using BRATS20 and BRATS21
datasets, to synthesize brain tumor images, and measure the correlation between
the synthetic images and all training images. Our results show that diffusion
models are much more likely to memorize the training images, especially for
small datasets. Researchers should be careful when using diffusion models for
medical imaging, if the final goal is to share the synthetic images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Provably Convergent Schrödinger Bridge with Applications to Probabilistic Time Series Imputation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 12, 2023 </span>    
         <span class="authors"> Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao Yang, Yikai Zhang, Kashif Rasul, Shandian Zhe, Anderson Schneider, Yuriy Nevmyvaka </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.07247" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The Schr\"odinger bridge problem (SBP) is gaining increasing attention in
generative modeling and showing promising potential even in comparison with the
score-based generative models (SGMs). SBP can be interpreted as an
entropy-regularized optimal transport problem, which conducts projections onto
every other marginal alternatingly. However, in practice, only approximated
projections are accessible and their convergence is not well understood. To
fill this gap, we present a first convergence analysis of the Schr\"odinger
bridge algorithm based on approximated projections. As for its practical
applications, we apply SBP to probabilistic time series imputation by
generating missing values conditioned on observed data. We show that optimizing
the transport cost improves the performance and the proposed algorithm achieves
the state-of-the-art result in healthcare and environmental data while
exhibiting the advantage of exploring both temporal and feature patterns in
probabilistic time series imputation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Exploiting Diffusion Prior for Real-World Image Super-Resolution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 11, 2023 </span>    
         <span class="authors"> Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C. K. Chan, Chen Change Loy </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.07015" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a novel approach to leverage prior knowledge encapsulated in
pre-trained text-to-image diffusion models for blind super-resolution (SR).
Specifically, by employing our time-aware encoder, we can achieve promising
restoration results without altering the pre-trained synthesis model, thereby
preserving the generative prior and minimizing training cost. To remedy the
loss of fidelity caused by the inherent stochasticity of diffusion models, we
introduce a controllable feature wrapping module that allows users to balance
quality and fidelity by simply adjusting a scalar value during the inference
process. Moreover, we develop a progressive aggregation sampling strategy to
overcome the fixed-size constraints of pre-trained diffusion models, enabling
adaptation to resolutions of any size. A comprehensive evaluation of our method
using both synthetic and real-world benchmarks demonstrates its superiority
over current state-of-the-art approaches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MolDiff: Addressing the Atom-Bond Inconsistency Problem in 3D Molecule Diffusion Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 11, 2023 </span>    
         <span class="authors"> Xingang Peng, Jiaqi Guan, Qiang Liu, Jianzhu Ma </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.07508" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.LG, q-bio.QM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep generative models have recently achieved superior performance in 3D
molecule generation. Most of them first generate atoms and then add chemical
bonds based on the generated atoms in a post-processing manner. However, there
might be no corresponding bond solution for the temporally generated atoms as
their locations are generated without considering potential bonds. We define
this problem as the atom-bond inconsistency problem and claim it is the main
reason for current approaches to generating unrealistic 3D molecules. To
overcome this problem, we propose a new diffusion model called MolDiff which
can generate atoms and bonds simultaneously while still maintaining their
consistency by explicitly modeling the dependence between their relationships.
We evaluated the generation ability of our proposed model and the quality of
the generated molecules using criteria related to both geometry and chemical
properties. The empirical studies showed that our model outperforms previous
approaches, achieving a three-fold improvement in success rate and generating
molecules with significantly better quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Analyzing Bias in Diffusion-based Face Generation Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 10, 2023 </span>    
         <span class="authors"> Malsha V. Perera, Vishal M. Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.06402" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are becoming increasingly popular in synthetic data
generation and image editing applications. However, these models can amplify
existing biases and propagate them to downstream applications. Therefore, it is
crucial to understand the sources of bias in their outputs. In this paper, we
investigate the presence of bias in diffusion-based face generation models with
respect to attributes such as gender, race, and age. Moreover, we examine how
dataset size affects the attribute composition and perceptual quality of both
diffusion and Generative Adversarial Network (GAN) based face generation models
across various attribute classes. Our findings suggest that diffusion models
tend to worsen distribution bias in the training data for various attributes,
which is heavily influenced by the size of the dataset. Conversely, GAN models
trained on balanced datasets with a larger number of samples show less bias
across different attributes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Relightify: Relightable 3D Faces from a Single Image via Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 10, 2023 </span>    
         <span class="authors"> Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Stefanos Zafeiriou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.06077" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Following the remarkable success of diffusion models on image generation,
recent works have also demonstrated their impressive ability to address a
number of inverse problems in an unsupervised way, by properly constraining the
sampling process based on a conditioning input. Motivated by this, in this
paper, we present the first approach to use diffusion models as a prior for
highly accurate 3D facial BRDF reconstruction from a single image. We start by
leveraging a high-quality UV dataset of facial reflectance (diffuse and
specular albedo and normals), which we render under varying illumination
settings to simulate natural RGB textures and, then, train an unconditional
diffusion model on concatenated pairs of rendered textures and reflectance
components. At test time, we fit a 3D morphable model to the given image and
unwrap the face in a partial UV texture. By sampling from the diffusion model,
while retaining the observed texture part intact, the model inpaints not only
the self-occluded areas but also the unknown reflectance components, in a
single sequence of denoising steps. In contrast to existing methods, we
directly acquire the observed texture from the input image, thus, resulting in
more faithful and consistent reflectance estimation. Through a series of
qualitative and quantitative comparisons, we demonstrate superior performance
in both texture completion as well as reflectance reconstruction tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-based Signal Refiner for Speech Enhancement
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 10, 2023 </span>    
         <span class="authors"> Masato Hirano, Kazuki Shimada, Yuichiro Koyama, Shusuke Takahashi, Yuki Mitsufuji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.05857" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We have developed a diffusion-based speech refiner that improves the
reference-free perceptual quality of the audio predicted by preceding
single-channel speech separation models. Although modern deep neural
network-based speech separation models have show high performance in
reference-based metrics, they often produce perceptually unnatural artifacts.
The recent advancements made to diffusion models motivated us to tackle this
problem by restoring the degraded parts of initial separations with a
generative approach. Utilizing the denoising diffusion restoration model (DDRM)
as a basis, we propose a shared DDRM-based refiner that generates samples
conditioned on the global information of preceding outputs from arbitrary
speech separation models. We experimentally show that our refiner can provide a
clearer harmonic structure of speech and improves the reference-free metric of
perceptual quality for arbitrary preceding model architectures. Furthermore, we
tune the variance of the measurement noise based on preceding outputs, which
results in higher scores in both reference-free and reference-based metrics.
The separation quality can also be further improved by blending the
discriminative and generative outputs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Controllable Light Diffusion for Portraits
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 08, 2023 </span>    
         <span class="authors"> David Futschik, Kelvin Ritland, James Vecore, Sean Fanello, Sergio Orts-Escolano, Brian Curless, Daniel Sýkora, Rohit Pandey </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.04745" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, I.4.3
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce light diffusion, a novel method to improve lighting in
portraits, softening harsh shadows and specular highlights while preserving
overall scene illumination. Inspired by professional photographers' diffusers
and scrims, our method softens lighting given only a single portrait photo.
Previous portrait relighting approaches focus on changing the entire lighting
environment, removing shadows (ignoring strong specular highlights), or
removing shading entirely. In contrast, we propose a learning based method that
allows us to control the amount of light diffusion and apply it on in-the-wild
portraits. Additionally, we design a method to synthetically generate plausible
external shadows with sub-surface scattering effects while conforming to the
shape of the subject's face. Finally, we show how our approach can increase the
robustness of higher level vision applications, such as albedo estimation,
geometry estimation and semantic segmentation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 08, 2023 </span>    
         <span class="authors"> Yupei Lin, Sen Zhang, Xiaojun Yang, Xiao Wang, Yukai Shi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.04651" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large-scale text-to-image models have demonstrated amazing ability to
synthesize diverse and high-fidelity images. However, these models are often
violated by several limitations. Firstly, they require the user to provide
precise and contextually relevant descriptions for the desired image
modifications. Secondly, current models can impose significant changes to the
original image content during the editing process. In this paper, we explore
ReGeneration learning in an image-to-image Diffusion model (ReDiffuser), that
preserves the content of the original image without human prompting and the
requisite editing direction is automatically discovered within the text
embedding space. To ensure consistent preservation of the shape during image
editing, we propose cross-attention guidance based on regeneration learning.
This novel approach allows for enhanced expression of the target domain
features while preserving the original shape of the image. In addition, we
introduce a cooperative update strategy, which allows for efficient
preservation of the original shape of an image, thereby improving the quality
and consistency of shape preservation throughout the editing process. Our
proposed method leverages an existing pre-trained text-image diffusion model
without any additional training. Extensive experiments show that the proposed
method outperforms existing work in both real and synthetic image editing.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Real-World Denoising via Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 08, 2023 </span>    
         <span class="authors"> Cheng Yang, Lijing Liang, Zhixun Su </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.04457" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Real-world image denoising is an extremely important image processing
problem, which aims to recover clean images from noisy images captured in
natural environments. In recent years, diffusion models have achieved very
promising results in the field of image generation, outperforming previous
generation models. However, it has not been widely used in the field of image
denoising because it is difficult to control the appropriate position of the
added noise. Inspired by diffusion models, this paper proposes a novel general
denoising diffusion model that can be used for real-world image denoising. We
introduce a diffusion process with linear interpolation, and the intermediate
noisy image is interpolated from the original clean image and the corresponding
real-world noisy image, so that this diffusion model can handle the level of
added noise. In particular, we also introduce two sampling algorithms for this
diffusion model. The first one is a simple sampling procedure defined according
to the diffusion process, and the second one targets the problem of the first
one and makes a number of improvements. Our experimental results show that our
proposed method with a simple CNNs Unet achieves comparable results compared to
the Transformer architecture. Both quantitative and qualitative evaluations on
real-world denoising benchmarks show that the proposed general diffusion model
performs almost as well as against the state-of-the-art methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Variational Perspective on Solving Inverse Problems with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 07, 2023 </span>    
         <span class="authors"> Morteza Mardani, Jiaming Song, Jan Kautz, Arash Vahdat </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.04391" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.NA, math.NA, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have emerged as a key pillar of foundation models in visual
domains. One of their critical applications is to universally solve different
downstream inverse tasks via a single diffusion prior without re-training for
each task. Most inverse tasks can be formulated as inferring a posterior
distribution over data (e.g., a full image) given a measurement (e.g., a masked
image). This is however challenging in diffusion models since the nonlinear and
iterative nature of the diffusion process renders the posterior intractable. To
cope with this challenge, we propose a variational approach that by design
seeks to approximate the true posterior distribution. We show that our approach
naturally leads to regularization by denoising diffusion process (RED-Diff)
where denoisers at different timesteps concurrently impose different structural
constraints over the image. To gauge the contribution of denoisers from
different timesteps, we propose a weighting mechanism based on
signal-to-noise-ratio (SNR). Our approach provides a new variational
perspective for solving inverse problems with diffusion models, allowing us to
formulate sampling as stochastic optimization, where one can simply apply
off-the-shelf solvers with lightweight iterates. Our experiments for image
restoration tasks such as inpainting and superresolution demonstrate the
strengths of our method compared with state-of-the-art sampling-based diffusion
models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Latent Diffusion Model for Protein Structure Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 06, 2023 </span>    
         <span class="authors"> Cong Fu, Keqiang Yan, Limei Wang, Wing Yee Au, Michael McThrow, Tao Komikado, Koji Maruhashi, Kanji Uchino, Xiaoning Qian, Shuiwang Ji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.04120" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Proteins are complex biomolecules that perform a variety of crucial functions
within living organisms. Designing and generating novel proteins can pave the
way for many future synthetic biology applications, including drug discovery.
However, it remains a challenging computational task due to the large modeling
space of protein structures. In this study, we propose a latent diffusion model
that can reduce the complexity of protein modeling while flexibly capturing the
distribution of natural protein structures in a condensed latent space.
Specifically, we propose an equivariant protein autoencoder that embeds
proteins into a latent space and then uses an equivariant diffusion model to
learn the distribution of the latent protein representations. Experimental
results demonstrate that our method can effectively generate novel protein
backbone structures with high designability and efficiency.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 06, 2023 </span>    
         <span class="authors"> Xiaohui Chen, Jiaxing He, Xu Han, Li-Ping Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.04111" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.SI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based generative graph models have been proven effective in
generating high-quality small graphs. However, they need to be more scalable
for generating large graphs containing thousands of nodes desiring graph
statistics. In this work, we propose EDGE, a new diffusion-based generative
graph model that addresses generative tasks with large graphs. To improve
computation efficiency, we encourage graph sparsity by using a discrete
diffusion process that randomly removes edges at each time step and finally
obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at
each denoising step. It makes much fewer edge predictions than previous
diffusion-based models. Moreover, EDGE admits explicitly modeling the node
degrees of the graphs, further improving the model performance. The empirical
study shows that EDGE is much more efficient than competing methods and can
generate large graphs with thousands of nodes. It also outperforms baseline
models in generation quality: graphs generated by our approach have more
similar graph statistics to those of the training graphs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 06, 2023 </span>    
         <span class="authors"> Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.03935" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have exhibited excellent performance in various domains. The
probability flow ordinary differential equation (ODE) of diffusion models
(i.e., diffusion ODEs) is a particular case of continuous normalizing flows
(CNFs), which enables deterministic inference and exact likelihood evaluation.
However, the likelihood estimation results by diffusion ODEs are still far from
those of the state-of-the-art likelihood-based generative models. In this work,
we propose several improved techniques for maximum likelihood estimation for
diffusion ODEs, including both training and evaluation perspectives. For
training, we propose velocity parameterization and explore variance reduction
techniques for faster convergence. We also derive an error-bounded high-order
flow matching objective for finetuning, which improves the ODE likelihood and
smooths its trajectory. For evaluation, we propose a novel training-free
truncated-normal dequantization to fill the training-evaluation gap commonly
existing in diffusion ODEs. Building upon these techniques, we achieve
state-of-the-art likelihood estimation results on image datasets (2.56 on
CIFAR-10, 3.43 on ImageNet-32) without variational dequantization or data
augmentation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DocDiff: Document Enhancement via Residual Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 06, 2023 </span>    
         <span class="authors"> Zongyuan Yang, Baolin Liu, Yongping Xiong, Lan Yi, Guibin Wu, Xiaojun Tang, Ziqi Liu, Junjie Zhou, Xing Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.03892" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Removing degradation from document images not only improves their visual
quality and readability, but also enhances the performance of numerous
automated document analysis and recognition tasks. However, existing
regression-based methods optimized for pixel-level distortion reduction tend to
suffer from significant loss of high-frequency information, leading to
distorted and blurred text edges. To compensate for this major deficiency, we
propose DocDiff, the first diffusion-based framework specifically designed for
diverse challenging document enhancement problems, including document
deblurring, denoising, and removal of watermarks and seals. DocDiff consists of
two modules: the Coarse Predictor (CP), which is responsible for recovering the
primary low-frequency content, and the High-Frequency Residual Refinement (HRR)
module, which adopts the diffusion models to predict the residual
(high-frequency information, including text edges), between the ground-truth
and the CP-predicted image. DocDiff is a compact and computationally efficient
model that benefits from a well-designed network architecture, an optimized
training loss objective, and a deterministic sampling process with short time
steps. Extensive experiments demonstrate that DocDiff achieves state-of-the-art
(SOTA) performance on multiple benchmark datasets, and can significantly
enhance the readability and recognizability of degraded document images.
Furthermore, our proposed HRR module in pre-trained DocDiff is plug-and-play
and ready-to-use, with only 4.17M parameters. It greatly sharpens the text
edges generated by SOTA deblurring methods without additional joint training.
Available codes: https://github.com/Royalvice/DocDiff
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 05, 2023 </span>    
         <span class="authors"> Leming Guo, Wanli Xue, Qing Guo, Yuxi Zhou, Tiantian Yuan, Shengyong Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.03614" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we are dedicated to leveraging the denoising diffusion models'
success and formulating feature refinement as the autoencoder-formed diffusion
process. The state-of-the-art CSLR framework consists of a spatial module, a
visual module, a sequence module, and a sequence learning function. However,
this framework has faced sequence module overfitting caused by the objective
function and small-scale available benchmarks, resulting in insufficient model
training. To overcome the overfitting problem, some CSLR studies enforce the
sequence module to learn more visual temporal information or be guided by more
informative supervision to refine its representations. In this work, we propose
a novel autoencoder-formed conditional diffusion feature refinement~(ACDR) to
refine the sequence representations to equip desired properties by learning the
encoding-decoding optimization process in an end-to-end way. Specifically, for
the ACDR, a noising Encoder is proposed to progressively add noise equipped
with semantic conditions to the sequence representations. And a denoising
Decoder is proposed to progressively denoise the noisy sequence representations
with semantic conditions. Therefore, the sequence representations can be imbued
with the semantics of provided semantic conditions. Further, a semantic
constraint is employed to prevent the denoised sequence representations from
semantic corruption. Extensive experiments are conducted to validate the
effectiveness of our ACDR, benefiting state-of-the-art methods and achieving a
notable gain on three benchmarks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 04, 2023 </span>    
         <span class="authors"> Seongmin Lee, Benjamin Hoover, Hendrik Strobelt, Zijie J. Wang, ShengYun Peng, Austin Wright, Kevin Li, Haekyu Park, Haoyang Yang, Duen Horng Chau </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.03509" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.AI, cs.HC, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based generative models' impressive ability to create convincing
images has captured global attention. However, their complex internal
structures and operations often make them difficult for non-experts to
understand. We present Diffusion Explainer, the first interactive visualization
tool that explains how Stable Diffusion transforms text prompts into images.
Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's
complex components with detailed explanations of their underlying operations,
enabling users to fluidly transition between multiple levels of abstraction
through animations and interactive elements. By comparing the evolutions of
image representations guided by two related text prompts over refinement
timesteps, users can discover the impact of prompts on image generation.
Diffusion Explainer runs locally in users' web browsers without the need for
installation or specialized hardware, broadening the public's education access
to modern AI techniques. Our open-sourced tool is available at:
https://poloclub.github.io/diffusion-explainer/. A video demo is available at
https://youtu.be/Zg4gxdIWDds.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 04, 2023 </span>    
         <span class="authors"> Chao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang, Jiangning Zhang, Ying Tai, Yong Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.02594" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Multimodal-driven talking face generation refers to animating a portrait with
the given pose, expression, and gaze transferred from the driving image and
video, or estimated from the text and audio. However, existing methods ignore
the potential of text modal, and their generators mainly follow the
source-oriented feature rearrange paradigm coupled with unstable GAN
frameworks. In this work, we first represent the emotion in the text prompt,
which could inherit rich semantics from the CLIP, allowing flexible and
generalized emotion control. We further reorganize these tasks as the
target-oriented texture transfer and adopt the Diffusion Models. More
specifically, given a textured face as the source and the rendered face
projected from the desired 3DMM coefficients as the target, our proposed
Texture-Geometry-aware Diffusion Model decomposes the complex transfer problem
into multi-conditional denoising process, where a Texture Attention-based
module accurately models the correspondences between appearance and geometry
cues contained in source and target conditions, and incorporate extra implicit
information for high-fidelity talking face generation. Additionally, TGDM can
be gracefully tailored for face swapping. We derive a novel paradigm free of
unstable seesaw-style optimization, resulting in simple, stable, and effective
training and inference schemes. Extensive experiments demonstrate the
superiority of our method.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LayoutDM: Transformer-based Diffusion Model for Layout Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 04, 2023 </span>    
         <span class="authors"> Shang Chai, Liansheng Zhuang, Fengying Yan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.02567" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Automatic layout generation that can synthesize high-quality layouts is an
important tool for graphic design in many applications. Though existing methods
based on generative models such as Generative Adversarial Networks (GANs) and
Variational Auto-Encoders (VAEs) have progressed, they still leave much room
for improving the quality and diversity of the results. Inspired by the recent
success of diffusion models in generating high-quality images, this paper
explores their potential for conditional layout generation and proposes
Transformer-based Layout Diffusion Model (LayoutDM) by instantiating the
conditional denoising diffusion probabilistic model (DDPM) with a purely
transformer-based architecture. Instead of using convolutional neural networks,
a transformer-based conditional Layout Denoiser is proposed to learn the
reverse diffusion process to generate samples from noised layout data.
Benefitting from both transformer and DDPM, our LayoutDM is of desired
properties such as high-quality generation, strong sample diversity, faithful
distribution coverage, and stationary training in comparison to GANs and VAEs.
Quantitative and qualitative experimental results show that our method
outperforms state-of-the-art generative models in terms of quality and
diversity.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 02, 2023 </span>    
         <span class="authors"> Asad Aali, Marius Arvinte, Sidharth Kumar, Jonathan I. Tamir </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.01166" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, eess.IV, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present SURE-Score: an approach for learning score-based generative models
using training samples corrupted by additive Gaussian noise. When a large
training set of clean samples is available, solving inverse problems via
score-based (diffusion) generative models trained on the underlying
fully-sampled data distribution has recently been shown to outperform
end-to-end supervised deep learning. In practice, such a large collection of
training data may be prohibitively expensive to acquire in the first place. In
this work, we present an approach for approximately learning a score-based
generative model of the clean distribution, from noisy training data. We
formulate and justify a novel loss function that leverages Stein's unbiased
risk estimate to jointly denoise the data and learn the score function via
denoising score matching, while using only the noisy samples. We demonstrate
the generality of SURE-Score by learning priors and applying posterior sampling
to ill-posed inverse problems in two practical applications from different
domains: compressive wireless multiple-input multiple-output channel estimation
and accelerated 2D multi-coil magnetic resonance imaging reconstruction, where
we demonstrate competitive reconstruction performance when learning at
signal-to-noise ratio values of 0 and 10 dB, respectively.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## In-Context Learning Unlocked for Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 01, 2023 </span>    
         <span class="authors"> Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.01115" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Prompt Diffusion, a framework for enabling in-context learning in
diffusion-based generative models. Given a pair of task-specific example
images, such as depth from/to image and scribble from/to image, and a text
guidance, our model automatically understands the underlying task and performs
the same task on a new query image following the text guidance. To achieve
this, we propose a vision-language prompt that can model a wide range of
vision-language tasks and a diffusion model that takes it as input. The
diffusion model is trained jointly over six different tasks using these
prompts. The resulting Prompt Diffusion model is the first diffusion-based
vision-language foundation model capable of in-context learning. It
demonstrates high-quality in-context generation on the trained tasks and
generalizes effectively to new, unseen vision tasks with their respective
prompts. Our model also shows compelling text-guided image editing results. Our
framework, with code publicly available at
https://github.com/Zhendong-Wang/Prompt-Diffusion, aims to facilitate research
into in-context learning for computer vision.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Class-Balancing Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 30, 2023 </span>    
         <span class="authors"> Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, Ya Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.00562" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based models have shown the merits of generating high-quality
visual data while preserving better diversity in recent studies. However, such
observation is only justified with curated data distribution, where the data
samples are nicely pre-processed to be uniformly distributed in terms of their
labels. In practice, a long-tailed data distribution appears more common and
how diffusion models perform on such class-imbalanced data remains unknown. In
this work, we first investigate this problem and observe significant
degradation in both diversity and fidelity when the diffusion model is trained
on datasets with class-imbalanced distributions. Especially in tail classes,
the generations largely lose diversity and we observe severe mode-collapse
issues. To tackle this problem, we set from the hypothesis that the data
distribution is not class-balanced, and propose Class-Balancing Diffusion
Models (CBDM) that are trained with a distribution adjustment regularizer as a
solution. Experiments show that images generated by CBDM exhibit higher
diversity and quality in both quantitative and qualitative ways. Our method
benchmarked the generation results on CIFAR100/CIFAR100LT dataset and shows
outstanding performance on the downstream recognition task.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Cycle-guided Denoising Diffusion Probability Model for 3D Cross-modality MRI Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 28, 2023 </span>    
         <span class="authors"> Shaoyan Pan, Chih-Wei Chang, Junbo Peng, Jiahan Zhang, Richard L. J. Qiu, Tonghe Wang, Justin Roper, Tian Liu, Hui Mao, Xiaofeng Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2305.00042" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This study aims to develop a novel Cycle-guided Denoising Diffusion
Probability Model (CG-DDPM) for cross-modality MRI synthesis. The CG-DDPM
deploys two DDPMs that condition each other to generate synthetic images from
two different MRI pulse sequences. The two DDPMs exchange random latent noise
in the reverse processes, which helps to regularize both DDPMs and generate
matching images in two modalities. This improves image-to-image translation
ac-curacy. We evaluated the CG-DDPM quantitatively using mean absolute error
(MAE), multi-scale structural similarity index measure (MSSIM), and peak
sig-nal-to-noise ratio (PSNR), as well as the network synthesis consistency, on
the BraTS2020 dataset. Our proposed method showed high accuracy and reliable
consistency for MRI synthesis. In addition, we compared the CG-DDPM with
several other state-of-the-art networks and demonstrated statistically
significant improvements in the image quality of synthetic MRIs. The proposed
method enhances the capability of current multimodal MRI synthesis approaches,
which could contribute to more accurate diagnosis and better treatment planning
for patients by synthesizing additional MRI modalities.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Motion-Conditioned Diffusion Model for Controllable Video Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 27, 2023 </span>    
         <span class="authors"> Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, Ming-Hsuan Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.14404" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advancements in diffusion models have greatly improved the quality and
diversity of synthesized content. To harness the expressive power of diffusion
models, researchers have explored various controllable mechanisms that allow
users to intuitively guide the content synthesis process. Although the latest
efforts have primarily focused on video synthesis, there has been a lack of
effective methods for controlling and describing desired content and motion. In
response to this gap, we introduce MCDiff, a conditional diffusion model that
generates a video from a starting image frame and a set of strokes, which allow
users to specify the intended content and dynamics for synthesis. To tackle the
ambiguity of sparse motion inputs and achieve better synthesis quality, MCDiff
first utilizes a flow completion model to predict the dense video motion based
on the semantic understanding of the video frame and the sparse motion control.
Then, the diffusion model synthesizes high-quality future frames to form the
output video. We qualitatively and quantitatively show that MCDiff achieves the
state-the-of-art visual quality in stroke-guided controllable video synthesis.
Additional experiments on MPII Human Pose further exhibit the capability of our
model on diverse content and motion synthesis.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffuseExpand: Expanding dataset for 2D medical image segmentation using diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 26, 2023 </span>    
         <span class="authors"> Shitong Shao, Xiaohan Yuan, Zhen Huang, Ziming Qiu, Shuai Wang, Kevin Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.13416" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Dataset expansion can effectively alleviate the problem of data scarcity for
medical image segmentation, due to privacy concerns and labeling difficulties.
However, existing expansion algorithms still face great challenges due to their
inability of guaranteeing the diversity of synthesized images with paired
segmentation masks. In recent years, Diffusion Probabilistic Models (DPMs) have
shown powerful image synthesis performance, even better than Generative
Adversarial Networks. Based on this insight, we propose an approach called
DiffuseExpand for expanding datasets for 2D medical image segmentation using
DPM, which first samples a variety of masks from Gaussian noise to ensure the
diversity, and then synthesizes images to ensure the alignment of images and
masks. After that, DiffuseExpand chooses high-quality samples to further
enhance the effectiveness of data expansion. Our comparison and ablation
experiments on COVID-19 and CGMH Pelvis datasets demonstrate the effectiveness
of DiffuseExpand. Our code is released at
https://anonymous.4open.science/r/DiffuseExpand.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 26, 2023 </span>    
         <span class="authors"> Zihao Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.13224" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The proposed BSDE-based diffusion model represents a novel approach to
diffusion modeling, which extends the application of stochastic differential
equations (SDEs) in machine learning. Unlike traditional SDE-based diffusion
models, our model can determine the initial conditions necessary to reach a
desired terminal distribution by adapting an existing score function. We
demonstrate the theoretical guarantees of the model, the benefits of using
Lipschitz networks for score matching, and its potential applications in
various areas such as diffusion inversion, conditional diffusion, and
uncertainty quantification. Our work represents a contribution to the field of
score-based generative learning and offers a promising direction for solving
real-world problems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Single-View Height Estimation with Conditional Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 26, 2023 </span>    
         <span class="authors"> Isaac Corley, Peyman Najafirad </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.13214" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Digital Surface Models (DSM) offer a wealth of height information for
understanding the Earth's surface as well as monitoring the existence or change
in natural and man-made structures. Classical height estimation requires
multi-view geospatial imagery or LiDAR point clouds which can be expensive to
acquire. Single-view height estimation using neural network based models shows
promise however it can struggle with reconstructing high resolution features.
The latest advancements in diffusion models for high resolution image synthesis
and editing have yet to be utilized for remote sensing imagery, particularly
height estimation. Our approach involves training a generative diffusion model
to learn the joint distribution of optical and DSM images across both domains
as a Markov chain. This is accomplished by minimizing a denoising score
matching objective while being conditioned on the source image to generate
realistic high resolution 3D surfaces. In this paper we experiment with
conditional denoising diffusion probabilistic models (DDPM) for height
estimation from a single remotely sensed image and show promising results on
the Vaihingen benchmark dataset.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 25, 2023 </span>    
         <span class="authors"> Jussi Leinonen, Ulrich Hamann, Daniele Nerini, Urs Germann, Gabriele Franch </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.12891" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  physics.ao-ph, cs.LG, eess.IV, I.2.10; J.2
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have been widely adopted in image generation, producing
higher-quality and more diverse samples than generative adversarial networks
(GANs). We introduce a latent diffusion model (LDM) for precipitation
nowcasting - short-term forecasting based on the latest observational data. The
LDM is more stable and requires less computation to train than GANs, albeit
with more computationally expensive generation. We benchmark it against the
GAN-based Deep Generative Models of Rainfall (DGMR) and a statistical model,
PySTEPS. The LDM produces more accurate precipitation predictions, while the
comparisons are more mixed when predicting whether the precipitation exceeds
predefined thresholds. The clearest advantage of the LDM is that it generates
more diverse predictions than DGMR or PySTEPS. Rank distribution tests indicate
that the distribution of samples from the LDM accurately reflects the
uncertainty of the predictions. Thus, LDMs are promising for any applications
where uncertainty quantification is important, such as weather and climate.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Probabilistic Model Based Accurate and High-Degree-of-Freedom Metasurface Inverse Design
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 25, 2023 </span>    
         <span class="authors"> Zezhou Zhang, Chuanchuan Yang, Yifeng Qin, Hao Feng, Jiqiang Feng, Hongbin Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.13038" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, physics.optics
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Conventional meta-atom designs rely heavily on researchers' prior knowledge
and trial-and-error searches using full-wave simulations, resulting in
time-consuming and inefficient processes. Inverse design methods based on
optimization algorithms, such as evolutionary algorithms, and topological
optimizations, have been introduced to design metamaterials. However, none of
these algorithms are general enough to fulfill multi-objective tasks. Recently,
deep learning methods represented by Generative Adversarial Networks (GANs)
have been applied to inverse design of metamaterials, which can directly
generate high-degree-of-freedom meta-atoms based on S-parameter requirements.
However, the adversarial training process of GANs makes the network unstable
and results in high modeling costs. This paper proposes a novel metamaterial
inverse design method based on the diffusion probability theory. By learning
the Markov process that transforms the original structure into a Gaussian
distribution, the proposed method can gradually remove the noise starting from
the Gaussian distribution and generate new high-degree-of-freedom meta-atoms
that meet S-parameter conditions, which avoids the model instability introduced
by the adversarial training process of GANs and ensures more accurate and
high-quality generation results. Experiments have proven that our method is
superior to representative methods of GANs in terms of model convergence speed,
generation accuracy, and quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 25, 2023 </span>    
         <span class="authors"> Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen, Mingyuan Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.12526" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are powerful, but they require a lot of time and data to
train. We propose Patch Diffusion, a generic patch-wise training framework, to
significantly reduce the training time costs while improving data efficiency,
which thus helps democratize diffusion model training to broader users. At the
core of our innovations is a new conditional score function at the patch level,
where the patch location in the original image is included as additional
coordinate channels, while the patch size is randomized and diversified
throughout training to encode the cross-region dependency at multiple scales.
Sampling with our method is as easy as in the original diffusion model. Through
Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while
maintaining comparable or better generation quality. Patch Diffusion meanwhile
improves the performance of diffusion models trained on relatively small
datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve
state-of-the-art FID scores 1.77 on CelebA-64$\times$64 and 1.93 on
AFHQv2-Wild-64$\times$64. We will share our code and pre-trained models soon.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## RenderDiffusion: Text Generation as Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 25, 2023 </span>    
         <span class="authors"> Junyi Li, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.12519" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have become a new generative paradigm for text generation.
Considering the discrete categorical nature of text, in this paper, we propose
GlyphDiffusion, a novel diffusion approach for text generation via text-guided
image generation. Our key idea is to render the target text as a glyph image
containing visual language content. In this way, conditional text generation
can be cast as a glyph image generation task, and it is then natural to apply
continuous diffusion models to discrete texts. Specially, we utilize a cascaded
architecture (ie a base and a super-resolution diffusion model) to generate
high-fidelity glyph images, conditioned on the input text. Furthermore, we
design a text grounding module to transform and refine the visual language
content from generated glyph images into the final texts. In experiments over
four conditional text generation tasks and two classes of metrics (ie quality
and diversity), GlyphDiffusion can achieve comparable or even better results
than several baselines, including pretrained language models. Our model also
makes significant improvements compared to the recent diffusion model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Variational Diffusion Auto-encoder: Deep Latent Variable Model with Unconditional Diffusion Prior
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 24, 2023 </span>    
         <span class="authors"> Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.12141" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 As a widely recognized approach to deep generative modeling, Variational
Auto-Encoders (VAEs) still face challenges with the quality of generated
images, often presenting noticeable blurriness. This issue stems from the
unrealistic assumption that approximates the conditional data distribution,
$p(\textbf{x} | \textbf{z})$, as an isotropic Gaussian. In this paper, we
propose a novel solution to address these issues. We illustrate how one can
extract a latent space from a pre-existing diffusion model by optimizing an
encoder to maximize the marginal data log-likelihood. Furthermore, we
demonstrate that a decoder can be analytically derived post encoder-training,
employing the Bayes rule for scores. This leads to a VAE-esque deep latent
variable model, which discards the need for Gaussian assumptions on
$p(\textbf{x} | \textbf{z})$ or the training of a separate decoder network. Our
method, which capitalizes on the strengths of pre-trained diffusion models and
equips them with latent spaces, results in a significant enhancement to the
performance of VAEs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 24, 2023 </span>    
         <span class="authors"> Zeyu Lu, Chengyue Wu, Xinyuan Chen, Yaohui Wang, Lei Bai, Yu Qiao, Xihui Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.11829" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have attained impressive visual quality for image synthesis.
However, how to interpret and manipulate the latent space of diffusion models
has not been extensively explored. Prior work diffusion autoencoders encode the
semantic representations into a semantic latent code, which fails to reflect
the rich information of details and the intrinsic feature hierarchy. To
mitigate those limitations, we propose Hierarchical Diffusion Autoencoders
(HDAE) that exploit the fine-grained-to-abstract and lowlevel-to-high-level
feature hierarchy for the latent space of diffusion models. The hierarchical
latent space of HDAE inherently encodes different abstract levels of semantics
and provides more comprehensive semantic representations. In addition, we
propose a truncated-feature-based approach for disentangled image manipulation.
We demonstrate the effectiveness of our proposed approach with extensive
experiments and applications on image reconstruction, style mixing,
controllable interpolation, detail-preserving and disentangled image
manipulation, and multi-modal semantic image synthesis.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Diffusion Models as Principled Priors for Inverse Imaging
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 23, 2023 </span>    
         <span class="authors"> Berthy T. Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L. Bouman, William T. Freeman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.11751" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 It is important in computational imaging to understand the uncertainty of
images reconstructed from imperfect measurements. We propose turning
score-based diffusion models into principled priors (``score-based priors'')
for analyzing a posterior of images given measurements. Previously,
probabilistic priors were limited to handcrafted regularizers and simple
distributions. In this work, we empirically validate the theoretically-proven
probability function of a score-based diffusion model. We show how to sample
from resulting posteriors by using this probability function for variational
inference. Our results, including experiments on denoising, deblurring, and
interferometric imaging, suggest that score-based priors enable principled
inference with a sophisticated, data-driven image prior.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffVoice: Text-to-Speech with Latent Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 23, 2023 </span>    
         <span class="authors"> Zhijun Liu, Yiwei Guo, Kai Yu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.11750" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.AI, cs.HC, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we present DiffVoice, a novel text-to-speech model based on
latent diffusion. We propose to first encode speech signals into a phoneme-rate
latent representation with a variational autoencoder enhanced by adversarial
training, and then jointly model the duration and the latent representation
with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS
datasets demonstrate that our method beats the best publicly available systems
in naturalness. By adopting recent generative inverse problem solving
algorithms for diffusion models, DiffVoice achieves the state-of-the-art
performance in text-based speech editing, and zero-shot adaptation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 22, 2023 </span>    
         <span class="authors"> Guoqiang Zhang, Niwa Kenta, W. Bastiaan Kleijn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.11328" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.NA, math.NA
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 One popular diffusion-based sampling strategy attempts to solve the reverse
ordinary differential equations (ODEs) effectively. The coefficients of the
obtained ODE solvers are pre-determined by the ODE formulation, the reverse
discrete timesteps, and the employed ODE methods. In this paper, we consider
accelerating several popular ODE-based sampling processes by optimizing certain
coefficients via improved integration approximation (IIA). At each reverse
timestep, we propose to minimize a mean squared error (MSE) function with
respect to certain selected coefficients. The MSE is constructed by applying
the original ODE solver for a set of fine-grained timesteps which in principle
provides a more accurate integration approximation in predicting the next
diffusion hidden state. Given a pre-trained diffusion model, the procedure for
IIA for a particular number of neural functional evaluations (NFEs) only needs
to be conducted once over a batch of samples. The obtained optimal solutions
for those selected coefficients via minimum MSE (MMSE) can be restored and
reused later on to accelerate the sampling process. Extensive experiments on
EDM and DDIM show the IIA technique leads to significant performance gain when
the numbers of NFEs are small.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Lookahead Diffusion Probabilistic Models for Refining Mean Estimation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 22, 2023 </span>    
         <span class="authors"> Guoqiang Zhang, Niwa Kenta, W. Bastiaan Kleijn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.11312" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose lookahead diffusion probabilistic models (LA-DPMs) to exploit the
correlation in the outputs of the deep neural networks (DNNs) over subsequent
timesteps in diffusion probabilistic models (DPMs) to refine the mean
estimation of the conditional Gaussian distributions in the backward process. A
typical DPM first obtains an estimate of the original data sample
$\boldsymbol{x}$ by feeding the most recent state $\boldsymbol{z}_i$ and index
$i$ into the DNN model and then computes the mean vector of the conditional
Gaussian distribution for $\boldsymbol{z}_{i-1}$. We propose to calculate a
more accurate estimate for $\boldsymbol{x}$ by performing extrapolation on the
two estimates of $\boldsymbol{x}$ that are obtained by feeding
$(\boldsymbol{z}_{i+1},i+1)$ and $(\boldsymbol{z}_{i},i)$ into the DNN model.
The extrapolation can be easily integrated into the backward process of
existing DPMs by introducing an additional connection over two consecutive
timesteps, and fine-tuning is not required. Extensive experiments showed that
plugging in the additional connection into DDPM, DDIM, DEIS, S-PNDM, and
high-order DPM-Solvers leads to a significant performance gain in terms of FID
score.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 21, 2023 </span>    
         <span class="authors"> Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang, Andrei Kulik, Matthias Grundmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.11267" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The rapid development and application of foundation models have
revolutionized the field of artificial intelligence. Large diffusion models
have gained significant attention for their ability to generate photorealistic
images and support various tasks. On-device deployment of these models provides
benefits such as lower server costs, offline functionality, and improved user
privacy. However, common large diffusion models have over 1 billion parameters
and pose challenges due to restricted computational and memory resources on
devices. We present a series of implementation optimizations for large
diffusion models that achieve the fastest reported inference latency to-date
(under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung
S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile
devices. These enhancements broaden the applicability of generative AI and
improve the overall user experience across a wide range of devices.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 21, 2023 </span>    
         <span class="authors"> Angela Castillo, Maria Escobar, Guillaume Jeanneret, Albert Pumarola, Pablo Arbeláez, Ali Thabet, Artsiom Sanakoyeu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.11118" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Mixed reality applications require tracking the user's full-body motion to
enable an immersive experience. However, typical head-mounted devices can only
track head and hand movements, leading to a limited reconstruction of full-body
motion due to variability in lower body configurations. We propose BoDiffusion
-- a generative diffusion model for motion synthesis to tackle this
under-constrained reconstruction problem. We present a time and space
conditioning scheme that allows BoDiffusion to leverage sparse tracking inputs
while generating smooth and realistic full-body motion sequences. To the best
of our knowledge, this is the first approach that uses the reverse diffusion
process to model full-body tracking as a conditional sequence generation task.
We conduct experiments on the large-scale motion-capture dataset AMASS and show
that our approach outperforms the state-of-the-art approaches by a significant
margin in terms of full-body motion realism and joint reconstruction error.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improved Diffusion-based Image Colorization via Piggybacked Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 21, 2023 </span>    
         <span class="authors"> Hanyuan Liu, Jinbo Xing, Minshan Xie, Chengze Li, Tien-Tsin Wong </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.11105" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image colorization has been attracting the research interests of the
community for decades. However, existing methods still struggle to provide
satisfactory colorized results given grayscale images due to a lack of
human-like global understanding of colors. Recently, large-scale Text-to-Image
(T2I) models have been exploited to transfer the semantic information from the
text prompts to the image domain, where text provides a global control for
semantic objects in the image. In this work, we introduce a colorization model
piggybacking on the existing powerful T2I diffusion model. Our key idea is to
exploit the color prior knowledge in the pre-trained T2I diffusion model for
realistic and diverse colorization. A diffusion guider is designed to
incorporate the pre-trained weights of the latent diffusion model to output a
latent color prior that conforms to the visual semantics of the grayscale
input. A lightness-aware VQVAE will then generate the colorized result with
pixel-perfect alignment to the given grayscale image. Our model can also
achieve conditional colorization with additional inputs (e.g. user hints and
texts). Extensive experiments show that our method achieves state-of-the-art
performance in terms of perceptual quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SILVR: Guided Diffusion for Molecule Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 21, 2023 </span>    
         <span class="authors"> Nicholas T. Runcie, Antonia S. J. S. Mey </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.10905" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Computationally generating novel synthetically accessible compounds with high
affinity and low toxicity is a great challenge in drug design. Machine-learning
models beyond conventional pharmacophoric methods have shown promise in
generating novel small molecule compounds, but require significant tuning for a
specific protein target. Here, we introduce a method called selective iterative
latent variable refinement (SILVR) for conditioning an existing diffusion-based
equivariant generative model without retraining. The model allows the
generation of new molecules that fit into a binding site of a protein based on
fragment hits. We use the SARS-CoV-2 Main protease fragments from Diamond
X-Chem that form part of the COVID Moonshot project as a reference dataset for
conditioning the molecule generation. The SILVR rate controls the extent of
conditioning and we show that moderate SILVR rates make it possible to generate
new molecules of similar shape to the original fragments, meaning that the new
molecules fit the binding site without knowledge of the protein. We can also
merge up to 3 fragments into a new molecule without affecting the quality of
molecules generated by the underlying generative model. Our method is
generalizable to any protein target with known fragments and any
diffusion-based model for molecule generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Persistently Trained, Diffusion-assisted Energy-based Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 21, 2023 </span>    
         <span class="authors"> Xinwei Zhang, Zhiqiang Tan, Zhijian Ou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.10707" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Maximum likelihood (ML) learning for energy-based models (EBMs) is
challenging, partly due to non-convergence of Markov chain Monte Carlo.Several
variations of ML learning have been proposed, but existing methods all fail to
achieve both post-training image generation and proper density estimation. We
propose to introduce diffusion data and learn a joint EBM, called diffusion
assisted-EBMs, through persistent training (i.e., using persistent contrastive
divergence) with an enhanced sampling algorithm to properly sample from
complex, multimodal distributions. We present results from a 2D illustrative
experiment and image experiments and demonstrate that, for the first time for
image data, persistently trained EBMs can {\it simultaneously} achieve long-run
stability, post-training image generation, and superior out-of-distribution
detection.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 21, 2023 </span>    
         <span class="authors"> Jason J. Yu, Fereshteh Forghani, Konstantinos G. Derpanis, Marcus A. Brubaker </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.10700" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Novel view synthesis from a single input image is a challenging task, where
the goal is to generate a new view of a scene from a desired camera pose that
may be separated by a large motion. The highly uncertain nature of this
synthesis task due to unobserved elements within the scene (i.e., occlusion)
and outside the field-of-view makes the use of generative models appealing to
capture the variety of possible outputs. In this paper, we propose a novel
generative model which is capable of producing a sequence of photorealistic
images consistent with a specified camera trajectory, and a single starting
image. Our approach is centred on an autoregressive conditional diffusion-based
model capable of interpolating visible scene elements, and extrapolating
unobserved regions in a view, in a geometrically consistent manner.
Conditioning is limited to an image capturing a single camera view and the
(relative) pose of the new camera view. To measure the consistency over a
sequence of generated views, we introduce a new metric, the thresholded
symmetric epipolar distance (TSED), to measure the number of consistent frame
pairs in a sequence. While previous methods have been shown to produce high
quality images and consistent semantics across pairs of views, we show
empirically with our metric that they are often inconsistent with the desired
camera poses. In contrast, we demonstrate that our method produces both
photorealistic and view-consistent imagery.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Collaborative Diffusion for Multi-Modal Face Generation and Editing
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 20, 2023 </span>    
         <span class="authors"> Ziqi Huang, Kelvin C. K. Chan, Yuming Jiang, Ziwei Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.10530" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models arise as a powerful generative tool recently. Despite the
great progress, existing diffusion models mainly focus on uni-modal control,
i.e., the diffusion process is driven by only one modality of condition. To
further unleash the users' creativity, it is desirable for the model to be
controllable by multiple modalities simultaneously, e.g., generating and
editing faces by describing the age (text-driven) while drawing the face shape
(mask-driven). In this work, we present Collaborative Diffusion, where
pre-trained uni-modal diffusion models collaborate to achieve multi-modal face
generation and editing without re-training. Our key insight is that diffusion
models driven by different modalities are inherently complementary regarding
the latent denoising steps, where bilateral connections can be established
upon. Specifically, we propose dynamic diffuser, a meta-network that adaptively
hallucinates multi-modal denoising steps by predicting the spatial-temporal
influence functions for each pre-trained uni-modal model. Collaborative
Diffusion not only collaborates generation capabilities from uni-modal
diffusion models, but also integrates multiple uni-modal manipulations to
perform multi-modal editing. Extensive qualitative and quantitative experiments
demonstrate the superiority of our framework in both image quality and
condition consistency.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A data augmentation perspective on diffusion models and retrieval
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 20, 2023 </span>    
         <span class="authors"> Max F. Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, Chris Russell </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.10253" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models excel at generating photorealistic images from text-queries.
Naturally, many approaches have been proposed to use these generative abilities
to augment training datasets for downstream tasks, such as classification.
However, diffusion models are themselves trained on large noisily supervised,
but nonetheless, annotated datasets. It is an open question whether the
generalization capabilities of diffusion models beyond using the additional
data of the pre-training process for augmentation lead to improved downstream
performance. We perform a systematic evaluation of existing methods to generate
images from diffusion models and study new extensions to assess their benefit
for data augmentation. While we find that personalizing diffusion models
towards the target data outperforms simpler prompting strategies, we also show
that using the training data of the diffusion model alone, via a simple nearest
neighbor retrieval procedure, leads to even stronger downstream performance.
Overall, our study probes the limitations of diffusion models for data
augmentation but also highlights its potential in generating new training data
to improve performance on simple downstream vision tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 19, 2023 </span>    
         <span class="authors"> Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, Sanja Fidler </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.09787" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Automatically generating high-quality real world 3D scenes is of enormous
interest for applications such as virtual reality and robotics simulation.
Towards this goal, we introduce NeuralField-LDM, a generative model capable of
synthesizing complex 3D environments. We leverage Latent Diffusion Models that
have been successfully utilized for efficient high-quality 2D content creation.
We first train a scene auto-encoder to express a set of image and pose pairs as
a neural field, represented as density and feature voxel grids that can be
projected to produce novel views of the scene. To further compress this
representation, we train a latent-autoencoder that maps the voxel grids to a
set of latent representations. A hierarchical diffusion model is then fit to
the latents to complete the scene generation pipeline. We achieve a substantial
improvement over existing state-of-the-art scene generation models.
Additionally, we show how NeuralField-LDM can be used for a variety of 3D
content creation applications, including conditional scene generation, scene
inpainting and scene style manipulation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiFaReli : Diffusion Face Relighting
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 19, 2023 </span>    
         <span class="authors"> Puntawat Ponglertnapakorn, Nontawat Tritrong, Supasorn Suwajanakorn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.09479" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a novel approach to single-view face relighting in the wild.
Handling non-diffuse effects, such as global illumination or cast shadows, has
long been a challenge in face relighting. Prior work often assumes Lambertian
surfaces, simplified lighting models or involves estimating 3D shape, albedo,
or a shadow map. This estimation, however, is error-prone and requires many
training examples with lighting ground truth to generalize well. Our work
bypasses the need for accurate estimation of intrinsic components and can be
trained solely on 2D images without any light stage data, multi-view images, or
lighting ground truth. Our key idea is to leverage a conditional diffusion
implicit model (DDIM) for decoding a disentangled light encoding along with
other encodings related to 3D shape and facial identity inferred from
off-the-shelf estimators. We also propose a novel conditioning technique that
eases the modeling of the complex interaction between light and geometry by
using a rendered shading reference to spatially modulate the DDIM. We achieve
state-of-the-art performance on standard benchmark Multi-PIE and can
photorealistically relight in-the-wild images. Please visit our page:
https://diffusion-face-relighting.github.io
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Medical Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 19, 2023 </span>    
         <span class="authors"> Pham Ngoc Huy, Tran Minh Quan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.09383" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this study, we introduce a generative model that can synthesize a large
number of radiographical image/label pairs, and thus is asymptotically
favorable to downstream activities such as segmentation in bio-medical image
analysis. Denoising Diffusion Medical Model (DDMM), the proposed technique, can
create realistic X-ray images and associated segmentations on a small number of
annotated datasets as well as other massive unlabeled datasets with no
supervision. Radiograph/segmentation pairs are generated jointly by the DDMM
sampling process in probabilistic mode. As a result, a vanilla UNet that uses
this data augmentation for segmentation task outperforms other similarly
data-centric approaches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 18, 2023 </span>    
         <span class="authors"> Soon Yau Cheong, Armin Mustafa, Andrew Gilbert </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.08870" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Existing person image generative models can do either image generation or
pose transfer but not both. We propose a unified diffusion model, UPGPT to
provide a universal solution to perform all the person image tasks -
generative, pose transfer, and editing. With fine-grained multimodality and
disentanglement capabilities, our approach offers fine-grained control over the
generation and the editing process of images using a combination of pose, text,
and image, all without needing a semantic segmentation mask which can be
challenging to obtain or edit. We also pioneer the parameterized body SMPL
model in pose-guided person image generation to demonstrate new capability -
simultaneous pose and camera view interpolation while maintaining a person's
appearance. Results on the benchmark DeepFashion dataset show that UPGPT is the
new state-of-the-art while simultaneously pioneering new capabilities of edit
and pose transfer in human image generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Two-stage Denoising Diffusion Model for Source Localization in Graph Inverse Problems
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 18, 2023 </span>    
         <span class="authors"> Bosong Huang, Weihao Yu, Ruzhong Xie, Jing Xiao, Jin Huang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.08841" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Source localization is the inverse problem of graph information dissemination
and has broad practical applications.
  However, the inherent intricacy and uncertainty in information dissemination
pose significant challenges, and the ill-posed nature of the source
localization problem further exacerbates these challenges. Recently, deep
generative models, particularly diffusion models inspired by classical
non-equilibrium thermodynamics, have made significant progress. While diffusion
models have proven to be powerful in solving inverse problems and producing
high-quality reconstructions, applying them directly to the source localization
is infeasible for two reasons. Firstly, it is impossible to calculate the
posterior disseminated results on a large-scale network for iterative denoising
sampling, which would incur enormous computational costs. Secondly, in the
existing methods for this field, the training data itself are ill-posed
(many-to-one); thus simply transferring the diffusion model would only lead to
local optima.
  To address these challenges, we propose a two-stage optimization framework,
the source localization denoising diffusion model (SL-Diff). In the coarse
stage, we devise the source proximity degrees as the supervised signals to
generate coarse-grained source predictions. This aims to efficiently initialize
the next stage, significantly reducing its convergence time and calibrating the
convergence process. Furthermore, the introduction of cascade temporal
information in this training method transforms the many-to-one mapping
relationship into a one-to-one relationship, perfectly addressing the ill-posed
problem. In the fine stage, we design a diffusion model for the graph inverse
problem that can quantify the uncertainty in the dissemination. The proposed
SL-Diff yields excellent prediction results within a reasonable sampling time
at extensive experiments.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 18, 2023 </span>    
         <span class="authors"> Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.08818" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Latent Diffusion Models (LDMs) enable high-quality image synthesis while
avoiding excessive compute demands by training a diffusion model in a
compressed lower-dimensional latent space. Here, we apply the LDM paradigm to
high-resolution video generation, a particularly resource-intensive task. We
first pre-train an LDM on images only; then, we turn the image generator into a
video generator by introducing a temporal dimension to the latent space
diffusion model and fine-tuning on encoded image sequences, i.e., videos.
Similarly, we temporally align diffusion model upsamplers, turning them into
temporally consistent video super resolution models. We focus on two relevant
real-world applications: Simulation of in-the-wild driving data and creative
content creation with text-to-video modeling. In particular, we validate our
Video LDM on real driving videos of resolution 512 x 1024, achieving
state-of-the-art performance. Furthermore, our approach can easily leverage
off-the-shelf pre-trained image LDMs, as we only need to train a temporal
alignment model in that case. Doing so, we turn the publicly available,
state-of-the-art text-to-image LDM Stable Diffusion into an efficient and
expressive text-to-video model with resolution up to 1280 x 2048. We show that
the temporal layers trained in this way generalize to different fine-tuned
text-to-image LDMs. Utilizing this property, we show the first results for
personalized text-to-video generation, opening exciting directions for future
content creation. Project page:
https://research.nvidia.com/labs/toronto-ai/VideoLDM/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Synthetic Data from Diffusion Models Improves ImageNet Classification
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 17, 2023 </span>    
         <span class="authors"> Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, David J. Fleet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.08466" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep generative models are becoming increasingly powerful, now generating
diverse high fidelity photo-realistic samples given text prompts. Have they
reached the point where models of natural images can be used for generative
data augmentation, helping to improve challenging discriminative tasks? We show
that large-scale text-to image diffusion models can be fine-tuned to produce
class conditional models with SOTA FID (1.76 at 256x256 resolution) and
Inception Score (239 at 256x256). The model also yields a new SOTA in
Classification Accuracy Scores (64.96 for 256x256 generative samples, improving
to 69.24 for 1024x1024 samples). Augmenting the ImageNet training set with
samples from the resulting models yields significant improvements in ImageNet
classification accuracy over strong ResNet and Vision Transformer baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 17, 2023 </span>    
         <span class="authors"> Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.08291" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This work aims to improve the applicability of diffusion models in realistic
image restoration. Specifically, we enhance the diffusion model in several
aspects such as network architecture, noise level, denoising steps, training
image size, and optimizer/scheduler. We show that tuning these hyperparameters
allows us to achieve better performance on both distortion and perceptual
scores. We also propose a U-Net based latent diffusion model which performs
diffusion in a low-resolution latent space while preserving high-resolution
information from the original input for the decoding process. Compared to the
previous latent-diffusion model which trains a VAE-GAN to compress the image,
our proposed U-Net compression strategy is significantly more stable and can
recover highly accurate images without relying on adversarial optimization.
Importantly, these modifications allow us to apply diffusion models to various
image restoration tasks, including real-world shadow removal, HR
non-homogeneous dehazing, stereo super-resolution, and bokeh effect
transformation. By simply replacing the datasets and slightly changing the
noise network, our model, named Refusion, is able to deal with large-size
images (e.g., 6000 x 4000 x 3 in HR dehazing) and produces good results on all
the above restoration problems. Our Refusion achieves the best perceptual
performance in the NTIRE 2023 Image Shadow Removal Challenge and wins 2nd place
overall.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Towards Controllable Diffusion Models via Reward-Guided Exploration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 14, 2023 </span>    
         <span class="authors"> Hengtong Zhang, Tingyang Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.07132" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, q-bio.BM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 By formulating data samples' formation as a Markov denoising process,
diffusion models achieve state-of-the-art performances in a collection of
tasks. Recently, many variants of diffusion models have been proposed to enable
controlled sample generation. Most of these existing methods either formulate
the controlling information as an input (i.e.,: conditional representation) for
the noise approximator, or introduce a pre-trained classifier in the test-phase
to guide the Langevin dynamic towards the conditional goal. However, the former
line of methods only work when the controlling information can be formulated as
conditional representations, while the latter requires the pre-trained guidance
classifier to be differentiable. In this paper, we propose a novel framework
named RGDM (Reward-Guided Diffusion Model) that guides the training-phase of
diffusion models via reinforcement learning (RL). The proposed training
framework bridges the objective of weighted log-likelihood and maximum entropy
RL, which enables calculating policy gradients via samples from a pay-off
distribution proportional to exponential scaled rewards, rather than from
policies themselves. Such a framework alleviates the high gradient variances
and enables diffusion models to explore for highly rewarded samples in the
reverse process. Experiments on 3D shape and molecule generation tasks show
significant improvements over existing conditional diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Delta Denoising Score
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 14, 2023 </span>    
         <span class="authors"> Amir Hertz, Kfir Aberman, Daniel Cohen-Or </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.07090" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce Delta Denoising Score (DDS), a novel scoring function for
text-based image editing that guides minimal modifications of an input image
towards the content described in a target prompt. DDS leverages the rich
generative prior of text-to-image diffusion models and can be used as a loss
term in an optimization problem to steer an image towards a desired direction
dictated by a text. DDS utilizes the Score Distillation Sampling (SDS)
mechanism for the purpose of image editing. We show that using only SDS often
produces non-detailed and blurry outputs due to noisy gradients. To address
this issue, DDS uses a prompt that matches the input image to identify and
remove undesired erroneous directions of SDS. Our key premise is that SDS
should be zero when calculated on pairs of matched prompts and images, meaning
that if the score is non-zero, its gradients can be attributed to the erroneous
component of SDS. Our analysis demonstrates the competence of DDS for text
based image-to-image translation. We further show that DDS can be used to train
an effective zero-shot image translation model. Experimental results indicate
that DDS outperforms existing methods in terms of stability and quality,
highlighting its potential for real-world applications in text-based image
editing.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Memory Efficient Diffusion Probabilistic Models via Patch-based Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 14, 2023 </span>    
         <span class="authors"> Shinei Arakawa, Hideki Tsunashima, Daichi Horita, Keitaro Tanaka, Shigeo Morishima </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.07087" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models have been successful in generating
high-quality and diverse images. However, traditional models, whose input and
output are high-resolution images, suffer from excessive memory requirements,
making them less practical for edge devices. Previous approaches for generative
adversarial networks proposed a patch-based method that uses positional
encoding and global content information. Nevertheless, designing a patch-based
approach for diffusion probabilistic models is non-trivial. In this paper, we
resent a diffusion probabilistic model that generates images on a
patch-by-patch basis. We propose two conditioning methods for a patch-based
generation. First, we propose position-wise conditioning using one-hot
representation to ensure patches are in proper positions. Second, we propose
Global Content Conditioning (GCC) to ensure patches have coherent content when
concatenated together. We evaluate our model qualitatively and quantitatively
on CelebA and LSUN bedroom datasets and demonstrate a moderate trade-off
between maximum memory consumption and generated image quality. Specifically,
when an entire image is divided into 2 x 2 patches, our proposed approach can
reduce the maximum memory consumption by half while maintaining comparable
image quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Soundini: Sound-Guided Diffusion for Natural Video Editing
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 13, 2023 </span>    
         <span class="authors"> Seung Hyun Lee, Sieun Kim, Innfarn Yoo, Feng Yang, Donghyeon Cho, Youngseo Kim, Huiwen Chang, Jinkyu Kim, Sangpil Kim </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06818" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a method for adding sound-guided visual effects to specific
regions of videos with a zero-shot setting. Animating the appearance of the
visual effect is challenging because each frame of the edited video should have
visual changes while maintaining temporal consistency. Moreover, existing video
editing solutions focus on temporal consistency across frames, ignoring the
visual style variations over time, e.g., thunderstorm, wave, fire crackling. To
overcome this limitation, we utilize temporal sound features for the dynamic
style. Specifically, we guide denoising diffusion probabilistic models with an
audio latent representation in the audio-visual latent space. To the best of
our knowledge, our work is the first to explore sound-guided natural video
editing from various sound sources with sound-specialized properties, such as
intensity, timbre, and volume. Additionally, we design optical flow-based
guidance to generate temporally consistent video frames, capturing the
pixel-wise relationship between adjacent frames. Experimental results show that
our method outperforms existing video editing techniques, producing more
realistic visual effects that reflect the properties of sound. Please visit our
page: https://kuai-lab.github.io/soundini-gallery/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 13, 2023 </span>    
         <span class="authors"> Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, Hao Su </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06714" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 3D-aware image synthesis encompasses a variety of tasks, such as scene
generation and novel view synthesis from images. Despite numerous task-specific
methods, developing a comprehensive model remains challenging. In this paper,
we present SSDNeRF, a unified approach that employs an expressive diffusion
model to learn a generalizable prior of neural radiance fields (NeRF) from
multi-view images of diverse objects. Previous studies have used two-stage
approaches that rely on pretrained NeRFs as real data to train diffusion
models. In contrast, we propose a new single-stage training paradigm with an
end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent
diffusion model, enabling simultaneous 3D reconstruction and prior learning,
even from sparsely available views. At test time, we can directly sample the
diffusion prior for unconditional generation, or combine it with arbitrary
observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates
robust results comparable to or better than leading task-specific methods in
unconditional generation and single/sparse-view 3D reconstruction.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionRig: Learning Personalized Priors for Facial Appearance Editing
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 13, 2023 </span>    
         <span class="authors"> Zheng Ding, Xuaner Zhang, Zhihao Xia, Lars Jebe, Zhuowen Tu, Xiuming Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06711" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We address the problem of learning person-specific facial priors from a small
number (e.g., 20) of portrait photos of the same person. This enables us to
edit this specific person's facial appearance, such as expression and lighting,
while preserving their identity and high-frequency facial details. Key to our
approach, which we dub DiffusionRig, is a diffusion model conditioned on, or
"rigged by," crude 3D face models estimated from single in-the-wild images by
an off-the-shelf estimator. On a high level, DiffusionRig learns to map
simplistic renderings of 3D face models to realistic photos of a given person.
Specifically, DiffusionRig is trained in two stages: It first learns generic
facial priors from a large-scale face dataset and then person-specific priors
from a small portrait photo collection of the person of interest. By learning
the CGI-to-photo mapping with such personalized priors, DiffusionRig can "rig"
the lighting, facial expression, head pose, etc. of a portrait photo,
conditioned only on coarse 3D models while preserving this person's identity
and other high-frequency characteristics. Qualitative and quantitative
experiments show that DiffusionRig outperforms existing approaches in both
identity preservation and photorealism. Please see the project website:
https://diffusionrig.github.io for the supplemental material, video, code, and
data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning Controllable 3D Diffusion Models from Single-view Images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 13, 2023 </span>    
         <span class="authors"> Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen, Lingjie Liu, Josh Susskind </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06700" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently become the de-facto approach for generative
modeling in the 2D domain. However, extending diffusion models to 3D is
challenging due to the difficulties in acquiring 3D ground truth data for
training. On the other hand, 3D GANs that integrate implicit 3D representations
into GANs have shown remarkable 3D-aware generation when trained only on
single-view image datasets. However, 3D GANs do not provide straightforward
ways to precisely control image synthesis. To address these challenges, We
present Control3Diff, a 3D diffusion model that combines the strengths of
diffusion models and 3D GANs for versatile, controllable 3D-aware image
synthesis for single-view datasets. Control3Diff explicitly models the
underlying latent distribution (optionally conditioned on external inputs),
thus enabling direct control during the diffusion process. Moreover, our
approach is general and applicable to any type of controlling input, allowing
us to train it with the same diffusion objective without any auxiliary
supervision. We validate the efficacy of Control3Diff on standard image
generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various
conditioning inputs such as images, sketches, and text prompts. Please see the
project website (\url{https://jiataogu.me/control3diff}) for video comparisons.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 13, 2023 </span>    
         <span class="authors"> Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, Zhenguo Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06648" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have proven to be highly effective in generating
high-quality images. However, adapting large pre-trained diffusion models to
new domains remains an open challenge, which is critical for real-world
applications. This paper proposes DiffFit, a parameter-efficient strategy to
fine-tune large pre-trained diffusion models that enable fast adaptation to new
domains. DiffFit is embarrassingly simple that only fine-tunes the bias term
and newly-added scaling factors in specific layers, yet resulting in
significant training speed-up and reduced model storage costs. Compared with
full fine-tuning, DiffFit achieves 2$\times$ training speed-up and only needs
to store approximately 0.12\% of the total model parameters. Intuitive
theoretical analysis has been provided to justify the efficacy of scaling
factors on fast adaptation. On 8 downstream datasets, DiffFit achieves superior
or competitive performances compared to the full fine-tuning while being more
efficient. Remarkably, we show that DiffFit can adapt a pre-trained
low-resolution generative model to a high-resolution one by adding minimal
cost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of
3.02 on ImageNet 512$\times$512 benchmark by fine-tuning only 25 epochs from a
public pre-trained ImageNet 256$\times$256 checkpoint while being 30$\times$
more training efficient than the closest competitor.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## An Edit Friendly DDPM Noise Space: Inversion and Manipulations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 12, 2023 </span>    
         <span class="authors"> Inbar Huberman-Spiegelglas, Vladimir Kulikov, Tomer Michaeli </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06140" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) employ a sequence of white
Gaussian noise samples to generate an image. In analogy with GANs, those noise
maps could be considered as the latent code associated with the generated
image. However, this native noise space does not possess a convenient
structure, and is thus challenging to work with in editing tasks. Here, we
propose an alternative latent noise space for DDPM that enables a wide range of
editing operations via simple means, and present an inversion method for
extracting these edit-friendly noise maps for any given image (real or
synthetically generated). As opposed to the native DDPM noise space, the
edit-friendly noise maps do not have a standard normal distribution and are not
statistically independent across timesteps. However, they allow perfect
reconstruction of any desired image, and simple transformations on them
translate into meaningful manipulations of the output image (e.g., shifting,
color edits). Moreover, in text-conditional models, fixing those noise maps
while changing the text prompt, modifies semantics while retaining structure.
We illustrate how this property enables text-based editing of real images via
the diverse DDPM sampling scheme (in contrast to the popular non-diverse DDIM
inversion). We also show how it can be used within existing diffusion-based
editing methods to improve their quality and diversity.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## E(3)xSO(3)-Equivariant Networks for Spherical Deconvolution in Diffusion MRI
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 12, 2023 </span>    
         <span class="authors"> Axel Elaldi, Guido Gerig, Neel Dey </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06103" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Roto-Translation Equivariant Spherical Deconvolution (RT-ESD), an
$E(3)\times SO(3)$ equivariant framework for sparse deconvolution of volumes
where each voxel contains a spherical signal. Such 6D data naturally arises in
diffusion MRI (dMRI), a medical imaging modality widely used to measure
microstructure and structural connectivity. As each dMRI voxel is typically a
mixture of various overlapping structures, there is a need for blind
deconvolution to recover crossing anatomical structures such as white matter
tracts. Existing dMRI work takes either an iterative or deep learning approach
to sparse spherical deconvolution, yet it typically does not account for
relationships between neighboring measurements. This work constructs
equivariant deep learning layers which respect to symmetries of spatial
rotations, reflections, and translations, alongside the symmetries of voxelwise
spherical rotations. As a result, RT-ESD improves on previous work across
several tasks including fiber recovery on the DiSCo dataset,
deconvolution-derived partial volume estimation on real-world \textit{in vivo}
human brain dMRI, and improved downstream reconstruction of fiber tractograms
on the Tractometer dataset. Our implementation is available at
https://github.com/AxelElaldi/e3so3_conv
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 12, 2023 </span>    
         <span class="authors"> James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, Hongxia Jin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06027" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent works demonstrate a remarkable ability to customize text-to-image
diffusion models while only providing a few example images. What happens if you
try to customize such models using multiple, fine-grained concepts in a
sequential (i.e., continual) manner? In our work, we show that recent
state-of-the-art customization of text-to-image models suffer from catastrophic
forgetting when new concepts arrive sequentially. Specifically, when adding a
new concept, the ability to generate high quality images of past, similar
concepts degrade. To circumvent this forgetting, we propose a new method,
C-LoRA, composed of a continually self-regularized low-rank adaptation in cross
attention layers of the popular Stable Diffusion model. Furthermore, we use
customization prompts which do not include the word of the customized object
(i.e., "person" for a human face dataset) and are initialized as completely
random embeddings. Importantly, our method induces only marginal additional
parameter costs and requires no storage of user data for replay. We show that
C-LoRA not only outperforms several baselines for our proposed setting of
text-to-image continual customization, which we refer to as Continual
Diffusion, but that we achieve a new state-of-the-art in the well-established
rehearsal-free continual learning setting for image classification. The high
achieving performance of C-LoRA in two separate domains positions it as a
compelling solution for a wide range of applications, and we believe it has
significant potential for practical impact.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 12, 2023 </span>    
         <span class="authors"> Johanna Karras, Aleksander Holynski, Ting-Chun Wang, Ira Kemelmacher-Shlizerman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.06025" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present DreamPose, a diffusion-based method for generating animated
fashion videos from still images. Given an image and a sequence of human body
poses, our method synthesizes a video containing both human and fabric motion.
To achieve this, we transform a pretrained text-to-image model (Stable
Diffusion) into a pose-and-image guided video synthesis model, using a novel
finetuning strategy, a set of architectural changes to support the added
conditioning signals, and techniques to encourage temporal consistency. We
fine-tune on a collection of fashion videos from the UBC Fashion dataset. We
evaluate our method on a variety of clothing styles and poses, and demonstrate
that our method produces state-of-the-art results on fashion video animation.
Video results are available on our project page.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SpectralDiff: Hyperspectral Image Classification with Spectral-Spatial Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 12, 2023 </span>    
         <span class="authors"> Ning Chen, Jun Yue, Leyuan Fang, Shaobo Xia </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.05961" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Hyperspectral image (HSI) classification is an important topic in the field
of remote sensing, and has a wide range of applications in Earth science. HSIs
contain hundreds of continuous bands, which are characterized by high dimension
and high correlation between adjacent bands. The high dimension and redundancy
of HSI data bring great difficulties to HSI classification. In recent years, a
large number of HSI feature extraction and classification methods based on deep
learning have been proposed. However, their ability to model the global
relationships among samples in both spatial and spectral domains is still
limited. In order to solve this problem, an HSI classification method with
spectral-spatial diffusion models is proposed. The proposed method realizes the
reconstruction of spectral-spatial distribution of the training samples with
the forward and reverse spectral-spatial diffusion process, thus modeling the
global spatial-spectral relationship between samples. Then, we use the
spectral-spatial denoising network of the reverse process to extract the
unsupervised diffusion features. Features extracted by the spectral-spatial
diffusion models can achieve cross-sample perception from the reconstructed
distribution of the training samples, thus obtaining better classification
performance. Experiments on three public HSI datasets show that the proposed
method can achieve better performance than the state-of-the-art methods. The
source code and the pre-trained spectral-spatial diffusion model will be
publicly available at https://github.com/chenning0115/SpectralDiff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion models with location-scale noise
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 12, 2023 </span>    
         <span class="authors"> Alexia Jolicoeur-Martineau, Kilian Fatras, Ke Li, Tal Kachman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.05907" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.NA, math.NA
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Models (DMs) are powerful generative models that add Gaussian noise
to the data and learn to remove it. We wanted to determine which noise
distribution (Gaussian or non-Gaussian) led to better generated data in DMs.
Since DMs do not work by design with non-Gaussian noise, we built a framework
that allows reversing a diffusion process with non-Gaussian location-scale
noise. We use that framework to show that the Gaussian distribution performs
the best over a wide range of other distributions (Laplace, Uniform, t,
Generalized-Gaussian).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 12, 2023 </span>    
         <span class="authors"> Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, Lan Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.05684" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We have recently seen tremendous progress in diffusion advances for
generating realistic human motions. Yet, they largely disregard the rich
multi-human interactions. In this paper, we present InterGen, an effective
diffusion-based approach that incorporates human-to-human interactions into the
motion diffusion process, which enables layman users to customize high-quality
two-person interaction motions, with only text guidance. We first contribute a
multimodal dataset, named InterHuman. It consists of about 107M frames for
diverse two-person interactions, with accurate skeletal motions and 16,756
natural language descriptions. For the algorithm side, we carefully tailor the
motion diffusion model to our two-person interaction setting. To handle the
symmetry of human identities during interactions, we propose two cooperative
transformer-based denoisers that explicitly share weights, with a mutual
attention mechanism to further connect the two denoising processes. Then, we
propose a novel representation for motion input in our interaction diffusion
model, which explicitly formulates the global relations between the two
performers in the world frame. We further introduce two novel regularization
terms to encode spatial relations, equipped with a corresponding damping scheme
during the training of our interaction diffusion model. Extensive experiments
validate the effectiveness and generalizability of InterGen. Notably, it can
generate more diverse and compelling two-person motions than previous methods
and enables various downstream applications for human interactions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CamDiff: Camouflage Image Augmentation via Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 11, 2023 </span>    
         <span class="authors"> Xue-Jing Luo, Shuo Wang, Zongwei Wu, Christos Sakaridis, Yun Cheng, Deng-Ping Fan, Luc Van Gool </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.05469" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The burgeoning field of camouflaged object detection (COD) seeks to identify
objects that blend into their surroundings. Despite the impressive performance
of recent models, we have identified a limitation in their robustness, where
existing methods may misclassify salient objects as camouflaged ones, despite
these two characteristics being contradictory. This limitation may stem from
lacking multi-pattern training images, leading to less saliency robustness. To
address this issue, we introduce CamDiff, a novel approach inspired by
AI-Generated Content (AIGC) that overcomes the scarcity of multi-pattern
training images. Specifically, we leverage the latent diffusion model to
synthesize salient objects in camouflaged scenes, while using the zero-shot
image classification ability of the Contrastive Language-Image Pre-training
(CLIP) model to prevent synthesis failures and ensure the synthesized object
aligns with the input prompt. Consequently, the synthesized image retains its
original camouflage label while incorporating salient objects, yielding
camouflage samples with richer characteristics. The results of user studies
show that the salient objects in the scenes synthesized by our framework
attract the user's attention more; thus, such samples pose a greater challenge
to the existing COD models. Our approach enables flexible editing and efficient
large-scale dataset generation at a low cost. It significantly enhances COD
baselines' training and testing phases, emphasizing robustness across diverse
domains. Our newly-generated datasets and source code are available at
https://github.com/drlxj/CamDiff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for Constrained Domains
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 11, 2023 </span>    
         <span class="authors"> Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, Michael Hutchinson </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.05364" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models are a recent class of generative models which
achieve state-of-the-art results in many domains such as unconditional image
generation and text-to-speech tasks. They consist of a noising process
destroying the data and a backward stage defined as the time-reversal of the
noising diffusion. Building on their success, diffusion models have recently
been extended to the Riemannian manifold setting. Yet, these Riemannian
diffusion models require geodesics to be defined for all times. While this
setting encompasses many important applications, it does not include manifolds
defined via a set of inequality constraints, which are ubiquitous in many
scientific domains such as robotics and protein design. In this work, we
introduce two methods to bridge this gap. First, we design a noising process
based on the logarithmic barrier metric induced by the inequality constraints.
Second, we introduce a noising process based on the reflected Brownian motion.
As existing diffusion model techniques cannot be applied in this setting, we
derive new tools to define such models in our framework. We empirically
demonstrate the applicability of our methods to a number of synthetic and
real-world tasks, including the constrained conformational modelling of protein
backbones and robotic arms.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Mask-conditioned latent diffusion for generating gastrointestinal polyp images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 11, 2023 </span>    
         <span class="authors"> Roman Macháček, Leila Mozaffari, Zahra Sepasdar, Sravanthi Parasa, Pål Halvorsen, Michael A. Riegler, Vajira Thambawita </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.05233" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In order to take advantage of AI solutions in endoscopy diagnostics, we must
overcome the issue of limited annotations. These limitations are caused by the
high privacy concerns in the medical field and the requirement of getting aid
from experts for the time-consuming and costly medical data annotation process.
In computer vision, image synthesis has made a significant contribution in
recent years as a result of the progress of generative adversarial networks
(GANs) and diffusion probabilistic models (DPM). Novel DPMs have outperformed
GANs in text, image, and video generation tasks. Therefore, this study proposes
a conditional DPM framework to generate synthetic GI polyp images conditioned
on given generated segmentation masks. Our experimental results show that our
system can generate an unlimited number of high-fidelity synthetic polyp images
with the corresponding ground truth masks of polyps. To test the usefulness of
the generated data, we trained binary image segmentation models to study the
effect of using synthetic data. Results show that the best micro-imagewise IOU
of 0.7751 was achieved from DeepLabv3+ when the training data consists of both
real data and synthetic data. However, the results reflect that achieving good
segmentation performance with synthetic data heavily depends on model
architectures.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generative modeling for time series via Schr{ö}dinger bridge
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 11, 2023 </span>    
         <span class="authors"> Mohamed Hamdouche, Pierre Henry-Labordere, Huyên Pham </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.05093" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  math.OC, math.PR, q-fin.CP, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a novel generative model for time series based on Schr{\"o}dinger
bridge (SB) approach. This consists in the entropic interpolation via optimal
transport between a reference probability measure on path space and a target
measure consistent with the joint data distribution of the time series. The
solution is characterized by a stochastic differential equation on finite
horizon with a path-dependent drift function, hence respecting the temporal
dynamics of the time series distribution. We can estimate the drift function
from data samples either by kernel regression methods or with LSTM neural
networks, and the simulation of the SB diffusion yields new synthetic data
samples of the time series. The performance of our generative model is
evaluated through a series of numerical experiments. First, we test with a toy
autoregressive model, a GARCH Model, and the example of fractional Brownian
motion, and measure the accuracy of our algorithm with marginal and temporal
dependencies metrics. Next, we use our SB generated synthetic samples for the
application to deep hedging on real-data sets. Finally, we illustrate the SB
approach for generating sequence of images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Binary Latent Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 10, 2023 </span>    
         <span class="authors"> Ze Wang, Jiang Wang, Zicheng Liu, Qiang Qiu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.04820" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we show that a binary latent space can be explored for compact
yet expressive image representations. We model the bi-directional mappings
between an image and the corresponding latent binary representation by training
an auto-encoder with a Bernoulli encoding distribution. On the one hand, the
binary latent space provides a compact discrete image representation of which
the distribution can be modeled more efficiently than pixels or continuous
latent representations. On the other hand, we now represent each image patch as
a binary vector instead of an index of a learned cookbook as in discrete image
representations with vector quantization. In this way, we obtain binary latent
representations that allow for better image quality and high-resolution image
representations without any multi-stage hierarchy in the latent space. In this
binary latent space, images can now be generated effectively using a binary
latent diffusion model tailored specifically for modeling the prior over the
binary image representations. We present both conditional and unconditional
image generation experiments with multiple datasets, and show that the proposed
method performs comparably to state-of-the-art methods while dramatically
improving the sampling efficiency to as few as 16 steps without using any
test-time acceleration. The proposed framework can also be seamlessly scaled to
$1024 \times 1024$ high-resolution image generation without resorting to latent
hierarchy or multi-stage refinements.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Ambiguous Medical Image Segmentation using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 10, 2023 </span>    
         <span class="authors"> Aimon Rahman, Jeya Maria Jose Valanarasu, Ilker Hacihaliloglu, Vishal M Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.04745" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Collective insights from a group of experts have always proven to outperform
an individual's best diagnostic for clinical tasks. For the task of medical
image segmentation, existing research on AI-based alternatives focuses more on
developing models that can imitate the best individual rather than harnessing
the power of expert groups. In this paper, we introduce a single diffusion
model-based approach that produces multiple plausible outputs by learning a
distribution over group insights. Our proposed model generates a distribution
of segmentation masks by leveraging the inherent stochastic sampling process of
diffusion using only minimal additional learning. We demonstrate on three
different medical image modalities- CT, ultrasound, and MRI that our model is
capable of producing several possible variants while capturing the frequencies
of their occurrences. Comprehensive results show that our proposed approach
outperforms existing state-of-the-art ambiguous segmentation networks in terms
of accuracy while preserving naturally occurring variation. We also propose a
new metric to evaluate the diversity as well as the accuracy of segmentation
predictions that aligns with the interest of clinical practice of collective
insights.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Reflected Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 10, 2023 </span>    
         <span class="authors"> Aaron Lou, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.04740" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based diffusion models learn to reverse a stochastic differential
equation that maps data to noise. However, for complex tasks, numerical error
can compound and result in highly unnatural samples. Previous work mitigates
this drift with thresholding, which projects to the natural data domain (such
as pixel space for images) after each diffusion step, but this leads to a
mismatch between the training and generative processes. To incorporate data
constraints in a principled manner, we present Reflected Diffusion Models,
which instead reverse a reflected stochastic differential equation evolving on
the support of the data. Our approach learns the perturbed score function
through a generalized score matching loss and extends key components of
standard diffusion models including diffusion guidance, likelihood-based
training, and ODE sampling. We also bridge the theoretical gap with
thresholding: such schemes are just discretizations of reflected SDEs. On
standard image benchmarks, our method is competitive with or surpasses the
state of the art and, for classifier-free guidance, our approach enables fast
exact sampling with ODEs and produces more faithful samples under high guidance
weight.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 10, 2023 </span>    
         <span class="authors"> ZiHan Cao, ShiQi Cao, Xiao Wu, JunMing Hou, Ran Ran, Liang-Jian Deng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.04774" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denosing diffusion model, as a generative model, has received a lot of
attention in the field of image generation recently, thanks to its powerful
generation capability. However, diffusion models have not yet received
sufficient research in the field of image fusion. In this article, we introduce
diffusion model to the image fusion field, treating the image fusion task as
image-to-image translation and designing two different conditional injection
modulation modules (i.e., style transfer modulation and wavelet modulation) to
inject coarse-grained style information and fine-grained high-frequency and
low-frequency information into the diffusion UNet, thereby generating fused
images. In addition, we also discussed the residual learning and the selection
of training objectives of the diffusion model in the image fusion task.
Extensive experimental results based on quantitative and qualitative
assessments compared with benchmarks demonstrates state-of-the-art results and
good generalization performance in image fusion tasks. Finally, it is hoped
that our method can inspire other works and gain insight into this field to
better apply the diffusion model to image fusion tasks. Code shall be released
for better reproducibility.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 10, 2023 </span>    
         <span class="authors"> Tao Chen, Chenhui Wang, Hongming Shan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.04429" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Medical image segmentation is a challenging task with inherent ambiguity and
high uncertainty, attributed to factors such as unclear tumor boundaries and
multiple plausible annotations. The accuracy and diversity of segmentation
masks are both crucial for providing valuable references to radiologists in
clinical practice. While existing diffusion models have shown strong capacities
in various visual generation tasks, it is still challenging to deal with
discrete masks in segmentation. To achieve accurate and diverse medical image
segmentation masks, we propose a novel conditional Bernoulli Diffusion model
for medical image segmentation (BerDiff). Instead of using the Gaussian noise,
we first propose to use the Bernoulli noise as the diffusion kernel to enhance
the capacity of the diffusion model for binary segmentation tasks, resulting in
more accurate segmentation masks. Second, by leveraging the stochastic nature
of the diffusion model, our BerDiff randomly samples the initial Bernoulli
noise and intermediate latent variables multiple times to produce a range of
diverse segmentation masks, which can highlight salient regions of interest
that can serve as valuable references for radiologists. In addition, our
BerDiff can efficiently sample sub-sequences from the overall trajectory of the
reverse diffusion, thereby speeding up the segmentation process. Extensive
experimental results on two medical image segmentation datasets with different
modalities demonstrate that our BerDiff outperforms other recently published
state-of-the-art methods. Our results suggest diffusion models could serve as a
strong backbone for medical image segmentation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Zero-shot CT Field-of-view Completion with Unconditional Generative Diffusion Prior
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 07, 2023 </span>    
         <span class="authors"> Kaiwen Xu, Aravind R. Krishnan, Thomas Z. Li, Yuankai Huo, Kim L. Sandler, Fabien Maldonado, Bennett A. Landman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.03760" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Anatomically consistent field-of-view (FOV) completion to recover truncated
body sections has important applications in quantitative analyses of computed
tomography (CT) with limited FOV. Existing solution based on conditional
generative models relies on the fidelity of synthetic truncation patterns at
training phase, which poses limitations for the generalizability of the method
to potential unknown types of truncation. In this study, we evaluate a
zero-shot method based on a pretrained unconditional generative diffusion
prior, where truncation pattern with arbitrary forms can be specified at
inference phase. In evaluation on simulated chest CT slices with synthetic FOV
truncation, the method is capable of recovering anatomically consistent body
sections and subcutaneous adipose tissue measurement error caused by FOV
truncation. However, the correction accuracy is inferior to the conditionally
trained counterpart.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ChiroDiff: Modelling chirographic data with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 07, 2023 </span>    
         <span class="authors"> Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, Yi-Zhe Song </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.03785" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative modelling over continuous-time geometric constructs, a.k.a such as
handwriting, sketches, drawings etc., have been accomplished through
autoregressive distributions. Such strictly-ordered discrete factorization
however falls short of capturing key properties of chirographic data -- it
fails to build holistic understanding of the temporal concept due to one-way
visibility (causality). Consequently, temporal data has been modelled as
discrete token sequences of fixed sampling rate instead of capturing the true
underlying concept. In this paper, we introduce a powerful model-class namely
"Denoising Diffusion Probabilistic Models" or DDPMs for chirographic data that
specifically addresses these flaws. Our model named "ChiroDiff", being
non-autoregressive, learns to capture holistic concepts and therefore remains
resilient to higher temporal sampling rate up to a good extent. Moreover, we
show that many important downstream utilities (e.g. conditional sampling,
creative mixing) can be flexibly implemented using ChiroDiff. We further show
some unique use-cases like stochastic vectorization, de-noising/healing,
abstraction are also possible with this model-class. We perform quantitative
and qualitative evaluation of our framework on relevant datasets and found it
to be better or on par with competing approaches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 06, 2023 </span>    
         <span class="authors"> Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi Jaakkola, Shiyu Chang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.03322" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image inpainting refers to the task of generating a complete, natural image
based on a partially revealed reference image. Recently, many research
interests have been focused on addressing this problem using fixed diffusion
models. These approaches typically directly replace the revealed region of the
intermediate or final generated images with that of the reference image or its
variants. However, since the unrevealed regions are not directly modified to
match the context, it results in incoherence between revealed and unrevealed
regions. To address the incoherence problem, a small number of methods
introduce a rigorous Bayesian framework, but they tend to introduce mismatches
between the generated and the reference images due to the approximation errors
in computing the posterior distributions. In this paper, we propose COPAINT,
which can coherently inpaint the whole image without introducing mismatches.
COPAINT also uses the Bayesian framework to jointly modify both revealed and
unrevealed regions, but approximates the posterior distribution in a way that
allows the errors to gradually drop to zero throughout the denoising steps,
thus strongly penalizing any mismatches with the reference image. Our
experiments verify that COPAINT can outperform the existing diffusion-based
methods under both objective and subjective metrics. The codes are available at
https://github.com/UCSB-NLP-Chang/CoPaint/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models as Masked Autoencoders
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 06, 2023 </span>    
         <span class="authors"> Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, Christoph Feichtenhofer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.03283" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 There has been a longstanding belief that generation can facilitate a true
understanding of visual data. In line with this, we revisit generatively
pre-training visual representations in light of recent interest in denoising
diffusion models. While directly pre-training with diffusion models does not
produce strong representations, we condition diffusion models on masked input
and formulate diffusion models as masked autoencoders (DiffMAE). Our approach
is capable of (i) serving as a strong initialization for downstream recognition
tasks, (ii) conducting high-quality image inpainting, and (iii) being
effortlessly extended to video where it produces state-of-the-art
classification accuracy. We further perform a comprehensive study on the pros
and cons of design choices and build connections between diffusion models and
masked autoencoders.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Anomaly Detection via Gumbel Noise Score Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 06, 2023 </span>    
         <span class="authors"> Ahsan Mahmood, Junier Oliva, Martin Styner </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.03220" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose Gumbel Noise Score Matching (GNSM), a novel unsupervised method to
detect anomalies in categorical data. GNSM accomplishes this by estimating the
scores, i.e. the gradients of log likelihoods w.r.t.~inputs, of continuously
relaxed categorical distributions. We test our method on a suite of anomaly
detection tabular datasets. GNSM achieves a consistently high performance
across all experiments. We further demonstrate the flexibility of GNSM by
applying it to image data where the model is tasked to detect poor segmentation
predictions. Images ranked anomalous by GNSM show clear segmentation failures,
with the outputs of GNSM strongly correlating with segmentation metrics
computed on ground-truth. We outline the score matching training objective
utilized by GNSM and provide an open-source implementation of our work.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 06, 2023 </span>    
         <span class="authors"> Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.02827" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The increasing demand for high-quality 3D content creation has motivated the
development of automated methods for creating 3D object models from a single
image and/or from a text prompt. However, the reconstructed 3D objects using
state-of-the-art image-to-3D methods still exhibit low correspondence to the
given image and low multi-view consistency. Recent state-of-the-art text-to-3D
methods are also limited, yielding 3D samples with low diversity per prompt
with long synthesis time. To address these challenges, we propose DITTO-NeRF, a
novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a
single image. Our DITTO-NeRF consists of constructing high-quality partial 3D
object for limited in-boundary (IB) angles using the given or text-generated 2D
image from the frontal view and then iteratively reconstructing the remaining
3D NeRF using inpainting latent diffusion model. We propose progressive 3D
object reconstruction schemes in terms of scales (low to high resolution),
angles (IB angles initially to outer-boundary (OB) later), and masks (object to
background boundary) in our DITTO-NeRF so that high-quality information on IB
can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods
in terms of fidelity and diversity qualitatively and quantitatively with much
faster training times than prior arts on image/text-to-3D such as DreamFusion,
and NeuralLift-360.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Zero-shot Medical Image Translation via Frequency-Guided Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 05, 2023 </span>    
         <span class="authors"> Yunxiang Li, Hua-Chieh Shao, Xiao Liang, Liyuan Chen, Ruiqi Li, Steve Jiang, Jing Wang, You Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.02742" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, the diffusion model has emerged as a superior generative model that
can produce high-quality images with excellent realism. There is a growing
interest in applying diffusion models to image translation tasks. However, for
medical image translation, the existing diffusion models are deficient in
accurately retaining structural information since the structure details of
source domain images are lost during the forward diffusion process and cannot
be fully recovered through learned reverse diffusion, while the integrity of
anatomical structures is extremely important in medical images. Training and
conditioning diffusion models using paired source and target images with
matching anatomy can help. However, such paired data are very difficult and
costly to obtain, and may also reduce the robustness of the developed model to
out-of-distribution testing data. We propose a frequency-guided diffusion model
(FGDM) that employs frequency-domain filters to guide the diffusion model for
structure-preserving image translation. Based on its design, FGDM allows
zero-shot learning, as it can be trained solely on the data from the target
domain, and used directly for source-to-target domain translation without any
exposure to the source-domain data during training. We trained FGDM solely on
the head-and-neck CT data, and evaluated it on both head-and-neck and lung
cone-beam CT (CBCT)-to-CT translation tasks. FGDM outperformed the
state-of-the-art methods (GAN-based, VAE-based, and diffusion-based) in all
metrics, showing its significant advantages in zero-shot medical image
translation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GenPhys: From Physical Processes to Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 05, 2023 </span>    
         <span class="authors"> Ziming Liu, Di Luo, Yilun Xu, Tommi Jaakkola, Max Tegmark </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.02637" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, physics.comp-ph, physics.data-an, quant-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Since diffusion models (DM) and the more recent Poisson flow generative
models (PFGM) are inspired by physical processes, it is reasonable to ask: Can
physical processes offer additional new generative models? We show that the
answer is yes. We introduce a general family, Generative Models from Physical
Processes (GenPhys), where we translate partial differential equations (PDEs)
describing physical processes to generative models. We show that generative
models can be constructed from s-generative PDEs (s for smooth). GenPhys
subsume the two existing generative models (DM and PFGM) and even give rise to
new families of generative models, e.g., "Yukawa Generative Models" inspired
from weak interactions. On the other hand, some physical processes by default
do not belong to the GenPhys family, e.g., the wave equation and the
Schr\"{o}dinger equation, but could be made into the GenPhys family with some
modifications. Our goal with GenPhys is to explore and expand the design space
of generative models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generative Novel View Synthesis with 3D-Aware Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 05, 2023 </span>    
         <span class="authors"> Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, Gordon Wetzstein </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.02602" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a diffusion-based model for 3D-aware generative novel view
synthesis from as few as a single input image. Our model samples from the
distribution of possible renderings consistent with the input and, even in the
presence of ambiguity, is capable of rendering diverse and plausible novel
views. To achieve this, our method makes use of existing 2D diffusion backbones
but, crucially, incorporates geometry priors in the form of a 3D feature
volume. This latent feature field captures the distribution over possible scene
representations and improves our method's ability to generate view-consistent
novel renderings. In addition to generating novel views, our method has the
ability to autoregressively synthesize 3D-consistent sequences. We demonstrate
state-of-the-art results on synthetic renderings and room-scale scenes; we also
show compelling results for challenging, real-world objects.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## EigenFold: Generative Protein Structure Prediction with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 05, 2023 </span>    
         <span class="authors"> Bowen Jing, Ezra Erives, Peter Pao-Huang, Gabriele Corso, Bonnie Berger, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.02198" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.LG, physics.bio-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Protein structure prediction has reached revolutionary levels of accuracy on
single structures, yet distributional modeling paradigms are needed to capture
the conformational ensembles and flexibility that underlie biological function.
Towards this goal, we develop EigenFold, a diffusion generative modeling
framework for sampling a distribution of structures from a given protein
sequence. We define a diffusion process that models the structure as a system
of harmonic oscillators and which naturally induces a cascading-resolution
generative process along the eigenmodes of the system. On recent CAMEO targets,
EigenFold achieves a median TMScore of 0.84, while providing a more
comprehensive picture of model uncertainty via the ensemble of sampled
structures relative to existing methods. We then assess EigenFold's ability to
model and predict conformational heterogeneity for fold-switching proteins and
ligand-induced conformational change. Code is available at
https://github.com/bjing2016/EigenFold.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Diffusion-based Method for Multi-turn Compositional Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 05, 2023 </span>    
         <span class="authors"> Chao Wang, Xiaoyu Yang, Jinmiao Huang, Kevin Ferreira </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.02192" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Multi-turn compositional image generation (M-CIG) is a challenging task that
aims to iteratively manipulate a reference image given a modification text.
While most of the existing methods for M-CIG are based on generative
adversarial networks (GANs), recent advances in image generation have
demonstrated the superiority of diffusion models over GANs. In this paper, we
propose a diffusion-based method for M-CIG named conditional denoising
diffusion with image compositional matching (CDD-ICM). We leverage CLIP as the
backbone of image and text encoders, and incorporate a gated fusion mechanism,
originally proposed for question answering, to compositionally fuse the
reference image and the modification text at each turn of M-CIG. We introduce a
conditioning scheme to generate the target image based on the fusion results.
To prioritize the semantic quality of the generated target image, we learn an
auxiliary image compositional match (ICM) objective, along with the conditional
denoising diffusion (CDD) objective in a multi-task learning framework.
Additionally, we also perform ICM guidance and classifier-free guidance to
improve performance. Experimental results show that CDD-ICM achieves
state-of-the-art results on two benchmark datasets for M-CIG, i.e., CoDraw and
i-CLEVR.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 04, 2023 </span>    
         <span class="authors"> Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, Or Litany </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.01893" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce a method for generating realistic pedestrian trajectories and
full-body animations that can be controlled to meet user-defined goals. We draw
on recent advances in guided diffusion modeling to achieve test-time
controllability of trajectories, which is normally only associated with
rule-based systems. Our guided diffusion model allows users to constrain
trajectories through target waypoints, speed, and specified social groups while
accounting for the surrounding environment context. This trajectory diffusion
model is integrated with a novel physics-based humanoid controller to form a
closed-loop, full-body pedestrian animation system capable of placing large
crowds in a simulated environment with varying terrains. We further propose
utilizing the value function learned during RL training of the animation
controller to guide diffusion to produce trajectories better suited for
particular scenarios such as collision avoidance and traversing uneven terrain.
Video results are available on the project page at
https://nv-tlabs.github.io/trace-pace .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 04, 2023 </span>    
         <span class="authors"> Duo Xu, Jonathan C. Tan, Chia-Jung Hsu, Ye Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.01670" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  astro-ph.GA, astro-ph.IM, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce the state-of-the-art deep learning Denoising Diffusion
Probabilistic Model (DDPM) as a method to infer the volume or number density of
giant molecular clouds (GMCs) from projected mass surface density maps. We
adopt magnetohydrodynamic simulations with different global magnetic field
strengths and large-scale dynamics, i.e., noncolliding and colliding GMCs. We
train a diffusion model on both mass surface density maps and their
corresponding mass-weighted number density maps from different viewing angles
for all the simulations. We compare the diffusion model performance with a more
traditional empirical two-component and three-component power-law fitting
method and with a more traditional neural network machine learning approach
(CASI-2D). We conclude that the diffusion model achieves an order of magnitude
improvement on the accuracy of predicting number density compared to that by
other methods. We apply the diffusion method to some example astronomical
column density maps of Taurus and the Infrared Dark Clouds (IRDCs) G28.37+0.07
and G35.39-0.33 to produce maps of their mean volume densities.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 04, 2023 </span>    
         <span class="authors"> Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, Chaoning Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.01565" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have become a new SOTA generative modeling method in various
fields, for which there are multiple survey works that provide an overall
survey. With the number of articles on diffusion models increasing
exponentially in the past few years, there is an increasing need for surveys of
diffusion models on specific fields. In this work, we are committed to
conducting a survey on the graph diffusion models. Even though our focus is to
cover the progress of diffusion models in graphs, we first briefly summarize
how other generative modeling methods are used for graphs. After that, we
introduce the mechanism of diffusion models in various forms, which facilitates
the discussion on the graph diffusion models. The applications of graph
diffusion models mainly fall into the category of AI-generated content (AIGC)
in science, for which we mainly focus on how graph diffusion models are
utilized for generating molecules and proteins but also cover other cases,
including materials design. Moreover, we discuss the issue of evaluating
diffusion models in the graph domain and the existing challenges.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 03, 2023 </span>    
         <span class="authors"> Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, Ziwei Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.01116" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 3D human motion generation is crucial for creative industry. Recent advances
rely on generative models with domain knowledge for text-driven motion
generation, leading to substantial progress in capturing common motions.
However, the performance on more diverse motions remains unsatisfactory. In
this work, we propose ReMoDiffuse, a diffusion-model-based motion generation
framework that integrates a retrieval mechanism to refine the denoising
process. ReMoDiffuse enhances the generalizability and diversity of text-driven
motion generation with three key designs: 1) Hybrid Retrieval finds appropriate
references from the database in terms of both semantic and kinematic
similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval
knowledge, adapting to the difference between retrieved samples and the target
motion sequence. 3) Condition Mixture better utilizes the retrieval database
during inference, overcoming the scale sensitivity in classifier-free guidance.
Extensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art
methods by balancing both text-motion consistency and motion quality,
especially for more diverse motion generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Bridge Mixture Transports, Schrödinger Bridge Problems and Generative Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 03, 2023 </span>    
         <span class="authors"> Stefano Peluchetti </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.00917" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The dynamic Schr\"odinger bridge problem seeks a stochastic process that
defines a transport between two target probability measures, while optimally
satisfying the criteria of being closest, in terms of Kullback-Leibler
divergence, to a reference process.
  We propose a novel sampling-based iterative algorithm, the iterated diffusion
bridge mixture transport (IDBM), aimed at solving the dynamic Schr\"odinger
bridge problem. The IDBM procedure exhibits the attractive property of
realizing a valid coupling between the target measures at each step. We perform
an initial theoretical investigation of the IDBM procedure, establishing its
convergence properties. The theoretical findings are complemented by numerous
numerical experiments illustrating the competitive performance of the IDBM
procedure across various applications.
  Recent advancements in generative modeling employ the time-reversal of a
diffusion process to define a generative process that approximately transports
a simple distribution to the data distribution. As an alternative, we propose
using the first iteration of the IDBM procedure as an approximation-free method
for realizing this transport. This approach offers greater flexibility in
selecting the generative process dynamics and exhibits faster training and
superior sample quality over longer discretization intervals. In terms of
implementation, the necessary modifications are minimally intrusive, being
limited to the training loss computation, with no changes necessary for
generative sampling.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 03, 2023 </span>    
         <span class="authors"> Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.00916" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present DreamAvatar, a text-and-shape guided framework for generating
high-quality 3D human avatars with controllable poses. While encouraging
results have been produced by recent methods on text-guided 3D common object
generation, generating high-quality human avatars remains an open challenge due
to the complexity of the human body's shape, pose, and appearance. We propose
DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for
predicting density and color features for 3D points and a pre-trained
text-to-image diffusion model for providing 2D self-supervision. Specifically,
we leverage SMPL models to provide rough pose and shape guidance for the
generation. We introduce a dual space design that comprises a canonical space
and an observation space, which are related by a learnable deformation field
through the NeRF, allowing for the transfer of well-optimized texture and
geometry from the canonical space to the target posed avatar. Additionally, we
exploit a normal-consistency regularization to allow for more vivid generation
with detailed geometry and texture. Through extensive evaluations, we
demonstrate that DreamAvatar significantly outperforms existing methods,
establishing a new state-of-the-art for text-and-shape guided 3D human
generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Textile Pattern Generation Using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 02, 2023 </span>    
         <span class="authors"> Halil Faruk Karagoz, Gulcin Baykal, Irem Arikan Eksi, Gozde Unal </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.00520" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The problem of text-guided image generation is a complex task in Computer
Vision, with various applications, including creating visually appealing
artwork and realistic product images. One popular solution widely used for this
task is the diffusion model, a generative model that generates images through
an iterative process. Although diffusion models have demonstrated promising
results for various image generation tasks, they may only sometimes produce
satisfactory results when applied to more specific domains, such as the
generation of textile patterns based on text guidance. This study presents a
fine-tuned diffusion model specifically trained for textile pattern generation
by text guidance to address this issue. The study involves the collection of
various textile pattern images and their captioning with the help of another AI
model. The fine-tuned diffusion model is trained with this newly created
dataset, and its results are compared with the baseline models visually and
numerically. The results demonstrate that the proposed fine-tuned diffusion
model outperforms the baseline models in terms of pattern quality and
efficiency in textile pattern generation by text guidance. This study presents
a promising solution to the problem of text-guided textile pattern generation
and has the potential to simplify the design process within the textile
industry.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Inf-Diff: Infinite Resolution Diffusion with Subsampled Mollified States
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 31, 2023 </span>    
         <span class="authors"> Sam Bond-Taylor, Chris G. Willcocks </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.18242" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce $\infty$-Diff, a generative diffusion model which directly
operates on infinite resolution data. By randomly sampling subsets of
coordinates during training and learning to denoise the content at those
coordinates, a continuous function is learned that allows sampling at arbitrary
resolutions. In contrast to other recent infinite resolution generative models,
our approach operates directly on the raw data, not requiring latent vector
compression for context, using hypernetworks, nor relying on discrete
components. As such, our approach achieves significantly higher sample quality,
as evidenced by lower FID scores, as well as being able to effectively scale to
higher resolutions than the training data while retaining detail.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Closer Look at Parameter-Efficient Tuning in Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 31, 2023 </span>    
         <span class="authors"> Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.18181" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large-scale diffusion models like Stable Diffusion are powerful and find
various real-world applications while customizing such models by fine-tuning is
both memory and time inefficient. Motivated by the recent progress in natural
language processing, we investigate parameter-efficient tuning in large
diffusion models by inserting small learnable modules (termed adapters). In
particular, we decompose the design space of adapters into orthogonal factors
-- the input position, the output position as well as the function form, and
perform Analysis of Variance (ANOVA), a classical statistical approach for
analyzing the correlation between discrete (design options) and continuous
variables (evaluation metrics). Our analysis suggests that the input position
of adapters is the critical factor influencing the performance of downstream
tasks. Then, we carefully study the choice of the input position, and we find
that putting the input position after the cross-attention block can lead to the
best performance, validated by additional visualization analyses. Finally, we
provide a recipe for parameter-efficient tuning in diffusion models, which is
comparable if not superior to the fully fine-tuned baseline (e.g., DreamBooth)
with only 0.75 \% extra parameters, across various customized tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Action Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 31, 2023 </span>    
         <span class="authors"> Daochang Liu, Qiyue Li, AnhDung Dinh, Tingting Jiang, Mubarak Shah, Chang Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17959" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Temporal action segmentation is crucial for understanding long-form videos.
Previous works on this task commonly adopt an iterative refinement paradigm by
using multi-stage models. Our paper proposes an essentially different framework
via denoising diffusion models, which nonetheless shares the same inherent
spirit of such iterative refinement. In this framework, action predictions are
progressively generated from random noise with input video features as
conditions. To enhance the modeling of three striking characteristics of human
actions, including the position prior, the boundary ambiguity, and the
relational dependency, we devise a unified masking strategy for the
conditioning inputs in our framework. Extensive experiments on three benchmark
datasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposed
method achieves superior or comparable results to state-of-the-art methods,
showing the effectiveness of a generative approach for action segmentation. Our
codes will be made available.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Reference-based Image Composition with Sketch via Structure-aware Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 31, 2023 </span>    
         <span class="authors"> Kangyeol Kim, Sunghyun Park, Junsoo Lee, Jaegul Choo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2304.09748" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent remarkable improvements in large-scale text-to-image generative models
have shown promising results in generating high-fidelity images. To further
enhance editability and enable fine-grained generation, we introduce a
multi-input-conditioned image composition model that incorporates a sketch as a
novel modal, alongside a reference image. Thanks to the edge-level
controllability using sketches, our method enables a user to edit or complete
an image sub-part with a desired structure (i.e., sketch) and content (i.e.,
reference image). Our framework fine-tunes a pre-trained diffusion model to
complete missing regions using the reference image while maintaining sketch
guidance. Albeit simple, this leads to wide opportunities to fulfill user needs
for obtaining the in-demand images. Through extensive experiments, we
demonstrate that our proposed method offers unique use cases for image
manipulation, enabling user-driven modifications of arbitrary scenes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Token Merging for Fast Stable Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 30, 2023 </span>    
         <span class="authors"> Daniel Bolya, Judy Hoffman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17604" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The landscape of image generation has been forever changed by open vocabulary
diffusion models. However, at their core these models use transformers, which
makes generation slow. Better implementations to increase the throughput of
these transformers have emerged, but they still evaluate the entire model. In
this paper, we instead speed up diffusion models by exploiting natural
redundancy in generated images by merging redundant tokens. After making some
diffusion-specific improvements to Token Merging (ToMe), our ToMe for Stable
Diffusion can reduce the number of tokens in an existing Stable Diffusion model
by up to 60% while still producing high quality images without any extra
training. In the process, we speed up image generation by up to 2x and reduce
memory consumption by up to 5.6x. Furthermore, this speed-up stacks with
efficient implementations such as xFormers, minimally impacting quality while
being up to 5.4x faster for large images. Code is available at
https://github.com/dbolya/tomesd.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 30, 2023 </span>    
         <span class="authors"> Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, Humphrey Shi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17591" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The unlearning problem of deep learning models, once primarily an academic
concern, has become a prevalent issue in the industry. The significant advances
in text-to-image generation techniques have prompted global discussions on
privacy, copyright, and safety, as numerous unauthorized personal IDs, content,
artistic creations, and potentially harmful materials have been learned by
these models and later utilized to generate and distribute uncontrolled
content. To address this challenge, we propose \textbf{Forget-Me-Not}, an
efficient and low-cost solution designed to safely remove specified IDs,
objects, or styles from a well-configured text-to-image model in as little as
30 seconds, without impairing its ability to generate other content. Alongside
our method, we introduce the \textbf{Memorization Score (M-Score)} and
\textbf{ConceptBench} to measure the models' capacity to generate general
concepts, grouped into three primary categories: ID, object, and style. Using
M-Score and ConceptBench, we demonstrate that Forget-Me-Not can effectively
eliminate targeted concepts while maintaining the model's performance on other
concepts. Furthermore, Forget-Me-Not offers two practical extensions: a)
removal of potentially harmful or NSFW content, and b) enhancement of model
accuracy, inclusion and diversity through \textbf{concept correction and
disentanglement}. It can also be adapted as a lightweight model patch for
Stable Diffusion, allowing for concept manipulation and convenient
distribution. To encourage future research in this critical area and promote
the development of safe and inclusive generative models, we will open-source
our code and ConceptBench at
\href{https://github.com/SHI-Labs/Forget-Me-Not}{https://github.com/SHI-Labs/Forget-Me-Not}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DDP: Diffusion Model for Dense Visual Prediction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 30, 2023 </span>    
         <span class="authors"> Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17559" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a simple, efficient, yet powerful framework for dense visual
predictions based on the conditional diffusion pipeline. Our approach follows a
"noise-to-map" generative paradigm for prediction by progressively removing
noise from a random Gaussian distribution, guided by the image. The method,
called DDP, efficiently extends the denoising diffusion process into the modern
perception pipeline. Without task-specific design and architecture
customization, DDP is easy to generalize to most dense prediction tasks, e.g.,
semantic segmentation and depth estimation. In addition, DDP shows attractive
properties such as dynamic inference and uncertainty awareness, in contrast to
previous single-step discriminative methods. We show top results on three
representative tasks with six diverse benchmarks, without tricks, DDP achieves
state-of-the-art or competitive performance on each task compared to the
specialist counterparts. For example, semantic segmentation (83.9 mIoU on
Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation
(0.05 REL on KITTI). We hope that our approach will serve as a solid baseline
and facilitate future research
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 30, 2023 </span>    
         <span class="authors"> Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, Humphrey Shi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17546" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image editing using diffusion models has witnessed extremely fast-paced
growth recently. There are various ways in which previous works enable
controlling and editing images. Some works use high-level conditioning such as
text, while others use low-level conditioning. Nevertheless, most of them lack
fine-grained control over the properties of the different objects present in
the image, i.e. object-level image editing. In this work, we consider an image
as a composition of multiple objects, each defined by various properties. Out
of these properties, we identify structure and appearance as the most intuitive
to understand and useful for editing purposes. We propose
Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is
trained using structure and appearance information explicitly extracted from
the images. The proposed model enables users to inject a reference image's
appearance into the input image at both the object and global levels.
Additionally, PAIR-Diffusion allows editing the structure while maintaining the
style of individual components of the image unchanged. We extensively evaluate
our method on LSUN datasets and the CelebA-HQ face dataset, and we demonstrate
fine-grained control over both structure and appearance at the object level. We
also applied the method to Stable Diffusion to edit any real image at the
object level.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 30, 2023 </span>    
         <span class="authors"> Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, Xi Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17189" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models have achieved great success in image synthesis.
However, when it comes to the layout-to-image generation where an image often
has a complex scene of multiple objects, how to make strong control over both
the global layout map and each detailed object remains a challenging task. In
this paper, we propose a diffusion model named LayoutDiffusion that can obtain
higher generation quality and greater controllability than the previous works.
To overcome the difficult multimodal fusion of image and layout, we propose to
construct a structural image patch with region information and transform the
patched image into a special layout to fuse with the normal layout in a unified
form. Moreover, Layout Fusion Module (LFM) and Object-aware Cross Attention
(OaCA) are proposed to model the relationship among multiple objects and
designed to be object-aware and position-sensitive, allowing for precisely
controlling the spatial related information. Extensive experiments show that
our LayoutDiffusion outperforms the previous SOTA methods on FID, CAS by
relatively 46.35%, 26.70% on COCO-stuff and 44.29%, 41.82% on VG. Code is
available at https://github.com/ZGCTroy/LayoutDiffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Discriminative Class Tokens for Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 30, 2023 </span>    
         <span class="authors"> Idan Schwartz, Vésteinn Snæbjarnarson, Sagie Benaim, Hila Chefer, Ryan Cotterell, Lior Wolf, Serge Belongie </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17155" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advances in text-to-image diffusion models have enabled the generation
of diverse and high-quality images. However, generated images often fall short
of depicting subtle details and are susceptible to errors due to ambiguity in
the input text. One way of alleviating these issues is to train diffusion
models on class-labeled datasets. This comes with a downside, doing so limits
their expressive power: (i) supervised datasets are generally small compared to
large-scale scraped text-image datasets on which text-to-image models are
trained, and so the quality and diversity of generated images are severely
affected, or (ii) the input is a hard-coded label, as opposed to free-form
text, which limits the control over the generated images.
  In this work, we propose a non-invasive fine-tuning technique that
capitalizes on the expressive potential of free-form text while achieving high
accuracy through discriminative signals from a pretrained classifier, which
guides the generation. This is done by iteratively modifying the embedding of a
single input token of a text-to-image diffusion model, using the classifier, by
steering generated images toward a given target class. Our method is fast
compared to prior fine-tuning methods and does not require a collection of
in-class images or retraining of a noise-tolerant classifier. We evaluate our
method extensively, showing that the generated images are: (i) more accurate
and of higher quality than standard diffusion models, (ii) can be used to
augment training data in a low-resource setting, and (iii) reveal information
about the data used to train the guiding classifier. The code is available at
\url{https://github.com/idansc/discriminative_class_tokens}
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffCollage: Parallel Generation of Large Content with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 30, 2023 </span>    
         <span class="authors"> Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, Ming-Yu Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17076" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present DiffCollage, a compositional diffusion model that can generate
large content by leveraging diffusion models trained on generating pieces of
the large content. Our approach is based on a factor graph representation where
each factor node represents a portion of the content and a variable node
represents their overlap. This representation allows us to aggregate
intermediate outputs from diffusion models defined on individual nodes to
generate content of arbitrary size and shape in parallel without resorting to
an autoregressive generation procedure. We apply DiffCollage to various tasks,
including infinite image generation, panorama image generation, and
long-duration text-guided motion generation. Extensive experimental results
with a comparison to strong autoregressive baselines verify the effectiveness
of our approach.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 29, 2023 </span>    
         <span class="authors"> Ziya Erkoç, Fangchang Ma, Qi Shan, Matthias Nießner, Angela Dai </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.17015" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Implicit neural fields, typically encoded by a multilayer perceptron (MLP)
that maps from coordinates (e.g., xyz) to signals (e.g., signed distances),
have shown remarkable promise as a high-fidelity and compact representation.
However, the lack of a regular and explicit grid structure also makes it
challenging to apply generative modeling directly on implicit neural fields in
order to synthesize new data. To this end, we propose HyperDiffusion, a novel
approach for unconditional generative modeling of implicit neural fields.
HyperDiffusion operates directly on MLP weights and generates new neural
implicit fields encoded by synthesized MLP parameters. Specifically, a
collection of MLPs is first optimized to faithfully represent individual data
samples. Subsequently, a diffusion process is trained in this MLP weight space
to model the underlying distribution of neural implicit fields. HyperDiffusion
enables diffusion modeling over a implicit, compact, and yet high-fidelity
representation of complex signals across 3D shapes and 4D mesh animations
within one single unified framework.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 29, 2023 </span>    
         <span class="authors"> Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, Chuang Gan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.16897" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Modeling sounds emitted from physical object interactions is critical for
immersive perceptual experiences in real and virtual worlds. Traditional
methods of impact sound synthesis use physics simulation to obtain a set of
physics parameters that could represent and synthesize the sound. However, they
require fine details of both the object geometries and impact locations, which
are rarely available in the real world and can not be applied to synthesize
impact sounds from common videos. On the other hand, existing video-driven deep
learning-based approaches could only capture the weak correspondence between
visual content and impact sounds since they lack of physics knowledge. In this
work, we propose a physics-driven diffusion model that can synthesize
high-fidelity impact sound for a silent video clip. In addition to the video
content, we propose to use additional physics priors to guide the impact sound
synthesis procedure. The physics priors include both physics parameters that
are directly estimated from noisy real-world impact sound examples without
sophisticated setup and learned residual parameters that interpret the sound
environment via neural networks. We further implement a novel diffusion model
with specific training and inference strategies to combine physics priors and
visual information for impact sound synthesis. Experimental results show that
our model outperforms several existing systems in generating realistic impact
sounds. More importantly, the physics-based representations are fully
interpretable and transparent, thus enabling us to perform sound editing
flexibly.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Schrödinger Bridge Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 29, 2023 </span>    
         <span class="authors"> Yuyang Shi, Valentin De Bortoli, Andrew Campbell, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.16852" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Solving transport problems, i.e. finding a map transporting one given
distribution to another, has numerous applications in machine learning. Novel
mass transport methods motivated by generative modeling have recently been
proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models
(FMMs) implement such a transport through a Stochastic Differential Equation
(SDE) or an Ordinary Differential Equation (ODE). However, while it is
desirable in many applications to approximate the deterministic dynamic Optimal
Transport (OT) map which admits attractive properties, DDMs and FMMs are not
guaranteed to provide transports close to the OT map. In contrast,
Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover
entropy-regularized versions of OT. Unfortunately, existing numerical methods
approximating SBs either scale poorly with dimension or accumulate errors
across iterations. In this work, we introduce Iterative Markovian Fitting, a
new methodology for solving SB problems, and Diffusion Schr\"odinger Bridge
Matching (DSBM), a novel numerical algorithm for computing IMF iterates. DSBM
significantly improves over previous SB numerics and recovers as
special/limiting cases various recent transport methods. We demonstrate the
performance of DSBM on a variety of problems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## 4D Facial Expression Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 29, 2023 </span>    
         <span class="authors"> Kaifeng Zou, Sylvain Faisan, Boyang Yu, Sébastien Valette, Hyewon Seo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.16611" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Facial expression generation is one of the most challenging and long-sought
aspects of character animation, with many interesting applications. The
challenging task, traditionally having relied heavily on digital craftspersons,
remains yet to be explored. In this paper, we introduce a generative framework
for generating 3D facial expression sequences (i.e. 4D faces) that can be
conditioned on different inputs to animate an arbitrary 3D face mesh. It is
composed of two tasks: (1) Learning the generative model that is trained over a
set of 3D landmark sequences, and (2) Generating 3D mesh sequences of an input
facial mesh driven by the generated landmark sequences. The generative model is
based on a Denoising Diffusion Probabilistic Model (DDPM), which has achieved
remarkable success in generative tasks of other domains. While it can be
trained unconditionally, its reverse process can still be conditioned by
various condition signals. This allows us to efficiently develop several
downstream tasks involving various conditional generation, by using expression
labels, text, partial sequences, or simply a facial geometry. To obtain the
full mesh deformation, we then develop a landmark-guided encoder-decoder to
apply the geometrical deformation embedded in landmarks on a given facial mesh.
Experiments show that our model has learned to generate realistic, quality
expressions solely from the dataset of relatively small size, improving over
the state-of-the-art methods. Videos and qualitative comparisons with other
methods can be found at https://github.com/ZOUKaifeng/4DFM. Code and models
will be made available upon acceptance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## WordStylist: Styled Verbatim Handwritten Text Generation with Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 29, 2023 </span>    
         <span class="authors"> Konstantina Nikolaidou, George Retsinas, Vincent Christlein, Mathias Seuret, Giorgos Sfikas, Elisa Barney Smith, Hamam Mokayed, Marcus Liwicki </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.16576" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-Image synthesis is the task of generating an image according to a
specific text description. Generative Adversarial Networks have been considered
the standard method for image synthesis virtually since their introduction.
Denoising Diffusion Probabilistic Models are recently setting a new baseline,
with remarkable results in Text-to-Image synthesis, among other fields. Aside
its usefulness per se, it can also be particularly relevant as a tool for data
augmentation to aid training models for other document image processing tasks.
In this work, we present a latent diffusion-based method for styled
text-to-text-content-image generation on word-level. Our proposed method is
able to generate realistic word image samples from different writer styles, by
using class index styles and text content prompts without the need of
adversarial training, writer recognition, or text recognition. We gauge system
performance with the Fr\'echet Inception Distance, writer recognition accuracy,
and writer retrieval. We show that the proposed model produces samples that are
aesthetically pleasing, help boosting text recognition performance, and get
similar writer retrieval score as real data. Code is available at:
https://github.com/koninik/WordStylist.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Your Diffusion Model is Secretly a Zero-Shot Classifier
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 28, 2023 </span>    
         <span class="authors"> Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, Deepak Pathak </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.16203" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, cs.NE, cs.RO
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The recent wave of large-scale text-to-image diffusion models has
dramatically increased our text-based image generation abilities. These models
can generate realistic images for a staggering variety of prompts and exhibit
impressive compositional generalization abilities. Almost all use cases thus
far have solely focused on sampling; however, diffusion models can also provide
conditional density estimates, which are useful for tasks beyond image
generation. In this paper, we show that the density estimates from large-scale
text-to-image diffusion models like Stable Diffusion can be leveraged to
perform zero-shot classification without any additional training. Our
generative approach to classification, which we call Diffusion Classifier,
attains strong results on a variety of benchmarks and outperforms alternative
methods of extracting knowledge from diffusion models. Although a gap remains
between generative and discriminative approaches on zero-shot recognition
tasks, we find that our diffusion-based approach has stronger multimodal
relational reasoning abilities than competing discriminative approaches.
Finally, we use Diffusion Classifier to extract standard classifiers from
class-conditional diffusion models trained on ImageNet. Even though these
models are trained with weak augmentations and no regularization, they approach
the performance of SOTA discriminative classifiers. Overall, our results are a
step toward using generative over discriminative models for downstream tasks.
Results and visualizations at https://diffusion-classifier.github.io/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Visual Chain-of-Thought Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 28, 2023 </span>    
         <span class="authors"> William Harvey, Frank Wood </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.16187" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent progress with conditional image diffusion models has been stunning,
and this holds true whether we are speaking about models conditioned on a text
description, a scene layout, or a sketch. Unconditional image diffusion models
are also improving but lag behind, as do diffusion models which are conditioned
on lower-dimensional features like class labels. We propose to close the gap
between conditional and unconditional models using a two-stage sampling
procedure. In the first stage we sample an embedding describing the semantic
content of the image. In the second stage we sample the image conditioned on
this embedding and then discard the embedding. Doing so lets us leverage the
power of conditional diffusion models on the unconditional generation task,
which we show improves FID by 25-50% compared to standard unconditional
generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DDMM-Synth: A Denoising Diffusion Model for Cross-modal Medical Image Synthesis with Sparse-view Measurement Embedding
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 28, 2023 </span>    
         <span class="authors"> Xiaoyue Li, Kai Shang, Gaoang Wang, Mark D. Butala </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.15770" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, physics.med-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Reducing the radiation dose in computed tomography (CT) is important to
mitigate radiation-induced risks. One option is to employ a well-trained model
to compensate for incomplete information and map sparse-view measurements to
the CT reconstruction. However, reconstruction from sparsely sampled
measurements is insufficient to uniquely characterize an object in CT, and a
learned prior model may be inadequate for unencountered cases. Medical modal
translation from magnetic resonance imaging (MRI) to CT is an alternative but
may introduce incorrect information into the synthesized CT images in addition
to the fact that there exists no explicit transformation describing their
relationship. To address these issues, we propose a novel framework called the
denoising diffusion model for medical image synthesis (DDMM-Synth) to close the
performance gaps described above. This framework combines an MRI-guided
diffusion model with a new CT measurement embedding reverse sampling scheme.
Specifically, the null-space content of the one-step denoising result is
refined by the MRI-guided data distribution prior, and its range-space
component derived from an explicit operator matrix and the sparse-view CT
measurements is directly integrated into the inference stage. DDMM-Synth can
adjust the projection number of CT a posteriori for a particular clinical
application and its modified version can even improve the results significantly
for noisy cases. Our results show that DDMM-Synth outperforms other
state-of-the-art supervised-learning-based baselines under fair experimental
conditions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffULD: Diffusive Universal Lesion Detection
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 28, 2023 </span>    
         <span class="authors"> Peiang Zhao, Han Li, Ruiyang Jin, S. Kevin Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.15728" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Universal Lesion Detection (ULD) in computed tomography (CT) plays an
essential role in computer-aided diagnosis. Promising ULD results have been
reported by anchor-based detection designs, but they have inherent drawbacks
due to the use of anchors: i) Insufficient training targets and ii)
Difficulties in anchor design. Diffusion probability models (DPM) have
demonstrated outstanding capabilities in many vision tasks. Many DPM-based
approaches achieve great success in natural image object detection without
using anchors. But they are still ineffective for ULD due to the insufficient
training targets. In this paper, we propose a novel ULD method, DiffULD, which
utilizes DPM for lesion detection. To tackle the negative effect triggered by
insufficient targets, we introduce a novel center-aligned bounding box padding
strategy that provides additional high-quality training targets yet avoids
significant performance deterioration. DiffULD is inherently advanced in
locating lesions with diverse sizes and shapes since it can predict with
arbitrary boxes. Experiments on the benchmark dataset DeepLesion show the
superiority of DiffULD when compared to state-of-the-art ULD approaches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Anti-DreamBooth: Protecting users from personalized text-to-image synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2023 </span>    
         <span class="authors"> Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, Anh Tran </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.15433" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-image diffusion models are nothing but a revolution, allowing anyone,
even without design skills, to create realistic images from simple text inputs.
With powerful personalization tools like DreamBooth, they can generate images
of a specific person just by learning from his/her few reference images.
However, when misused, such a powerful and convenient tool can produce fake
news or disturbing content targeting any individual victim, posing a severe
negative social impact. In this paper, we explore a defense system called
Anti-DreamBooth against such malicious use of DreamBooth. The system aims to
add subtle noise perturbation to each user's image before publishing in order
to disrupt the generation quality of any DreamBooth model trained on these
perturbed images. We investigate a wide range of algorithms for perturbation
optimization and extensively evaluate them on two facial datasets over various
text-to-image model versions. Despite the complicated formulation of DreamBooth
and Diffusion-based text-to-image models, our methods effectively defend users
from the malicious use of those models. Their effectiveness withstands even
adverse conditions, such as model or prompt/term mismatching between training
and testing. Our code will be available at
\href{https://github.com/VinAIResearch/Anti-DreamBooth.git}{https://github.com/VinAIResearch/Anti-DreamBooth.git}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2023 </span>    
         <span class="authors"> Susung Hong, Donghoon Ahn, Seungryong Kim </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.15413" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CL, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The view inconsistency problem in score-distilling text-to-3D generation,
also known as the Janus problem, arises from the intrinsic bias of 2D diffusion
models, which leads to the unrealistic generation of 3D objects. In this work,
we explore score-distilling text-to-3D generation and identify the main causes
of the Janus problem. Based on these findings, we propose two approaches to
debias the score-distillation frameworks for robust text-to-3D generation. Our
first approach, called score debiasing, involves gradually increasing the
truncation value for the score estimated by 2D diffusion models throughout the
optimization process. Our second approach, called prompt debiasing, identifies
conflicting words between user prompts and view prompts utilizing a language
model and adjusts the discrepancy between view prompts and object-space camera
poses. Our experimental results show that our methods improve realism by
significantly reducing artifacts and achieve a good trade-off between
faithfulness to the 2D diffusion models and 3D consistency with little
overhead.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Training-free Style Transfer Emerges from h-space in Diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2023 </span>    
         <span class="authors"> Jaeseok Jeong, Mingi Kwon, Youngjung Uh </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.15403" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models (DMs) synthesize high-quality images in various domains.
However, controlling their generative process is still hazy because the
intermediate variables in the process are not rigorously studied. Recently,
StyleCLIP-like editing of DMs is found in the bottleneck of the U-Net, named
$h$-space. In this paper, we discover that DMs inherently have disentangled
representations for content and style of the resulting images: $h$-space
contains the content and the skip connections convey the style. Furthermore, we
introduce a principled way to inject content of one image to another
considering progressive nature of the generative process. Briefly, given the
original generative process, 1) the feature of the source content should be
gradually blended, 2) the blended feature should be normalized to preserve the
distribution, 3) the change of skip connections due to content injection should
be calibrated. Then, the resulting image has the source content with the style
of the original image just like image-to-image translation. Interestingly,
injecting contents to styles of unseen domains produces harmonization-like
style transfer. To the best of our knowledge, our method introduces the first
training-free feed-forward style transfer only with an unconditional pretrained
frozen generative network. The code is available at
https://curryjung.github.io/DiffStyle/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Exploring Continual Learning of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2023 </span>    
         <span class="authors"> Michał Zając, Kamil Deja, Anna Kuzina, Jakub M. Tomczak, Tomasz Trzciński, Florian Shkurti, Piotr Miłoś </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.15342" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have achieved remarkable success in generating high-quality
images thanks to their novel training procedures applied to unprecedented
amounts of data. However, training a diffusion model from scratch is
computationally expensive. This highlights the need to investigate the
possibility of training these models iteratively, reusing computation while the
data distribution changes. In this study, we take the first step in this
direction and evaluate the continual learning (CL) properties of diffusion
models. We begin by benchmarking the most common CL methods applied to
Denoising Diffusion Probabilistic Models (DDPMs), where we note the strong
performance of the experience replay with the reduced rehearsal coefficient.
Furthermore, we provide insights into the dynamics of forgetting, which exhibit
diverse behavior across diffusion timesteps. We also uncover certain pitfalls
of using the bits-per-dimension metric for evaluating CL.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Text-to-Image Diffusion Models are Zero-Shot Classifiers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2023 </span>    
         <span class="authors"> Kevin Clark, Priyank Jaini </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.15233" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The excellent generative capabilities of text-to-image diffusion models
suggest they learn informative representations of image-text data. However,
what knowledge their representations capture is not fully understood, and they
have not been thoroughly explored on downstream tasks. We investigate diffusion
models by proposing a method for evaluating them as zero-shot classifiers. The
key idea is using a diffusion model's ability to denoise a noised image given a
text description of a label as a proxy for that label's likelihood. We apply
our method to Imagen, using it to probe fine-grained aspects of Imagen's
knowledge and comparing it with CLIP's zero-shot abilities. Imagen performs
competitively with CLIP on a wide range of zero-shot image classification
datasets. Additionally, it achieves state-of-the-art results on shape/texture
bias tests and can successfully perform attribute binding while CLIP cannot.
Although generative pre-training is prevalent in NLP, visual foundation models
often use other methods such as contrastive learning. Based on our findings, we
argue that generative pre-training should be explored as a compelling
alternative for vision and vision-language problems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2023 </span>    
         <span class="authors"> Nicola Franco, Daniel Korth, Jeanette Miriam Lorenz, Karsten Roscher, Stephan Guennemann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14961" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 As the use of machine learning continues to expand, the importance of
ensuring its safety cannot be overstated. A key concern in this regard is the
ability to identify whether a given sample is from the training distribution,
or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries can
manipulate OOD samples in ways that lead a classifier to make a confident
prediction. In this study, we present a novel approach for certifying the
robustness of OOD detection within a $\ell_2$-norm around the input, regardless
of network architecture and without the need for specific components or
additional training. Further, we improve current techniques for detecting
adversarial attacks on OOD samples, while providing high levels of certified
and adversarial robustness on in-distribution samples. The average of all OOD
detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$
relative to previous approaches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Seer: Language Instructed Video Prediction with Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2023 </span>    
         <span class="authors"> Xianfan Gu, Chuan Wen, Jiaming Song, Yang Gao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14897" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Imagining the future trajectory is the key for robots to make sound planning
and successfully reach their goals. Therefore, text-conditioned video
prediction (TVP) is an essential task to facilitate general robot policy
learning, i.e., predicting future video frames with a given language
instruction and reference frames. It is a highly challenging task to ground
task-level goals specified by instructions and high-fidelity frames together,
requiring large-scale data and computation. To tackle this task and empower
robots with the ability to foresee the future, we propose a sample and
computation-efficient model, named \textbf{Seer}, by inflating the pretrained
text-to-image (T2I) stable diffusion models along the temporal axis. We inflate
the denoising U-Net and language conditioning model with two novel techniques,
Autoregressive Spatial-Temporal Attention and Frame Sequential Text Decomposer,
to propagate the rich prior knowledge in the pretrained T2I models across the
frames. With the well-designed architecture, Seer makes it possible to generate
high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a
few layers on a small amount of data. The experimental results on Something
Something V2 (SSv2) and Bridgedata datasets demonstrate our superior video
prediction performance with around 210-hour training on 4 RTX 3090 GPUs:
decreasing the FVD of the current SOTA model from 290 to 200 on SSv2 and
achieving at least 70\% preference in the human evaluation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2023 </span>    
         <span class="authors"> Sauradip Nag, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14863" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, cs.MM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a new formulation of temporal action detection (TAD) with
denoising diffusion, DiffTAD in short. Taking as input random temporal
proposals, it can yield action proposals accurately given an untrimmed long
video. This presents a generative modeling perspective, against previous
discriminative learning manners. This capability is achieved by first diffusing
the ground-truth proposals to random ones (i.e., the forward/noising process)
and then learning to reverse the noising process (i.e., the backward/denoising
process). Concretely, we establish the denoising process in the Transformer
decoder (e.g., DETR) by introducing a temporal location query design with
faster convergence in training. We further propose a cross-step selective
conditioning algorithm for inference acceleration. Extensive evaluations on
ActivityNet and THUMOS show that our DiffTAD achieves top performance compared
to previous art alternatives. The code will be made available at
https://github.com/sauradip/DiffusionTAD.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Score-Based Reconstructions for Multi-contrast MRI
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 26, 2023 </span>    
         <span class="authors"> Brett Levac, Ajil Jalal, Kannan Ramchandran, Jonathan I. Tamir </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14795" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Magnetic resonance imaging (MRI) exam protocols consist of multiple
contrast-weighted images of the same anatomy to emphasize different tissue
properties. Due to the long acquisition times required to collect fully sampled
k-space measurements, it is common to only collect a fraction of k-space for
some, or all, of the scans and subsequently solve an inverse problem for each
contrast to recover the desired image from sub-sampled measurements. Recently,
there has been a push to further accelerate MRI exams using data-driven priors,
and generative models in particular, to regularize the ill-posed inverse
problem of image reconstruction. These methods have shown promising
improvements over classical methods. However, many of the approaches neglect
the multi-contrast nature of clinical MRI exams and treat each scan as an
independent reconstruction. In this work we show that by learning a joint
Bayesian prior over multi-contrast data with a score-based generative model we
are able to leverage the underlying structure between multi-contrast images and
thus improve image reconstruction fidelity over generative models that only
reconstruct images of a single contrast.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 26, 2023 </span>    
         <span class="authors"> Tenglong Ao, Zeyi Zhang, Libin Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14613" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The automatic generation of stylized co-speech gestures has recently received
increasing attention. Previous systems typically allow style control via
predefined text labels or example motion clips, which are often not flexible
enough to convey user intent accurately. In this work, we present
GestureDiffuCLIP, a neural network framework for synthesizing realistic,
stylized co-speech gestures with flexible style control. We leverage the power
of the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and
present a novel CLIP-guided mechanism that extracts efficient style
representations from multiple input modalities, such as a piece of text, an
example motion clip, or a video. Our system learns a latent diffusion model to
generate high-quality gestures and infuses the CLIP representations of style
into the generator via an adaptive instance normalization (AdaIN) layer. We
further devise a gesture-transcript alignment mechanism that ensures a
semantically correct gesture generation based on contrastive learning. Our
system can also be extended to allow fine-grained style control of individual
body parts. We demonstrate an extensive set of examples showing the flexibility
and generalizability of our model to a variety of style descriptions. In a user
study, we show that our system outperforms the state-of-the-art approaches
regarding human likeness, appropriateness, and style correctness.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 25, 2023 </span>    
         <span class="authors"> Zalan Fabian, Berk Tinaz, Mahdi Soltanolkotabi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14353" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG, I.2.6; I.4.4; I.4.5
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have established new state of the art in a multitude of
computer vision tasks, including image restoration. Diffusion-based inverse
problem solvers generate reconstructions of exceptional visual quality from
heavily corrupted measurements. However, in what is widely known as the
perception-distortion trade-off, the price of perceptually appealing
reconstructions is often paid in declined distortion metrics, such as PSNR.
Distortion metrics measure faithfulness to the observation, a crucial
requirement in inverse problems. In this work, we propose a novel framework for
inverse problem solving, namely we assume that the observation comes from a
stochastic degradation process that gradually degrades and noises the original
clean image. We learn to reverse the degradation process in order to recover
the clean image. Our technique maintains consistency with the original
measurement throughout the reverse process, and allows for great flexibility in
trading off perceptual quality for improved distortion metrics and sampling
speedup via early-stopping. We demonstrate the efficiency of our method on
different high-resolution datasets and inverse problems, achieving great
improvements over other state-of-the-art diffusion-based methods with respect
to both perceptual and distortion metrics. Source code and pre-trained models
will be released soon.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 24, 2023 </span>    
         <span class="authors"> Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, Matthias Nießner </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14207" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present DiffuScene for indoor 3D scene synthesis based on a novel scene
graph denoising diffusion probabilistic model, which generates 3D instance
properties stored in a fully-connected scene graph and then retrieves the most
similar object geometry for each graph node i.e. object instance which is
characterized as a concatenation of different attributes, including location,
size, orientation, semantic, and geometry features. Based on this scene graph,
we designed a diffusion model to determine the placements and types of 3D
instances. Our method can facilitate many downstream applications, including
scene completion, scene arrangement, and text-conditioned scene synthesis.
Experiments on the 3D-FRONT dataset show that our method can synthesize more
physically plausible and diverse indoor scenes than state-of-the-art methods.
Extensive ablation studies verify the effectiveness of our design choice in
scene diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 24, 2023 </span>    
         <span class="authors"> Yizhuo Lu, Changde Du, Dianpeng Wang, Huiguang He </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14139" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Reconstructing visual stimuli from measured functional magnetic resonance
imaging (fMRI) has been a meaningful and challenging task. Previous studies
have successfully achieved reconstructions with structures similar to the
original images, such as the outlines and size of some natural images. However,
these reconstructions lack explicit semantic information and are difficult to
discern. In recent years, many studies have utilized multi-modal pre-trained
models with stronger generative capabilities to reconstruct images that are
semantically similar to the original ones. However, these images have
uncontrollable structural information such as position and orientation. To
address both of the aforementioned issues simultaneously, we propose a
two-stage image reconstruction model called MindDiffuser, utilizing Stable
Diffusion. In Stage 1, the VQ-VAE latent representations and the CLIP text
embeddings decoded from fMRI are put into the image-to-image process of Stable
Diffusion, which yields a preliminary image that contains semantic and
structural information. In Stage 2, we utilize the low-level CLIP visual
features decoded from fMRI as supervisory information, and continually adjust
the two features in Stage 1 through backpropagation to align the structural
information. The results of both qualitative and quantitative analyses
demonstrate that our proposed model has surpassed the current state-of-the-art
models in terms of reconstruction results on Natural Scenes Dataset (NSD).
Furthermore, the results of ablation experiments indicate that each component
of our model is effective for image reconstruction.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CoLa-Diff: Conditional Latent Diffusion Model for Multi-Modal MRI Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 24, 2023 </span>    
         <span class="authors"> Lan Jiang, Ye Mao, Xi Chen, Xiangfeng Wang, Chao Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.14081" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, I.3.3; I.4.10
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 MRI synthesis promises to mitigate the challenge of missing MRI modality in
clinical practice. Diffusion model has emerged as an effective technique for
image synthesis by modelling complex and variable data distributions. However,
most diffusion-based MRI synthesis models are using a single modality. As they
operate in the original image domain, they are memory-intensive and less
feasible for multi-modal synthesis. Moreover, they often fail to preserve the
anatomical structure in MRI. Further, balancing the multiple conditions from
multi-modal MRI inputs is crucial for multi-modal synthesis. Here, we propose
the first diffusion-based multi-modality MRI synthesis model, namely
Conditioned Latent Diffusion Model (CoLa-Diff). To reduce memory consumption,
we design CoLa-Diff to operate in the latent space. We propose a novel network
architecture, e.g., similar cooperative filtering, to solve the possible
compression and noise in latent space. To better maintain the anatomical
structure, brain region masks are introduced as the priors of density
distributions to guide diffusion process. We further present auto-weight
adaptation to employ multi-modal information effectively. Our experiments
demonstrate that CoLa-Diff outperforms other state-of-the-art MRI synthesis
methods, promising to serve as an effective tool for multi-modal MRI synthesis.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DisC-Diff: Disentangled Conditional Diffusion Model for Multi-Contrast MRI Super-Resolution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 24, 2023 </span>    
         <span class="authors"> Ye Mao, Lan Jiang, Xi Chen, Chao Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13933" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Multi-contrast magnetic resonance imaging (MRI) is the most common management
tool used to characterize neurological disorders based on brain tissue
contrasts. However, acquiring high-resolution MRI scans is time-consuming and
infeasible under specific conditions. Hence, multi-contrast super-resolution
methods have been developed to improve the quality of low-resolution contrasts
by leveraging complementary information from multi-contrast MRI. Current deep
learning-based super-resolution methods have limitations in estimating
restoration uncertainty and avoiding mode collapse. Although the diffusion
model has emerged as a promising approach for image enhancement, capturing
complex interactions between multiple conditions introduced by multi-contrast
MRI super-resolution remains a challenge for clinical applications. In this
paper, we propose a disentangled conditional diffusion model, DisC-Diff, for
multi-contrast brain MRI super-resolution. It utilizes the sampling-based
generation and simple objective function of diffusion models to estimate
uncertainty in restorations effectively and ensure a stable optimization
process. Moreover, DisC-Diff leverages a disentangled multi-stream network to
fully exploit complementary information from multi-contrast MRI, improving
model interpretation under multiple conditions of multi-contrast inputs. We
validated the effectiveness of DisC-Diff on two datasets: the IXI dataset,
which contains 578 normal brains, and a clinical dataset with 316 pathological
brains. Our experimental results demonstrate that DisC-Diff outperforms other
state-of-the-art methods both quantitatively and visually.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Image-to-Video Generation with Latent Flow Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 24, 2023 </span>    
         <span class="authors"> Haomiao Ni, Changhao Shi, Kai Li, Sharon X. Huang, Martin Renqiang Min </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13744" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Conditional image-to-video (cI2V) generation aims to synthesize a new
plausible video starting from an image (e.g., a person's face) and a condition
(e.g., an action class label like smile). The key challenge of the cI2V task
lies in the simultaneous generation of realistic spatial appearance and
temporal dynamics corresponding to the given image and condition. In this
paper, we propose an approach for cI2V using novel latent flow diffusion models
(LFDM) that synthesize an optical flow sequence in the latent space based on
the given condition to warp the given image. Compared to previous
direct-synthesis-based works, our proposed LFDM can better synthesize spatial
details and temporal motion by fully utilizing the spatial content of the given
image and warping it in the latent space according to the generated
temporally-coherent flow. The training of LFDM consists of two separate stages:
(1) an unsupervised learning stage to train a latent flow auto-encoder for
spatial content generation, including a flow predictor to estimate latent flow
between pairs of video frames, and (2) a conditional learning stage to train a
3D-UNet-based diffusion model (DM) for temporal latent flow generation. Unlike
previous DMs operating in pixel space or latent feature space that couples
spatial and temporal information, the DM in our LFDM only needs to learn a
low-dimensional latent flow space for motion generation, thus being more
computationally efficient. We conduct comprehensive experiments on multiple
datasets, where LFDM consistently outperforms prior arts. Furthermore, we show
that LFDM can be easily adapted to new domains by simply finetuning the image
decoder. Our code is available at https://github.com/nihaomiao/CVPR23_LFDM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## End-to-End Diffusion Latent Optimization Improves Classifier Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2023 </span>    
         <span class="authors"> Bram Wallace, Akash Gokul, Stefano Ermon, Nikhil Naik </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13703" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Classifier guidance -- using the gradients of an image classifier to steer
the generations of a diffusion model -- has the potential to dramatically
expand the creative control over image generation and editing. However,
currently classifier guidance requires either training new noise-aware models
to obtain accurate gradients or using a one-step denoising approximation of the
final generation, which leads to misaligned gradients and sub-optimal control.
We highlight this approximation's shortcomings and propose a novel guidance
method: Direct Optimization of Diffusion Latents (DOODL), which enables
plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of
a pre-trained classifier on the true generated pixels, using an invertible
diffusion process to achieve memory-efficient backpropagation. Showcasing the
potential of more precise guidance, DOODL outperforms one-step classifier
guidance on computational and human evaluation metrics across different forms
of guidance: using CLIP guidance to improve generations of complex prompts from
DrawBench, using fine-grained visual classifiers to expand the vocabulary of
Stable Diffusion, enabling image-conditioned generation with a CLIP visual
encoder, and improving image aesthetics using an aesthetic scoring network.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Ablating Concepts in Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2023 </span>    
         <span class="authors"> Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, Jun-Yan Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13516" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large-scale text-to-image diffusion models can generate high-fidelity images
with powerful compositional ability. However, these models are typically
trained on an enormous amount of Internet data, often containing copyrighted
material, licensed images, and personal photos. Furthermore, they have been
found to replicate the style of various living artists or memorize exact
training samples. How can we remove such copyrighted concepts or images without
retraining the model from scratch? To achieve this goal, we propose an
efficient method of ablating concepts in the pretrained model, i.e., preventing
the generation of a target concept. Our algorithm learns to match the image
distribution for a target style, instance, or text prompt we wish to ablate to
the distribution corresponding to an anchor concept. This prevents the model
from generating target concepts given its text condition. Extensive experiments
show that our method can successfully prevent the generation of the ablated
concept while preserving closely related concepts in the model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2023 </span>    
         <span class="authors"> Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13439" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent text-to-video generation approaches rely on computationally heavy
training and require large-scale video datasets. In this paper, we introduce a
new task of zero-shot text-to-video generation and propose a low-cost approach
(without any training or optimization) by leveraging the power of existing
text-to-image synthesis methods (e.g., Stable Diffusion), making them suitable
for the video domain.
  Our key modifications include (i) enriching the latent codes of the generated
frames with motion dynamics to keep the global scene and the background time
consistent; and (ii) reprogramming frame-level self-attention using a new
cross-frame attention of each frame on the first frame, to preserve the
context, appearance, and identity of the foreground object.
  Experiments show that this leads to low overhead, yet high-quality and
remarkably consistent video generation. Moreover, our approach is not limited
to text-to-video synthesis but is also applicable to other tasks such as
conditional and content-specialized video generation, and Video
Instruct-Pix2Pix, i.e., instruction-guided video editing.
  As experiments show, our method performs comparably or sometimes better than
recent approaches, despite not being trained on additional video data. Our code
will be open sourced at: https://github.com/Picsart-AI-Research/Text2Video-Zero .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Medical diffusion on a budget: textual inversion for medical image generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2023 </span>    
         <span class="authors"> Bram de Wilde, Anindo Saha, Richard P. G. ten Broek, Henkjan Huisman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13430" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based models for text-to-image generation have gained immense
popularity due to recent advancements in efficiency, accessibility, and
quality. Although it is becoming increasingly feasible to perform inference
with these systems using consumer-grade GPUs, training them from scratch still
requires access to large datasets and significant computational resources. In
the case of medical image generation, the availability of large, publicly
accessible datasets that include text reports is limited due to legal and
ethical concerns. While training a diffusion model on a private dataset may
address this issue, it is not always feasible for institutions lacking the
necessary computational resources. This work demonstrates that pre-trained
Stable Diffusion models, originally trained on natural images, can be adapted
to various medical imaging modalities by training text embeddings with textual
inversion. In this study, we conducted experiments using medical datasets
comprising only 100 samples from three medical modalities. Embeddings were
trained in a matter of hours, while still retaining diagnostic relevance in
image generation. Experiments were designed to achieve several objectives.
Firstly, we fine-tuned the training and inference processes of textual
inversion, revealing that larger embeddings and more examples are required.
Secondly, we validated our approach by demonstrating a 2\% increase in the
diagnostic accuracy (AUC) for detecting prostate cancer on MRI, which is a
challenging multi-modal imaging modality, from 0.78 to 0.80. Thirdly, we
performed simulations by interpolating between healthy and diseased states,
combining multiple pathologies, and inpainting to show embedding flexibility
and control of disease appearance. Finally, the embeddings trained in this
study are small (less than 1 MB), which facilitates easy sharing of medical
data with reduced privacy concerns.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2023 </span>    
         <span class="authors"> Ce Zheng, Guo-Jun Qi, Chen Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13397" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.HC, cs.MM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Human mesh recovery (HMR) provides rich human body information for various
real-world applications such as gaming, human-computer interaction, and virtual
reality. Compared to single image-based methods, video-based methods can
utilize temporal information to further improve performance by incorporating
human body motion priors. However, many-to-many approaches such as VIBE suffer
from motion smoothness and temporal inconsistency. While many-to-one approaches
such as TCMR and MPS-Net rely on the future frames, which is non-causal and
time inefficient during inference. To address these challenges, a novel
Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is
presented. DDT is designed to decode specific motion patterns from the input
sequence, enhancing motion smoothness and temporal consistency. As a
many-to-many approach, the decoder of our DDT outputs the human mesh of all the
frames, making DDT more viable for real-world applications where time
efficiency is crucial and a causal model is desired. Extensive experiments are
conducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),
which demonstrated the effectiveness and efficiency of our DDT.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2023 </span>    
         <span class="authors"> Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, In So Kweon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13336" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.AI, cs.LG, cs.MM, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative AI has demonstrated impressive performance in various fields,
among which speech synthesis is an interesting direction. With the diffusion
model as the most popular generative model, numerous works have attempted two
active tasks: text to speech and speech enhancement. This work conducts a
survey on audio diffusion model, which is complementary to existing surveys
that either lack the recent progress of diffusion-based speech synthesis or
highlight an overall picture of applying diffusion model in multiple fields.
Specifically, this work first briefly introduces the background of audio and
diffusion model. As for the text-to-speech task, we divide the methods into
three categories based on the stage where diffusion model is adopted: acoustic
model, vocoder and end-to-end framework. Moreover, we categorize various speech
enhancement tasks by either certain signals are removed or added into the input
speech. Comparisons of experimental results and discussions are also covered in
this survey.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2023 </span>    
         <span class="authors"> Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wenjing Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13126" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The advent of open-source AI communities has produced a cornucopia of
powerful text-guided diffusion models that are trained on various datasets.
While few explorations have been conducted on ensembling such models to combine
their strengths. In this work, we propose a simple yet effective method called
Saliency-aware Noise Blending (SNB) that can empower the fused text-guided
diffusion models to achieve more controllable generation. Specifically, we
experimentally find that the responses of classifier-free guidance are highly
related to the saliency of generated images. Thus we propose to trust different
models in their areas of expertise by blending the predicted noises of two
diffusion models in a saliency-aware manner. SNB is training-free and can be
completed within a DDIM sampling process. Additionally, it can automatically
align the semantics of two noise spaces without requiring additional
annotations such as masks. Extensive experiments show the impressive
effectiveness of SNB in various applications. Project page is available at
https://magicfusion.github.io/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Enhancing Unsupervised Speech Recognition with Diffusion GANs
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2023 </span>    
         <span class="authors"> Xianchao Wu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.13559" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We enhance the vanilla adversarial training method for unsupervised Automatic
Speech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance
noises of various intensities to the generator's output and unlabeled reference
text which are sampled from pretrained phoneme language models with a length
constraint, (2) asks diffusion timestep-dependent discriminators to separate
them, and (3) back-propagates the gradients to update the generator.
Word/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for
test-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our
enhancement strategies work effectively.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam Computed Tomography Reconstruction with Incomplete Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 22, 2023 </span>    
         <span class="authors"> Wenjun Xia, Chuang Niu, Wenxiang Cong, Ge Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12861" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.LG, eess.SP, physics.bio-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep learning (DL) has emerged as a new approach in the field of computed
tomography (CT) with many applicaitons. A primary example is CT reconstruction
from incomplete data, such as sparse-view image reconstruction. However,
applying DL to sparse-view cone-beam CT (CBCT) remains challenging. Many models
learn the mapping from sparse-view CT images to the ground truth but often fail
to achieve satisfactory performance. Incorporating sinogram data and performing
dual-domain reconstruction improve image quality with artifact suppression, but
a straightforward 3D implementation requires storing an entire 3D sinogram in
memory and many parameters of dual-domain networks. This remains a major
challenge, limiting further research, development and applications. In this
paper, we propose a sub-volume-based 3D denoising diffusion probabilistic model
(DDPM) for CBCT image reconstruction from down-sampled data. Our DDPM network,
trained on data cubes extracted from paired fully sampled sinograms and
down-sampled sinograms, is employed to inpaint down-sampled sinograms. Our
method divides the entire sinogram into overlapping cubes and processes them in
parallel on multiple GPUs, successfully overcoming the memory limitation.
Experimental results demonstrate that our approach effectively suppresses
few-view artifacts while preserving textural details faithfully.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffuse-Denoise-Count: Accurate Crowd-Counting with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 22, 2023 </span>    
         <span class="authors"> Yasiru Ranasinghe, Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12790" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Crowd counting is a key aspect of crowd analysis and has been typically
accomplished by estimating a crowd-density map and summing over the density
values. However, this approach suffers from background noise accumulation and
loss of density due to the use of broad Gaussian kernels to create the ground
truth density maps. This issue can be overcome by narrowing the Gaussian
kernel. However, existing approaches perform poorly when trained with such
ground truth density maps. To overcome this limitation, we propose using
conditional diffusion models to predict density maps, as diffusion models are
known to model complex distributions well and show high fidelity to training
data during crowd-density map generation. Furthermore, as the intermediate time
steps of the diffusion process are noisy, we incorporate a regression branch
for direct crowd estimation only during training to improve the feature
learning. In addition, owing to the stochastic nature of the diffusion model,
we introduce producing multiple density maps to improve the counting
performance contrary to the existing crowd counting pipelines. Further, we also
differ from the density summation and introduce contour detection followed by
summation as the counting operation, which is more immune to background noise.
We conduct extensive experiments on public datasets to validate the
effectiveness of our method. Specifically, our novel crowd-counting pipeline
improves the error of crowd-counting by up to $6\%$ on JHU-CROWD++ and up to
$7\%$ on UCF-QNRF.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Pix2Video: Video Editing using Image Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 22, 2023 </span>    
         <span class="authors"> Duygu Ceylan, Chun-Hao Paul Huang, Niloy J. Mitra </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12688" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image diffusion models, trained on massive image collections, have emerged as
the most versatile image generator model in terms of quality and diversity.
They support inverting real images and conditional (e.g., text) generation,
making them attractive for high-quality image editing applications. We
investigate how to use such pre-trained image models for text-guided video
editing. The critical challenge is to achieve the target edits while still
preserving the content of the source video. Our method works in two simple
steps: first, we use a pre-trained structure-guided (e.g., depth) image
diffusion model to perform text-guided edits on an anchor frame; then, in the
key step, we progressively propagate the changes to the future frames via
self-attention feature injection to adapt the core denoising step of the
diffusion model. We then consolidate the changes by adjusting the latent code
for the frame before continuing the process. Our approach is training-free and
generalizes to a wide range of edits. We demonstrate the effectiveness of the
approach by extensive experimentation and compare it against four different
prior and parallel efforts (on ArXiv). We demonstrate that realistic
text-guided video edits are possible, without any compute-intensive
preprocessing or video-specific finetuning.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## EDGI: Equivariant Diffusion for Planning with Embodied Agents
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 22, 2023 </span>    
         <span class="authors"> Johann Brehmer, Joey Bose, Pim de Haan, Taco Cohen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12410" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.RO, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Embodied agents operate in a structured world, often solving tasks with
spatial, temporal, and permutation symmetries. Most algorithms for planning and
model-based reinforcement learning (MBRL) do not take this rich geometric
structure into account, leading to sample inefficiency and poor generalization.
We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an
algorithm for MBRL and planning that is equivariant with respect to the product
of the spatial symmetry group $\mathrm{SE(3)}$, the discrete-time translation
group $\mathbb{Z}$, and the object permutation group $\mathrm{S}_n$. EDGI
follows the Diffuser framework (Janner et al. 2022) in treating both learning a
world model and planning in it as a conditional generative modeling problem,
training a diffusion model on an offline trajectory dataset. We introduce a new
$\mathrm{SE(3)} \times \mathbb{Z} \times \mathrm{S}_n$-equivariant diffusion
model that supports multiple representations. We integrate this model in a
planning loop, where conditioning and classifier-based guidance allow us to
softly break the symmetry for specific tasks as needed. On navigation and
object manipulation tasks, EDGI improves sample efficiency and generalization.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 22, 2023 </span>    
         <span class="authors"> Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, Nan Duan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12346" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion
architecture for eXtremely Long video generation. Most current work generates
long videos segment by segment sequentially, which normally leads to the gap
between training on short videos and inferring long videos, and the sequential
generation is inefficient. Instead, our approach adopts a ``coarse-to-fine''
process, in which the video can be generated in parallel at the same
granularity. A global diffusion model is applied to generate the keyframes
across the entire time range, and then local diffusion models recursively fill
in the content between nearby frames. This simple yet effective strategy allows
us to directly train on long videos (3376 frames) to reduce the
training-inference gap, and makes it possible to generate all segments in
parallel. To evaluate our model, we build FlintstonesHD dataset, a new
benchmark for long video generation. Experiments show that our model not only
generates high-quality long videos with both global and local coherence, but
also decreases the average inference time from 7.55min to 26s (by 94.26\%) at
the same hardware setting when generating 1024 frames. The homepage link is
\url{https://msra-nuwa.azurewebsites.net/}
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 22, 2023 </span>    
         <span class="authors"> Koutilya Pnvr, Bharat Singh, Pallabi Ghosh, Behjat Siddiquie, David Jacobs </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12343" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a technique for segmenting real and AI-generated images using
latent diffusion models (LDMs) trained on internet-scale datasets. First, we
show that the latent space of LDMs (z-space) is a better input representation
compared to other feature representations like RGB images or CLIP encodings for
text-based image segmentation. By training the segmentation models on the
latent z-space, which creates a compressed representation across several
domains like different forms of art, cartoons, illustrations, and photographs,
we are also able to bridge the domain gap between real and AI-generated images.
We show that the internal features of LDMs contain rich semantic information
and present a technique in the form of LD-ZNet to further boost the performance
of text-based segmentation. Overall, we show up to 6% improvement over standard
baselines for text-to-image segmentation on natural images. For AI-generated
imagery, we show close to 20% improvement compared to state-of-the-art
techniques.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 22, 2023 </span>    
         <span class="authors"> Haipeng Zhou, Lei Zhu, Yuyin Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12313" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The Diffusion Probabilistic Model (DPM) has emerged as a highly effective
generative model in the field of computer vision. Its intermediate latent
vectors offer rich semantic information, making it an attractive option for
various downstream tasks such as segmentation and detection. In order to
explore its potential further, we have taken a step forward and considered a
more complex scenario in the medical image domain, specifically, under an
unsupervised adaptation condition. To this end, we propose a Diffusion-based
and Prototype-guided network (DP-Net) for unsupervised domain adaptive
segmentation. Concretely, our DP-Net consists of two stages: 1) Distribution
Aligned Diffusion (DADiff), which involves training a domain discriminator to
minimize the difference between the intermediate features generated by the DPM,
thereby aligning the inter-domain distribution; and 2) Prototype-guided
Consistency Learning (PCL), which utilizes feature centroids as prototypes and
applies a prototype-guided loss to ensure that the segmentor learns consistent
content from both source and target domains. Our approach is evaluated on
fundus datasets through a series of experiments, which demonstrate that the
performance of the proposed method is reliable and outperforms state-of-the-art
methods. Our work presents a promising direction for using DPM in complex
medical image scenarios, opening up new possibilities for further research in
medical imaging.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Compositional 3D Scene Generation using Locally Conditioned Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 21, 2023 </span>    
         <span class="authors"> Ryan Po, Gordon Wetzstein </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12218" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Designing complex 3D scenes has been a tedious, manual process requiring
domain expertise. Emerging text-to-3D generative models show great promise for
making this task more intuitive, but existing approaches are limited to
object-level generation. We introduce \textbf{locally conditioned diffusion} as
an approach to compositional scene diffusion, providing control over semantic
parts using text prompts and bounding boxes while ensuring seamless transitions
between these parts. We demonstrate a score distillation sampling--based
text-to-3D synthesis pipeline that enables compositional 3D scene generation at
a higher fidelity than relevant baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Affordance Diffusion: Synthesizing Hand-Object Interactions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 21, 2023 </span>    
         <span class="authors"> Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birchfield, Jiaming Song, Shubham Tulsiani, Sifei Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12538" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.RO
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent successes in image synthesis are powered by large-scale diffusion
models. However, most methods are currently limited to either text- or
image-conditioned generation for synthesizing an entire image, texture transfer
or inserting objects into a user-specified region. In contrast, in this work we
focus on synthesizing complex interactions (ie, an articulated hand) with a
given object. Given an RGB image of an object, we aim to hallucinate plausible
images of a human hand interacting with it. We propose a two-step generative
approach: a LayoutNet that samples an articulation-agnostic
hand-object-interaction layout, and a ContentNet that synthesizes images of a
hand grasping the object given the predicted layout. Both are built on top of a
large-scale pretrained diffusion model to make use of its latent
representation. Compared to baselines, the proposed method is shown to
generalize better to novel objects and perform surprisingly well on
out-of-distribution in-the-wild scenes of portable-sized objects. The resulting
system allows us to predict descriptive affordance information, such as hand
articulation and approaching orientation. Project page:
https://judyye.github.io/affordiffusion-www
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 21, 2023 </span>    
         <span class="authors"> Matthias Keicher, Matan Atad, David Schinz, Alexandra S. Gersing, Sarah C. Foreman, Sophia S. Goller, Juergen Weissinger, Jon Rischewski, Anna-Sophia Dietrich, Benedikt Wiestler, Jan S. Kirschke, Nassir Navab </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.12031" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Vertebral fractures are a consequence of osteoporosis, with significant
health implications for affected patients. Unfortunately, grading their
severity using CT exams is hard and subjective, motivating automated grading
methods. However, current approaches are hindered by imbalance and scarcity of
data and a lack of interpretability. To address these challenges, this paper
proposes a novel approach that leverages unlabelled data to train a generative
Diffusion Autoencoder (DAE) model as an unsupervised feature extractor. We
model fracture grading as a continuous regression, which is more reflective of
the smooth progression of fractures. Specifically, we use a binary, supervised
fracture classifier to construct a hyperplane in the DAE's latent space. We
then regress the severity of the fracture as a function of the distance to this
hyperplane, calibrating the results to the Genant scale. Importantly, the
generative nature of our method allows us to visualize different grades of a
given vertebra, providing interpretability and insight into the features that
contribute to automated grading.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 21, 2023 </span>    
         <span class="authors"> Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11916" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.IR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper proposes a novel diffusion-based model, CompoDiff, for solving
Composed Image Retrieval (CIR) with latent diffusion and presents a newly
created dataset of 18 million reference images, conditions, and corresponding
target image triplets to train the model. CompoDiff not only achieves a new
zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also
enables a more versatile CIR by accepting various conditions, such as negative
text and image mask conditions, which are unavailable with existing CIR
methods. In addition, the CompoDiff features are on the intact CLIP embedding
space so that they can be directly used for all existing models exploiting the
CLIP space. The code and dataset used for the training, and the pre-trained
weights are available at https://github.com/navervision/CompoDiff
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 21, 2023 </span>    
         <span class="authors"> Junyi Zhang, Jiaqi Guo, Shizhao Sun, Jian-Guang Lou, Dongmei Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11589" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Creating graphic layouts is a fundamental step in graphic designs. In this
work, we present a novel generative model named LayoutDiffusion for automatic
layout generation. As layout is typically represented as a sequence of discrete
tokens, LayoutDiffusion models layout generation as a discrete denoising
diffusion process. It learns to reverse a mild forward process, in which
layouts become increasingly chaotic with the growth of forward steps and
layouts in the neighboring steps do not differ too much. Designing such a mild
forward process is however very challenging as layout has both categorical
attributes and ordinal attributes. To tackle the challenge, we summarize three
critical factors for achieving a mild forward process for the layout, i.e.,
legality, coordinate proximity and type disruption. Based on the factors, we
propose a block-wise transition matrix coupled with a piece-wise linear noise
schedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion
outperforms state-of-the-art approaches significantly. Moreover, it enables two
conditional layout generation tasks in a plug-and-play manner without
re-training and achieves better performance than existing methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 21, 2023 </span>    
         <span class="authors"> Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, Wen Gao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11579" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, a novel Diffusion-based 3D Pose estimation (D3DP) method with
Joint-wise reProjection-based Multi-hypothesis Aggregation (JPMA) is proposed
for probabilistic 3D human pose estimation. On the one hand, D3DP generates
multiple possible 3D pose hypotheses for a single 2D observation. It gradually
diffuses the ground truth 3D poses to a random distribution, and learns a
denoiser conditioned on 2D keypoints to recover the uncontaminated 3D poses.
The proposed D3DP is compatible with existing 3D pose estimators and supports
users to balance efficiency and accuracy during inference through two
customizable parameters. On the other hand, JPMA is proposed to assemble
multiple hypotheses generated by D3DP into a single 3D pose for practical use.
It reprojects 3D pose hypotheses to the 2D camera plane, selects the best
hypothesis joint-by-joint based on the reprojection errors, and combines the
selected joints into the final pose. The proposed JPMA conducts aggregation at
the joint level and makes use of the 2D prior information, both of which have
been overlooked by previous approaches. Extensive experiments on Human3.6M and
MPI-INF-3DHP datasets show that our method outperforms the state-of-the-art
deterministic and probabilistic approaches by 1.5% and 8.9%, respectively. Code
is available at https://github.com/paTRICK-swk/D3DP.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Aman Shrivastava, P. Thomas Fletcher </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11477" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, q-bio.QM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In recent years, computational pathology has seen tremendous progress driven
by deep learning methods in segmentation and classification tasks aiding
prognostic and diagnostic settings. Nuclei segmentation, for instance, is an
important task for diagnosing different cancers. However, training deep
learning models for nuclei segmentation requires large amounts of annotated
data, which is expensive to collect and label. This necessitates explorations
into generative modeling of histopathological images. In this work, we use
recent advances in conditional diffusion modeling to formulate a
first-of-its-kind nuclei-aware semantic tissue generation framework (NASDM)
which can synthesize realistic tissue samples given a semantic instance mask of
up to six different nuclei types, enabling pixel-perfect nuclei localization in
generated samples. These synthetic images are useful in applications in
pathology pedagogy, validation of models, and supplementation of existing
nuclei segmentation datasets. We demonstrate that NASDM is able to synthesize
high-quality histopathology images of the colon with superior quality and
semantic controllability over existing generative methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Mauricio Delbracio, Peyman Milanfar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11435" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Inversion by Direct Iteration (InDI) is a new formulation for supervised
image restoration that avoids the so-called ``regression to the mean'' effect
and produces more realistic and detailed images than existing regression-based
methods. It does this by gradually improving image quality in small steps,
similar to generative denoising diffusion models.
  Image restoration is an ill-posed problem where multiple high-quality images
are plausible reconstructions of a given low-quality input. Therefore, the
outcome of a single step regression model is typically an aggregate of all
possible explanations, therefore lacking details and realism. The main
advantage of InDI is that it does not try to predict the clean target image in
a single step but instead gradually improves the image in small steps,
resulting in better perceptual quality.
  While generative denoising diffusion models also work in small steps, our
formulation is distinct in that it does not require knowledge of any analytic
form of the degradation process. Instead, we directly learn an iterative
restoration process from low-quality and high-quality paired examples. InDI can
be applied to virtually any image degradation, given paired training data. In
conditional denoising diffusion image restoration the denoising network
generates the restored image by repeatedly denoising an initial image of pure
noise, conditioned on the degraded input. Contrary to conditional denoising
formulations, InDI directly proceeds by iteratively restoring the input
low-quality image, producing high-quality results on a variety of image
restoration tasks, including motion and out-of-focus deblurring,
super-resolution, compression artifact removal, and denoising.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Text2Tex: Text-driven Texture Synthesis via Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, Matthias Nießner </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11396" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Text2Tex, a novel method for generating high-quality textures for
3D meshes from the given text prompts. Our method incorporates inpainting into
a pre-trained depth-aware image diffusion model to progressively synthesize
high resolution partial textures from multiple viewpoints. To avoid
accumulating inconsistent and stretched artifacts across views, we dynamically
segment the rendered view into a generation mask, which represents the
generation status of each visible texel. This partitioned view representation
guides the depth-aware inpainting model to generate and update partial textures
for the corresponding regions. Furthermore, we propose an automatic view
sequence generation scheme to determine the next best view for updating the
partial texture. Extensive experiments demonstrate that our method
significantly outperforms the existing text-driven approaches and GAN-based
methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SVDiff: Compact Parameter Space for Diffusion Fine-Tuning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, Feng Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11305" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have achieved remarkable success in text-to-image
generation, enabling the creation of high-quality images from text prompts or
other modalities. However, existing methods for customizing these models are
limited by handling multiple personalized subjects and the risk of overfitting.
Moreover, their large number of parameters is inefficient for model storage. In
this paper, we propose a novel approach to address these limitations in
existing text-to-image diffusion models for personalization. Our method
involves fine-tuning the singular values of the weight matrices, leading to a
compact and efficient parameter space that reduces the risk of overfitting and
language-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique
to enhance the quality of multi-subject image generation and a simple
text-based image editing framework. Our proposed SVDiff method has a
significantly smaller model size (1.7MB for StableDiffusion) compared to
existing methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it
more practical for real-world applications.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Cascaded Latent Diffusion Models for High-Resolution Chest X-ray Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11224" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While recent advances in large-scale foundational models show promising
results, their application to the medical domain has not yet been explored in
detail. In this paper, we progress into the realms of large-scale modeling in
medical synthesis by proposing Cheff - a foundational cascaded latent diffusion
model, which generates highly-realistic chest radiographs providing
state-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,
which is a unified interface for public chest datasets and forms the largest
open collection of chest X-rays up to date. With Cheff conditioned on
radiological reports, we further guide the synthesis process over text prompts
and unveil the research area of report-to-chest-X-ray generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## AnimeDiffusion: Anime Face Line Drawing Colorization via Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Yu Cao, Xiangqiao Meng, P. Y. Mok, Xueting Liu, Tong-Yee Lee, Ping Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11137" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 It is a time-consuming and tedious work for manually colorizing anime line
drawing images, which is an essential stage in cartoon animation creation
pipeline. Reference-based line drawing colorization is a challenging task that
relies on the precise cross-domain long-range dependency modelling between the
line drawing and reference image. Existing learning methods still utilize
generative adversarial networks (GANs) as one key module of their model
architecture. In this paper, we propose a novel method called AnimeDiffusion
using diffusion models that performs anime face line drawing colorization
automatically. To the best of our knowledge, this is the first diffusion model
tailored for anime content creation. In order to solve the huge training
consumption problem of diffusion models, we design a hybrid training strategy,
first pre-training a diffusion model with classifier-free guidance and then
fine-tuning it with image reconstruction guidance. We find that with a few
iterations of fine-tuning, the model shows wonderful colorization performance,
as illustrated in Fig. 1. For training AnimeDiffusion, we conduct an anime face
line drawing colorization benchmark dataset, which contains 31696 training data
and 579 testing data. We hope this dataset can fill the gap of no available
high resolution anime face dataset for colorization method evaluation. Through
multiple quantitative metrics evaluated on our dataset and a user study, we
demonstrate AnimeDiffusion outperforms state-of-the-art GANs-based models for
anime face line drawing colorization. We also collaborate with professional
artists to test and apply our AnimeDiffusion for their creation work. We
release our code on https://github.com/xq-meng/AnimeDiffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Positional Diffusion: Ordering Unordered Sets with Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Francesco Giuliari, Gianluca Scarpellini, Stuart James, Yiming Wang, Alessio Del Bue </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.11120" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Positional reasoning is the process of ordering unsorted parts contained in a
set into a consistent structure. We present Positional Diffusion, a
plug-and-play graph formulation with Diffusion Probabilistic Models to address
positional reasoning. We use the forward process to map elements' positions in
a set to random positions in a continuous space. Positional Diffusion learns to
reverse the noising process and recover the original positions through an
Attention-based Graph Neural Network. We conduct extensive experiments with
benchmark datasets including two puzzle datasets, three sentence ordering
datasets, and one visual storytelling dataset, demonstrating that our method
outperforms long-lasting research on puzzle solving with up to +18% compared to
the second-best deep learning method, and performs on par against the
state-of-the-art methods on sentence ordering and visual storytelling. Our work
highlights the suitability of diffusion models for ordering problems and
proposes a novel formulation and method for solving various ordering tasks.
Project website at https://iit-pavis.github.io/Positional_Diffusion/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Leapfrog Diffusion Model for Stochastic Trajectory Prediction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Weibo Mao, Chenxin Xu, Qi Zhu, Siheng Chen, Yanfeng Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.10895" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 To model the indeterminacy of human behaviors, stochastic trajectory
prediction requires a sophisticated multi-modal distribution of future
trajectories. Emerging diffusion models have revealed their tremendous
representation capacities in numerous generation tasks, showing potential for
stochastic trajectory prediction. However, expensive time consumption prevents
diffusion models from real-time prediction, since a large number of denoising
steps are required to assure sufficient representation ability. To resolve the
dilemma, we present LEapfrog Diffusion model (LED), a novel diffusion-based
trajectory prediction model, which provides real-time, precise, and diverse
predictions. The core of the proposed LED is to leverage a trainable leapfrog
initializer to directly learn an expressive multi-modal distribution of future
trajectories, which skips a large number of denoising steps, significantly
accelerating inference speed. Moreover, the leapfrog initializer is trained to
appropriately allocate correlated samples to provide a diversity of predicted
future trajectories, significantly improving prediction performances. Extensive
experiments on four real-world datasets, including NBA/NFL/SDD/ETH-UCY, show
that LED consistently improves performance and achieves 23.7%/21.9% ADE/FDE
improvement on NFL. The proposed LED also speeds up the inference
19.3/30.8/24.3/25.1 times compared to the standard diffusion model on
NBA/NFL/SDD/ETH-UCY, satisfying real-time inference needs. Code is available at
https://github.com/MediaBrain-SJTU/LED.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Object-Centric Slot Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 20, 2023 </span>    
         <span class="authors"> Jindong Jiang, Fei Deng, Gautam Singh, Sungjin Ahn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.10834" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite remarkable recent advances, making object-centric learning work for
complex natural scenes remains the main challenge. The recent success of
adopting the transformer-based image generative model in object-centric
learning suggests that having a highly expressive image generator is crucial
for dealing with complex scenes. In this paper, inspired by this observation,
we aim to answer the following question: can we benefit from the other pillar
of modern deep generative models, i.e., the diffusion models, for
object-centric learning and what are the pros and cons of such a model? To this
end, we propose a new object-centric learning model, Latent Slot Diffusion
(LSD). LSD can be seen from two perspectives. From the perspective of
object-centric learning, it replaces the conventional slot decoders with a
latent diffusion model conditioned on the object slots. Conversely, from the
perspective of diffusion models, it is the first unsupervised compositional
conditional diffusion model which, unlike traditional diffusion models, does
not require supervised annotation such as a text description to learn to
compose. In experiments on various object-centric tasks, including the FFHQ
dataset for the first time in this line of research, we demonstrate that LSD
significantly outperforms the state-of-the-art transformer-based decoder,
particularly when the scene is more complex. We also show a superior quality in
unsupervised compositional generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 19, 2023 </span>    
         <span class="authors"> Yijun Yang, Huazhu Fu, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Lei Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.10610" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Probabilistic Models have recently shown remarkable performance in
generative image modeling, attracting significant attention in the computer
vision community. However, while a substantial amount of diffusion-based
research has focused on generative tasks, few studies have applied diffusion
models to general medical image classification. In this paper, we propose the
first diffusion-based model (named DiffMIC) to address general medical image
classification by eliminating unexpected noise and perturbations in medical
images and robustly capturing semantic representation. To achieve this goal, we
devise a dual conditional guidance strategy that conditions each diffusion step
with multiple granularities to improve step-wise regional attention.
Furthermore, we propose learning the mutual information in each granularity by
enforcing Maximum-Mean Discrepancy regularization during the diffusion forward
process. We evaluate the effectiveness of our DiffMIC on three medical
classification tasks with different image modalities, including placental
maturity grading on ultrasound images, skin lesion classification using
dermatoscopic images, and diabetic retinopathy grading using fundus images. Our
experimental results demonstrate that DiffMIC outperforms state-of-the-art
methods by a significant margin, indicating the universality and effectiveness
of the proposed model. Our code will be publicly available at
https://github.com/scott-yjyang/DiffMIC.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diff-UNet: A Diffusion Embedded Network for Volumetric Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 18, 2023 </span>    
         <span class="authors"> Zhaohu Xing, Liang Wan, Huazhu Fu, Guang Yang, Lei Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.10326" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In recent years, Denoising Diffusion Models have demonstrated remarkable
success in generating semantically valuable pixel-wise representations for
image generative modeling. In this study, we propose a novel end-to-end
framework, called Diff-UNet, for medical volumetric segmentation. Our approach
integrates the diffusion model into a standard U-shaped architecture to extract
semantic information from the input volume effectively, resulting in excellent
pixel-level representations for medical volumetric segmentation. To enhance the
robustness of the diffusion model's prediction results, we also introduce a
Step-Uncertainty based Fusion (SUF) module during inference to combine the
outputs of the diffusion models at each step. We evaluate our method on three
datasets, including multimodal brain tumors in MRI, liver tumors, and
multi-organ CT volumes, and demonstrate that Diff-UNet outperforms other
state-of-the-art methods significantly. Our experimental results also indicate
the universality and effectiveness of the proposed model. The proposed
framework has the potential to facilitate the accurate diagnosis and treatment
of medical conditions by enabling more precise segmentation of anatomical
structures. The codes of Diff-UNet are available at
https://github.com/ge-xing/Diff-UNet
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Recipe for Watermarking Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 17, 2023 </span>    
         <span class="authors"> Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, Min Lin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.10137" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models (DMs) have demonstrated their advantageous
potential for generative tasks. Widespread interest exists in incorporating DMs
into downstream applications, such as producing or editing photorealistic
images. However, practical deployment and unprecedented power of DMs raise
legal issues, including copyright protection and monitoring of generated
content. In this regard, watermarking has been a proven solution for copyright
protection and content monitoring, but it is underexplored in the DMs
literature. Specifically, DMs generate samples from longer tracks and may have
newly designed multimodal structures, necessitating the modification of
conventional watermarking pipelines. To this end, we conduct comprehensive
analyses and derive a recipe for efficiently watermarking state-of-the-art DMs
(e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe
is straightforward but involves empirically ablated implementation details,
providing a solid foundation for future research on watermarking DMs. Our Code:
https://github.com/yunqing-me/WatermarkDM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionRet: Generative Text-Video Retrieval with Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 17, 2023 </span>    
         <span class="authors"> Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji, Chang Liu, Li Yuan, Jie Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09867" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Existing text-video retrieval solutions are, in essence, discriminant models
focused on maximizing the conditional likelihood, i.e., p(candidates|query).
While straightforward, this de facto paradigm overlooks the underlying data
distribution p(query), which makes it challenging to identify
out-of-distribution data. To address this limitation, we creatively tackle this
task from a generative viewpoint and model the correlation between the text and
the video as their joint probability p(candidates,query). This is accomplished
through a diffusion-based text-video retrieval framework (DiffusionRet), which
models the retrieval task as a process of gradually generating joint
distribution from noise. During training, DiffusionRet is optimized from both
the generation and discrimination perspectives, with the generator being
optimized by generation loss and the feature extractor trained with contrastive
loss. In this way, DiffusionRet cleverly leverages the strengths of both
generative and discriminative methods. Extensive experiments on five commonly
used text-video retrieval benchmarks, including MSRVTT, LSMDC, MSVD,
ActivityNet Captions, and DiDeMo, with superior performances, justify the
efficacy of our method. More encouragingly, without any modification,
DiffusionRet even performs well in out-domain retrieval settings. We believe
this work brings fundamental insights into the related fields. Code will be
available at https://github.com/jpthu17/DiffusionRet.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 17, 2023 </span>    
         <span class="authors"> Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, Jian Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09833" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, conditional diffusion models have gained popularity in numerous
applications due to their exceptional generation ability. However, many
existing methods are training-required. They need to train a time-dependent
classifier or a condition-dependent score estimator, which increases the cost
of constructing conditional diffusion models and is inconvenient to transfer
across different conditions. Some current works aim to overcome this limitation
by proposing training-free solutions, but most can only be applied to a
specific category of tasks and not to more general conditions. In this work, we
propose a training-Free conditional Diffusion Model (FreeDoM) used for various
conditions. Specifically, we leverage off-the-shelf pre-trained networks, such
as a face detection model, to construct time-independent energy functions,
which guide the generation process without requiring training. Furthermore,
because the construction of the energy function is very flexible and adaptable
to various conditions, our proposed FreeDoM has a broader range of applications
than existing training-free methods. FreeDoM is advantageous in its simplicity,
effectiveness, and low cost. Experiments demonstrate that FreeDoM is effective
for various conditions and suitable for diffusion models of diverse data
domains, including image and latent code domains.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 17, 2023 </span>    
         <span class="authors"> Chaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Jinxiang Liu, Yu Wang, Ya Zhang, Yanfeng Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09813" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Learning from a large corpus of data, pre-trained models have achieved
impressive progress nowadays. As popular generative pre-training, diffusion
models capture both low-level visual knowledge and high-level semantic
relations. In this paper, we propose to exploit such knowledgeable diffusion
models for mainstream discriminative tasks, i.e., unsupervised object
discovery: saliency segmentation and object localization. However, the
challenges exist as there is one structural difference between generative and
discriminative models, which limits the direct use. Besides, the lack of
explicitly labeled data significantly limits performance in unsupervised
settings. To tackle these issues, we introduce DiffusionSeg, one novel
synthesis-exploitation framework containing two-stage strategies. To alleviate
data insufficiency, we synthesize abundant images, and propose a novel
training-free AttentionCut to obtain masks in the first synthesis stage. In the
second exploitation stage, to bridge the structural gap, we use the inversion
technique, to map the given image back to diffusion features. These features
can be directly used by downstream architectures. Extensive experiments and
ablation studies demonstrate the superiority of adapting diffusion for
unsupervised object discovery.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Autoencoders are Unified Self-supervised Learners
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 17, 2023 </span>    
         <span class="authors"> Weilai Xiang, Hongyu Yang, Di Huang, Yunhong Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09769" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Inspired by recent advances in diffusion models, which are reminiscent of
denoising autoencoders, we investigate whether they can acquire discriminative
representations for classification via generative pre-training. This paper
shows that the networks in diffusion models, namely denoising diffusion
autoencoders (DDAE), are unified self-supervised learners: by pre-training on
unconditional image generation, DDAE has already learned strongly
linear-separable representations at its intermediate layers without auxiliary
encoders, thus making diffusion pre-training emerge as a general approach for
self-supervised generative and discriminative learning. To verify this, we
perform linear probe and fine-tuning evaluations on multi-class datasets. Our
diffusion-based approach achieves 95.9% and 50.0% linear probe accuracies on
CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to masked
autoencoders and contrastive learning for the first time. Additionally,
transfer learning from ImageNet confirms DDAE's suitability for latent-space
Vision Transformers, suggesting the potential for scaling DDAEs as unified
foundation models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusing the Optimal Topology: A Generative Optimization Approach
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 17, 2023 </span>    
         <span class="authors"> Giorgio Giannone, Faez Ahmed </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09760" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CE
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Topology Optimization seeks to find the best design that satisfies a set of
constraints while maximizing system performance. Traditional iterative
optimization methods like SIMP can be computationally expensive and get stuck
in local minima, limiting their applicability to complex or large-scale
problems. Learning-based approaches have been developed to accelerate the
topology optimization process, but these methods can generate designs with
floating material and low performance when challenged with out-of-distribution
constraint configurations. Recently, deep generative models, such as Generative
Adversarial Networks and Diffusion Models, conditioned on constraints and
physics fields have shown promise, but they require extensive pre-processing
and surrogate models for improving performance. To address these issues, we
propose a Generative Optimization method that integrates classic optimization
like SIMP as a refining mechanism for the topology generated by a deep
generative model. We also remove the need for conditioning on physical fields
using a computationally inexpensive approximation inspired by classic ODE
solutions and reduce the number of steps needed to generate a feasible and
performant topology. Our method allows us to efficiently generate good
topologies and explicitly guide them to regions with high manufacturability and
high performance, without the need for external auxiliary models or additional
labeled data. We believe that our method can lead to significant advancements
in the design and optimization of structures in engineering applications, and
can be applied to a broader spectrum of performance-aware engineering design
problems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SUD2: Supervision by Denoising Diffusion Models for Image Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> Matthew A. Chan, Sean I. Young, Christopher A. Metzler </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09642" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Many imaging inverse problems$\unicode{x2014}$such as image-dependent
in-painting and dehazing$\unicode{x2014}$are challenging because their forward
models are unknown or depend on unknown latent parameters. While one can solve
such problems by training a neural network with vast quantities of paired
training data, such paired training data is often unavailable. In this paper,
we propose a generalized framework for training image reconstruction networks
when paired training data is scarce. In particular, we demonstrate the ability
of image denoising algorithms and, by extension, denoising diffusion models to
supervise network training in the absence of paired training data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Post-Processing for Low-Light Image Enhancement
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> Savvas Panagiotou, Anna S. Bosman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09627" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Low-light image enhancement (LLIE) techniques attempt to increase the
visibility of images captured in low-light scenarios. However, as a result of
enhancement, a variety of image degradations such as noise and color bias are
revealed. Furthermore, each particular LLIE approach may introduce a different
form of flaw within its enhanced results. To combat these image degradations,
post-processing denoisers have widely been used, which often yield oversmoothed
results lacking detail. We propose using a diffusion model as a post-processing
approach, and we introduce Low-light Post-processing Diffusion Model (LPDM) in
order to model the conditional distribution between under-exposed and
normally-exposed images. We apply LPDM in a manner which avoids the
computationally expensive generative reverse process of typical diffusion
models, and post-process images in one pass through LPDM. Extensive experiments
demonstrate that our approach outperforms competing post-processing denoisers
by increasing the perceptual quality of enhanced low-light images on a variety
of challenging low-light datasets. Source code is available at
https://github.com/savvaki/LPDM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> Maham Tanveer, Yizhi Wang, Ali Mahdavi-Amiri, Hao Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09604" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce a novel method to automatically generate an artistic typography
by stylizing one or more letter fonts to visually convey the semantics of an
input word, while ensuring that the output remains readable. To address an
assortment of challenges with our task at hand including conflicting goals
(artistic stylization vs. legibility), lack of ground truth, and immense search
space, our approach utilizes large language models to bridge texts and visual
images for stylization and build an unsupervised generative model with a
diffusion model backbone. Specifically, we employ the denoising generator in
Latent Diffusion Model (LDM), with the key addition of a CNN-based
discriminator to adapt the input style onto the input text. The discriminator
uses rasterized images of a given letter/word font as real samples and output
of the denoising generator as fake samples. Our model is coined DS-Fusion for
discriminated and stylized diffusion. We showcase the quality and versatility
of our method through numerous examples, qualitative and quantitative
evaluation, as well as ablation studies. User studies comparing to strong
baselines including CLIPDraw and DALL-E 2, as well as artist-crafted
typographies, demonstrate strong performance of DS-Fusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Efficient Diffusion Training via Min-SNR Weighting Strategy
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, Baining Guo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09556" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models have been a mainstream approach for image
generation, however, training these models often suffers from slow convergence.
In this paper, we discovered that the slow convergence is partly due to
conflicting optimization directions between timesteps. To address this issue,
we treat the diffusion training as a multi-task learning problem, and introduce
a simple yet effective approach referred to as Min-SNR-$\gamma$. This method
adapts loss weights of timesteps based on clamped signal-to-noise ratios, which
effectively balances the conflicts among timesteps. Our results demonstrate a
significant improvement in converging speed, 3.4$\times$ faster than previous
weighting strategies. It is also more effective, achieving a new record FID
score of 2.06 on the ImageNet $256\times256$ benchmark using smaller
architectures than that employed in previous state-of-the-art. The code is
available at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-HPC: Generating Synthetic Images with Realistic Humans
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> Zhenzhen Weng, Laura Bravo-Sánchez, Serena Yeung </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09541" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent text-to-image generative models have exhibited remarkable abilities in
generating high-fidelity and photo-realistic images. However, despite the
visually impressive results, these models often struggle to preserve plausible
human structure in the generations. Due to this reason, while generative models
have shown promising results in aiding downstream image recognition tasks by
generating large volumes of synthetic data, they remain infeasible for
improving downstream human pose perception and understanding. In this work, we
propose Diffusion model with Human Pose Correction (Diffusion HPC), a
text-conditioned method that generates photo-realistic images with plausible
posed humans by injecting prior knowledge about human body structure. We show
that Diffusion HPC effectively improves the realism of human generations.
Furthermore, as the generations are accompanied by 3D meshes that serve as
ground truths, Diffusion HPC's generated image-mesh pairs are well-suited for
downstream human mesh recovery task, where a shortage of 3D training data has
long been an issue.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffIR: Efficient Diffusion Model for Image Restoration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, Luc Van Gool </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09472" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion model (DM) has achieved SOTA performance by modeling the image
synthesis process into a sequential application of a denoising network.
However, different from image synthesis generating each pixel from scratch,
most pixels of image restoration (IR) are given. Thus, for IR, traditional DMs
running massive iterations on a large model to estimate whole images or feature
maps is inefficient. To address this issue, we propose an efficient DM for IR
(DiffIR), which consists of a compact IR prior extraction network (CPEN),
dynamic IR transformer (DIRformer), and denoising network. Specifically, DiffIR
has two training stages: pretraining and training DM. In pretraining, we input
ground-truth images into CPEN$_{S1}$ to capture a compact IR prior
representation (IPR) to guide DIRformer. In the second stage, we train the DM
to directly estimate the same IRP as pretrained CPEN$_{S1}$ only using LQ
images. We observe that since the IPR is only a compact vector, DiffIR can use
fewer iterations than traditional DM to obtain accurate estimations and
generate more stable and realistic results. Since the iterations are few, our
DiffIR can adopt a joint optimization of CPEN$_{S2}$, DIRformer, and denoising
network, which can further reduce the estimation error influence. We conduct
extensive experiments on several IR tasks and achieve SOTA performance while
consuming less computational costs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> David Svitov, Dmitrii Gudkov, Renat Bashirov, Victor Lempitsky </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09375" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present DINAR, an approach for creating realistic rigged fullbody avatars
from single RGB images. Similarly to previous works, our method uses neural
textures combined with the SMPL-X body model to achieve photo-realistic quality
of avatars while keeping them easy to animate and fast to infer. To restore the
texture, we use a latent diffusion model and show how such model can be trained
in the neural texture space. The use of the diffusion model allows us to
realistically reconstruct large unseen regions such as the back of a person
given the frontal view. The models in our pipeline are trained using 2D images
and videos only. In the experiments, our approach achieves state-of-the-art
rendering quality and good generalization to new poses and viewpoints. In
particular, the approach improves state-of-the-art on the SnapshotPeople public
benchmark.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, Jiaying Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09319" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Language-guided image generation has achieved great success nowadays by using
diffusion models. However, texts can be less detailed to describe
highly-specific subjects such as a particular dog or a certain car, which makes
pure text-to-image generation not accurate enough to satisfy user requirements.
In this work, we present a novel Unified Multi-Modal Latent Diffusion
(UMM-Diffusion) which takes joint texts and images containing specified
subjects as input sequences and generates customized images with the subjects.
To be more specific, both input texts and images are encoded into one unified
multi-modal latent space, in which the input images are learned to be projected
to pseudo word embedding and can be further combined with text to guide image
generation. Besides, to eliminate the irrelevant parts of the input images such
as background or illumination, we propose a novel sampling technique of
diffusion models used by the image generator which fuses the results guided by
multi-modal input and pure text input. By leveraging the large-scale
pre-trained text-to-image generator and the designed image encoder, our method
is able to generate high-quality images with complex semantics from both
aspects of input texts and images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DIRE for Diffusion-Generated Image Detection
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2023 </span>    
         <span class="authors"> Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, Houqiang Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.09295" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown remarkable success in visual synthesis, but have
also raised concerns about potential abuse for malicious purposes. In this
paper, we seek to build a detector for telling apart real images from
diffusion-generated images. We find that existing detectors struggle to detect
images generated by diffusion models, even if we include generated images from
a specific diffusion model in their training data. To address this issue, we
propose a novel image representation called DIffusion Reconstruction Error
(DIRE), which measures the error between an input image and its reconstruction
counterpart by a pre-trained diffusion model. We observe that
diffusion-generated images can be approximately reconstructed by a diffusion
model while real images cannot. It provides a hint that DIRE can serve as a
bridge to distinguish generated and real images. DIRE provides an effective way
to detect images generated by most diffusion models, and it is general for
detecting generated images from unseen diffusion models and robust to various
perturbations. Furthermore, we establish a comprehensive diffusion-generated
benchmark including images generated by eight diffusion models to evaluate the
performance of diffusion-generated image detectors. Extensive experiments on
our collected benchmark demonstrate that DIRE exhibits superiority over
previous generated-image detectors. The code and dataset are available at
https://github.com/ZhendongWang6/DIRE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Stochastic Segmentation with Conditional Categorical Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Lukas Zbinden, Lars Doorenbos, Theodoros Pissas, Adrian Thomas Huber, Raphael Sznitman, Pablo Márquez-Neila </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08888" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Semantic segmentation has made significant progress in recent years thanks to
deep neural networks, but the common objective of generating a single
segmentation output that accurately matches the image's content may not be
suitable for safety-critical domains such as medical diagnostics and autonomous
driving. Instead, multiple possible correct segmentation maps may be required
to reflect the true distribution of annotation maps. In this context,
stochastic semantic segmentation methods must learn to predict conditional
distributions of labels given the image, but this is challenging due to the
typically multimodal distributions, high-dimensional output spaces, and limited
annotation data. To address these challenges, we propose a conditional
categorical diffusion model (CCDM) for semantic segmentation based on Denoising
Diffusion Probabilistic Models. Our model is conditioned to the input image,
enabling it to generate multiple segmentation label maps that account for the
aleatoric uncertainty arising from divergent ground truth annotations. Our
experimental results show that CCDM achieves state-of-the-art performance on
LIDC, a stochastic semantic segmentation dataset, and outperforms established
baselines on the classical segmentation dataset Cityscapes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Jan Oscar Cross-Zamirski, Praveen Anand, Guy Williams, Elizabeth Mouchet, Yinhai Wang, Carola-Bibiane Schönlieb </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08863" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, q-bio.QM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image-to-image reconstruction problems with free or inexpensive metadata in
the form of class labels appear often in biological and medical image domains.
Existing text-guided or style-transfer image-to-image approaches do not
translate to datasets where additional information is provided as discrete
classes. We introduce and implement a model which combines image-to-image and
class-guided denoising diffusion probabilistic models. We train our model on a
real-world dataset of microscopy images used for drug discovery, with and
without incorporating metadata labels. By exploring the properties of
image-to-image diffusion with relevant labels, we show that class-guided
image-to-image diffusion can improve the meaningful content of the
reconstructed images and outperform the unguided model in useful downstream
tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Stochastic Interpolants: A Unifying Framework for Flows and Diffusions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08797" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cond-mat.dis-nn, math.PR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 A class of generative models that unifies flow-based and diffusion-based
methods is introduced. These models extend the framework proposed in Albergo &
Vanden-Eijnden (2023), enabling the use of a broad class of continuous-time
stochastic processes called `stochastic interpolants' to bridge any two
arbitrary probability density functions exactly in finite time. These
interpolants are built by combining data from the two prescribed densities with
an additional latent variable that shapes the bridge in a flexible way. The
time-dependent probability density function of the stochastic interpolant is
shown to satisfy a first-order transport equation as well as a family of
forward and backward Fokker-Planck equations with tunable diffusion. Upon
consideration of the time evolution of an individual sample, this viewpoint
immediately leads to both deterministic and stochastic generative models based
on probability flow equations or stochastic differential equations with an
adjustable level of noise. The drift coefficients entering these models are
time-dependent velocity fields characterized as the unique minimizers of simple
quadratic objective functions, one of which is a new objective for the score of
the interpolant density. Remarkably, we show that minimization of these
quadratic objectives leads to control of the likelihood for any of our
generative models built upon stochastic dynamics. By contrast, we establish
that generative models based upon a deterministic dynamics must, in addition,
control the Fisher divergence between the target and the model. We also
construct estimators for the likelihood and the cross-entropy of
interpolant-based generative models, discuss connections with other stochastic
bridges, and demonstrate that such models recover the Schr\"odinger bridge
between the two target densities when explicitly optimizing over the
interpolant.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionAD: Denoising Diffusion for Anomaly Detection
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Hui Zhang, Zheng Wang, Zuxuan Wu, Yu-Gang Jiang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08730" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Anomaly detection is widely applied due to its remarkable effectiveness and
efficiency in meeting the needs of real-world industrial manufacturing. We
introduce a new pipeline, DiffusionAD, to anomaly detection. We frame anomaly
detection as a ``noise-to-norm'' paradigm, in which anomalies are identified as
inconsistencies between a query image and its flawless approximation. Our
pipeline achieves this by restoring the anomalous regions from the noisy
corrupted query image while keeping the normal regions unchanged. DiffusionAD
includes a denoising sub-network and a segmentation sub-network, which work
together to provide intuitive anomaly detection and localization in an
end-to-end manner, without the need for complicated post-processing steps.
Remarkably, during inference, this framework delivers satisfactory performance
with just one diffusion reverse process step, which is tens to hundreds of
times faster than general diffusion methods. Extensive evaluations on standard
and challenging benchmarks including VisA and DAGM show that DiffusionAD
outperforms current state-of-the-art paradigms, demonstrating the effectiveness
and generalizability of the proposed pipeline.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Shuyao Shang, Zhengyang Shan, Guangxing Liu, Jinglin Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08714" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Adapting the Diffusion Probabilistic Model (DPM) for direct image
super-resolution is wasteful, given that a simple Convolutional Neural Network
(CNN) can recover the main low-frequency content. Therefore, we present
ResDiff, a novel Diffusion Probabilistic Model based on Residual structure for
Single Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN,
which restores primary low-frequency components, and a DPM, which predicts the
residual between the ground-truth image and the CNN-predicted image. In
contrast to the common diffusion-based methods that directly use LR images to
guide the noise towards HR space, ResDiff utilizes the CNN's initial prediction
to direct the noise towards the residual space between HR space and
CNN-predicted space, which not only accelerates the generation process but also
acquires superior sample quality. Additionally, a frequency-domain-based loss
function for CNN is introduced to facilitate its restoration, and a
frequency-domain guided diffusion is designed for DPM on behalf of predicting
high-frequency details. The extensive experiments on multiple benchmark
datasets demonstrate that ResDiff outperforms previous diffusion-based methods
in terms of shorter model convergence time, superior generation quality, and
more diverse samples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Speech Signal Improvement Using Causal Generative Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Julius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, Tal Peer, Timo Gerkmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08674" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we present a causal speech signal improvement system that is
designed to handle different types of distortions. The method is based on a
generative diffusion model which has been shown to work well in scenarios with
missing data and non-linear corruptions. To guarantee causal processing, we
modify the network architecture of our previous work and replace global
normalization with causal adaptive gain control. We generate diverse training
data containing a broad range of distortions. This work was performed in the
context of an "ICASSP Signal Processing Grand Challenge" and submitted to the
non-real-time track of the "Speech Signal Improvement Challenge 2023", where it
was ranked fifth.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generating symbolic music using diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Lilac Atassi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08385" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Probabilistic models have emerged as simple yet very
powerful generative models. Unlike other generative models, diffusion models do
not suffer from mode collapse or require a discriminator to generate
high-quality samples. In this paper, a diffusion model that uses a binomial
prior distribution to generate piano rolls is proposed. The paper also proposes
an efficient method to train the model and generate samples. The generated
music has coherence at time scales up to the length of the training piano roll
segments. The paper demonstrates how this model is conditioned on the input and
can be used to harmonize a given melody, complete an incomplete piano roll, or
generate a variation of a given piece. The code is publicly shared to encourage
the use and development of the method by the community.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffBEV: Conditional Diffusion Model for Bird's Eye View Perception
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Jiayu Zou, Zheng Zhu, Yun Ye, Xingang Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08333" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 BEV perception is of great importance in the field of autonomous driving,
serving as the cornerstone of planning, controlling, and motion prediction. The
quality of the BEV feature highly affects the performance of BEV perception.
However, taking the noises in camera parameters and LiDAR scans into
consideration, we usually obtain BEV representation with harmful noises.
Diffusion models naturally have the ability to denoise noisy samples to the
ideal data, which motivates us to utilize the diffusion model to get a better
BEV representation. In this work, we propose an end-to-end framework, named
DiffBEV, to exploit the potential of diffusion model to generate a more
comprehensive BEV representation. To the best of our knowledge, we are the
first to apply diffusion model to BEV perception. In practice, we design three
types of conditions to guide the training of the diffusion model which denoises
the coarse samples and refines the semantic feature in a progressive way.
What's more, a cross-attention module is leveraged to fuse the context of BEV
feature and the semantic content of conditional diffusion model. DiffBEV
achieves a 25.9% mIoU on the nuScenes dataset, which is 6.2% higher than the
best-performing existing approach. Quantitative and qualitative results on
multiple benchmarks demonstrate the effectiveness of DiffBEV in BEV semantic
segmentation and 3D object detection tasks. The code will be available soon.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Decomposed Diffusion Models for High-Quality Video Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 15, 2023 </span>    
         <span class="authors"> Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08320" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 A diffusion probabilistic model (DPM), which constructs a forward diffusion
process by gradually adding noise to data points and learns the reverse
denoising process to generate new samples, has been shown to handle complex
data distribution. Despite its recent success in image synthesis, applying DPMs
to video generation is still challenging due to high-dimensional data spaces.
Previous methods usually adopt a standard diffusion process, where frames in
the same video clip are destroyed with independent noises, ignoring the content
redundancy and temporal correlation. This work presents a decomposed diffusion
process via resolving the per-frame noise into a base noise that is shared
among all frames and a residual noise that varies along the time axis. The
denoising pipeline employs two jointly-learned networks to match the noise
decomposition accordingly. Experiments on various datasets confirm that our
approach, termed as VideoFusion, surpasses both GAN-based and diffusion-based
alternatives in high-quality video generation. We further show that our
decomposed formulation can benefit from pre-trained image diffusion models and
well-support text-conditioned video creation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for Contrast Harmonization of Magnetic Resonance Images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 14, 2023 </span>    
         <span class="authors"> Alicia Durrer, Julia Wolleb, Florentin Bieder, Tim Sinnecker, Matthias Weigel, Robin Sandkühler, Cristina Granziera, Özgür Yaldizli, Philippe C. Cattin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08189" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Magnetic resonance (MR) images from multiple sources often show differences
in image contrast related to acquisition settings or the used scanner type. For
long-term studies, longitudinal comparability is essential but can be impaired
by these contrast differences, leading to biased results when using automated
evaluation tools. This study presents a diffusion model-based approach for
contrast harmonization. We use a data set consisting of scans of 18 Multiple
Sclerosis patients and 22 healthy controls. Each subject was scanned in two MR
scanners of different magnetic field strengths (1.5 T and 3 T), resulting in a
paired data set that shows scanner-inherent differences. We map images from the
source contrast to the target contrast for both directions, from 3 T to 1.5 T
and from 1.5 T to 3 T. As we only want to change the contrast, not the
anatomical information, our method uses the original image to guide the
image-to-image translation process by adding structural information. The aim is
that the mapped scans display increased comparability with scans of the target
contrast for downstream tasks. We evaluate this method for the task of
segmentation of cerebrospinal fluid, grey matter and white matter. Our method
achieves good and consistent results for both directions of the mapping.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LayoutDM: Discrete Diffusion Model for Controllable Layout Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 14, 2023 </span>    
         <span class="authors"> Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08137" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Controllable layout generation aims at synthesizing plausible arrangement of
element bounding boxes with optional constraints, such as type or position of a
specific element. In this work, we try to solve a broad range of layout
generation tasks in a single model that is based on discrete state-space
diffusion models. Our model, named LayoutDM, naturally handles the structured
layout data in the discrete representation and learns to progressively infer a
noiseless layout from the initial input, where we model the layout corruption
process by modality-wise discrete diffusion. For conditional generation, we
propose to inject layout constraints in the form of masking or logit adjustment
during inference. We show in the experiments that our LayoutDM successfully
generates high-quality layouts and outperforms both task-specific and
task-agnostic baselines on several layout tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MeshDiffusion: Score-based Generative 3D Mesh Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 14, 2023 </span>    
         <span class="authors"> Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, Weiyang Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08133" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.GR, cs.AI, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We consider the task of generating realistic 3D shapes, which is useful for a
variety of applications such as automatic scene generation and physical
simulation. Compared to other 3D representations like voxels and point clouds,
meshes are more desirable in practice, because (1) they enable easy and
arbitrary manipulation of shapes for relighting and simulation, and (2) they
can fully leverage the power of modern graphics pipelines which are mostly
optimized for meshes. Previous scalable methods for generating meshes typically
rely on sub-optimal post-processing, and they tend to produce overly-smooth or
noisy surfaces without fine-grained geometric details. To overcome these
shortcomings, we take advantage of the graph structure of meshes and use a
simple yet very effective generative modeling method to generate 3D meshes.
Specifically, we represent meshes with deformable tetrahedral grids, and then
train a diffusion model on this direct parametrization. We demonstrate the
effectiveness of our model on multiple generative tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Point Cloud Diffusion Models for Automatic Implant Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 14, 2023 </span>    
         <span class="authors"> Paul Friedrich, Julia Wolleb, Florentin Bieder, Florian M. Thieringer, Philippe C. Cattin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.08061" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Advances in 3D printing of biocompatible materials make patient-specific
implants increasingly popular. The design of these implants is, however, still
a tedious and largely manual process. Existing approaches to automate implant
generation are mainly based on 3D U-Net architectures on downsampled or
patch-wise data, which can result in a loss of detail or contextual
information. Following the recent success of Diffusion Probabilistic Models, we
propose a novel approach for implant generation based on a combination of 3D
point cloud diffusion models and voxelization networks. Due to the stochastic
sampling process in our diffusion model, we can propose an ensemble of
different implants per defect, from which the physicians can choose the most
suitable one. We evaluate our method on the SkullBreak and SkullFix datasets,
generating high-quality implants and achieving competitive evaluation scores.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Text-to-image Diffusion Model in Generative AI: A Survey
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 14, 2023 </span>    
         <span class="authors"> Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In So Kweon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.07909" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This survey reviews text-to-image diffusion models in the context that
diffusion models have emerged to be popular for a wide range of generative
tasks. As a self-contained work, this survey starts with a brief introduction
of how a basic diffusion model works for image synthesis, followed by how
condition or guidance improves learning. Based on that, we present a review of
state-of-the-art methods on text-conditioned image synthesis, i.e.,
text-to-image. We further summarize applications beyond text-to-image
generation: text-guided creative generation and text-guided image editing.
Beyond the progress made so far, we discuss existing challenges and promising
future directions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generalised Scale-Space Properties for Probabilistic Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 14, 2023 </span>    
         <span class="authors"> Pascal Peter </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.07900" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Probabilistic diffusion models enjoy increasing popularity in the deep
learning community. They generate convincing samples from a learned
distribution of input images with a wide field of practical applications.
Originally, these approaches were motivated from drift-diffusion processes, but
these origins find less attention in recent, practice-oriented publications. We
investigate probabilistic diffusion models from the viewpoint of scale-space
research and show that they fulfil generalised scale-space properties on
evolving probability distributions. Moreover, we discuss similarities and
differences between interpretations of the physical core concept of
drift-diffusion in the deep learning and model-based world. To this end, we
examine relations of probabilistic diffusion to osmosis filters.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffuseRoll: Multi-track multi-category music generation based on diffusion model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 14, 2023 </span>    
         <span class="authors"> Hongfei Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.07794" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.MM, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advancements in generative models have shown remarkable progress in
music generation. However, most existing methods focus on generating monophonic
or homophonic music, while the generation of polyphonic and multi-track music
with rich attributes is still a challenging task. In this paper, we propose a
novel approach for multi-track, multi-attribute symphonic music generation
using the diffusion model. Specifically, we generate piano-roll representations
with a diffusion model and map them to MIDI format for output. To capture rich
attribute information, we introduce a color coding scheme to encode note
sequences into color and position information that represents pitch,velocity,
and instrument. This scheme enables a seamless mapping between discrete music
sequences and continuous images. We also propose a post-processing method to
optimize the generated scores for better performance. Experimental results show
that our method outperforms state-of-the-art methods in terms of polyphonic
music generation with rich attribute information compared to the figure
methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Erasing Concepts from Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 13, 2023 </span>    
         <span class="authors"> Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, David Bau </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.07345" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Motivated by recent advancements in text-to-image diffusion, we study erasure
of specific concepts from the model's weights. While Stable Diffusion has shown
promise in producing explicit or realistic artwork, it has raised concerns
regarding its potential for misuse. We propose a fine-tuning method that can
erase a visual concept from a pre-trained diffusion model, given only the name
of the style and using negative guidance as a teacher. We benchmark our method
against previous approaches that remove sexually explicit content and
demonstrate its effectiveness, performing on par with Safe Latent Diffusion and
censored training. To evaluate artistic style removal, we conduct experiments
erasing five modern artists from the network and conduct a user study to assess
the human perception of the removed styles. Unlike previous methods, our
approach can remove concepts from a diffusion model permanently rather than
modifying the output at the inference time, so it cannot be circumvented even
if a user has access to model weights. Our code, data, and results are
available at https://erasing.baulab.info/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Synthesizing Realistic Image Restoration Training Pairs: A Diffusion Approach
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 13, 2023 </span>    
         <span class="authors"> Tao Yang, Peiran Ren, Xuansong xie, Lei Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.06994" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In supervised image restoration tasks, one key issue is how to obtain the
aligned high-quality (HQ) and low-quality (LQ) training image pairs.
Unfortunately, such HQ-LQ training pairs are hard to capture in practice, and
hard to synthesize due to the complex unknown degradation in the wild. While
several sophisticated degradation models have been manually designed to
synthesize LQ images from their HQ counterparts, the distribution gap between
the synthesized and real-world LQ images remains large. We propose a new
approach to synthesizing realistic image restoration training pairs using the
emerging denoising diffusion probabilistic model (DDPM).
  First, we train a DDPM, which could convert a noisy input into the desired LQ
image, with a large amount of collected LQ images, which define the target data
distribution. Then, for a given HQ image, we synthesize an initial LQ image by
using an off-the-shelf degradation model, and iteratively add proper Gaussian
noises to it. Finally, we denoise the noisy LQ image using the pre-trained DDPM
to obtain the final LQ image, which falls into the target distribution of
real-world LQ images. Thanks to the strong capability of DDPM in distribution
approximation, the synthesized HQ-LQ image pairs can be used to train robust
models for real-world image restoration tasks, such as blind face image
restoration and blind image super-resolution. Experiments demonstrated the
superiority of our proposed approach to existing degradation models. Code and
data will be released.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 13, 2023 </span>    
         <span class="authors"> Zhixin Wang, Xiaoyun Zhang, Ziying Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, Yanfeng Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.06885" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Blind face restoration usually synthesizes degraded low-quality data with a
pre-defined degradation model for training, while more complex cases could
happen in the real world. This gap between the assumed and actual degradation
hurts the restoration performance where artifacts are often observed in the
output. However, it is expensive and infeasible to include every type of
degradation to cover real-world cases in the training data. To tackle this
robustness issue, we propose Diffusion-based Robust Degradation Remover (DR2)
to first transform the degraded image to a coarse but degradation-invariant
prediction, then employ an enhancement module to restore the coarse prediction
to a high-quality image. By leveraging a well-performing denoising diffusion
probabilistic model, our DR2 diffuses input images to a noisy status where
various types of degradation give way to Gaussian noise, and then captures
semantic information through iterative denoising steps. As a result, DR2 is
robust against common degradation (e.g. blur, resize, noise and compression)
and compatible with different designs of enhancement modules. Experiments in
various settings show that our framework outperforms state-of-the-art methods
on heavily degraded synthetic and real-world datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 12, 2023 </span>    
         <span class="authors"> Yuchun Miao, Lefei Zhang, Liangpei Zhang, Dacheng Tao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.06682" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently received a surge of interest due to their
impressive performance for image restoration, especially in terms of noise
robustness. However, existing diffusion-based methods are trained on a large
amount of training data and perform very well in-distribution, but can be quite
susceptible to distribution shift. This is especially inappropriate for
data-starved hyperspectral image (HSI) restoration. To tackle this problem,
this work puts forth a self-supervised diffusion model for HSI restoration,
namely Denoising Diffusion Spatio-Spectral Model (\texttt{DDS2M}), which works
by inferring the parameters of the proposed Variational Spatio-Spectral Module
(VS2M) during the reverse diffusion process, solely using the degraded HSI
without any extra training data. In VS2M, a variational inference-based loss
function is customized to enable the untrained spatial and spectral networks to
learn the posterior distribution, which serves as the transitions of the
sampling chain to help reverse the diffusion process. Benefiting from its
self-supervised nature and the diffusion process, \texttt{DDS2M} enjoys
stronger generalization ability to various HSIs compared to existing
diffusion-based methods and superior robustness to noise compared to existing
HSI restoration methods. Extensive experiments on HSI denoising, noisy HSI
completion and super-resolution on a variety of HSIs demonstrate
\texttt{DDS2M}'s superiority over the existing task-specific state-of-the-arts.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 12, 2023 </span>    
         <span class="authors"> Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.06555" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit
all distributions relevant to a set of multi-modal data in one model. Our key
insight is -- learning diffusion models for marginal, conditional, and joint
distributions can be unified as predicting the noise in the perturbed data,
where the perturbation levels (i.e. timesteps) can be different for different
modalities. Inspired by the unified view, UniDiffuser learns all distributions
simultaneously with a minimal modification to the original diffusion model --
perturbs data in all modalities instead of a single modality, inputs individual
timesteps in different modalities, and predicts the noise of all modalities
instead of a single modality. UniDiffuser is parameterized by a transformer for
diffusion models to handle input types of different modalities. Implemented on
large-scale paired image-text data, UniDiffuser is able to perform image, text,
text-to-image, image-to-text, and image-text pair generation by setting proper
timesteps without additional overhead. In particular, UniDiffuser is able to
produce perceptually realistic samples in all tasks and its quantitative
results (e.g., the FID and CLIP score) are not only superior to existing
general-purpose models but also comparable to the bespoken models (e.g., Stable
Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image
generation).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PARASOL: Parametric Style Control for Diffusion Image Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 11, 2023 </span>    
         <span class="authors"> Gemma Canet Tarrés, Dan Ruta, Tu Bui, John Collomosse </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.06464" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose PARASOL, a multi-modal synthesis model that enables disentangled,
parametric control of the visual style of the image by jointly conditioning
synthesis on both content and a fine-grained visual style embedding. We train a
latent diffusion model (LDM) using specific losses for each modality and adapt
the classifier-free guidance for encouraging disentangled control over
independent content and style modalities at inference time. We leverage
auxiliary semantic and style-based search to create training triplets for
supervision of the LDM, ensuring complementarity of content and style cues.
PARASOL shows promise for enabling nuanced control over visual style in
diffusion models for image creation and stylization, as well as generative
search where text-based search results may be adapted to more closely match
user intent by interpolating both content and style descriptors.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 10, 2023 </span>    
         <span class="authors"> Yunguan Fu, Yiwen Li, Shaheer U. Saeed, Matthew J. Clarkson, Yipeng Hu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.06040" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, denoising diffusion probabilistic models (DDPM) have been applied
to image segmentation by generating segmentation masks conditioned on images,
while the applications were mainly limited to 2D networks without exploiting
potential benefits from the 3D formulation. In this work, for the first time,
DDPMs are used for 3D multiclass image segmentation. We make three key
contributions that all focus on aligning the training strategy with the
evaluation methodology, and improving efficiency. Firstly, the model predicts
segmentation masks instead of sampled noise and is optimised directly via Dice
loss. Secondly, the predicted mask in the previous time step is recycled to
generate noise-corrupted masks to reduce information leakage. Finally, the
diffusion process during training was reduced to five steps, the same as the
evaluation. Through studies on two large multiclass data sets (prostate MR and
abdominal CT), we demonstrated significantly improved performance compared to
existing DDPMs, and reached competitive performance with non-diffusion
segmentation models, based on U-net, within the same compute budget. The
JAX-based diffusion framework has been released on
https://github.com/mathpluscode/ImgX-DiffSeg.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Generative Models for Medical Image Segmentation using Signed Distance Functions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 10, 2023 </span>    
         <span class="authors"> Lea Bogensperger, Dominik Narnhofer, Filip Ilic, Thomas Pock </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05966" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Medical image segmentation is a crucial task that relies on the ability to
accurately identify and isolate regions of interest in images. Thereby,
generative approaches allow to capture the statistical properties of
segmentation masks that are dependent on the respective medical images. In this
work we propose a conditional score-based generative modeling framework that
leverages the signed distance function to represent an implicit and smoother
distribution of segmentation masks. The score function of the conditional
distribution of segmentation masks is learned in a conditional denoising
process, which can be effectively used to generate accurate segmentation masks.
Moreover, uncertainty maps can be generated, which can aid in further analysis
and thus enhance the predictive robustness. We qualitatively and quantitatively
illustrate competitive performance of the proposed method on a public nuclei
and gland segmentation data set, highlighting its potential utility in medical
image segmentation applications.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 10, 2023 </span>    
         <span class="authors"> Weixin Chen, Dawn Song, Bo Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05762" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have achieved great success in a range of tasks, such as
image synthesis and molecule design. As such successes hinge on large-scale
training data collected from diverse sources, the trustworthiness of these
collected data is hard to control or audit. In this work, we aim to explore the
vulnerabilities of diffusion models under potential training data manipulations
and try to answer: How hard is it to perform Trojan attacks on well-trained
diffusion models? What are the adversarial targets that such Trojan attacks can
achieve? To answer these questions, we propose an effective Trojan attack
against diffusion models, TrojDiff, which optimizes the Trojan diffusion and
generative processes during training. In particular, we design novel
transitions during the Trojan diffusion process to diffuse adversarial targets
into a biased Gaussian distribution and propose a new parameterization of the
Trojan generative process that leads to an effective training objective for the
attack. In addition, we consider three types of adversarial targets: the
Trojaned diffusion models will always output instances belonging to a certain
class from the in-domain distribution (In-D2D attack), out-of-domain
distribution (Out-D2D-attack), and one specific instance (D2I attack). We
evaluate TrojDiff on CIFAR-10 and CelebA datasets against both DDPM and DDIM
diffusion models. We show that TrojDiff always achieves high attack performance
under different adversarial targets using different types of triggers, while
the performance in benign environments is preserved. The code is available at
https://github.com/chenweixin107/TrojDiff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 10, 2023 </span>    
         <span class="authors"> Hyungjin Chung, Suhyeon Lee, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05754" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown exceptional performance in solving inverse
problems. However, one major limitation is the slow inference time. While
faster diffusion samplers have been developed for unconditional sampling, there
has been limited research on conditional sampling in the context of inverse
problems. In this study, we propose a novel and efficient diffusion sampling
strategy that employs the geometric decomposition of diffusion sampling.
Specifically, we discover that the samples generated from diffusion models can
be decomposed into two orthogonal components: a ``denoised" component obtained
by projecting the sample onto the clean data manifold, and a ``noise" component
that induces a transition to the next lower-level noisy manifold with the
addition of stochastic noise. Furthermore, we prove that, under some conditions
on the clean data manifold, the conjugate gradient update for imposing
conditioning from the denoised signal belongs to the clean manifold, resulting
in a much faster and more accurate diffusion sampling. Our method is applicable
regardless of the parameterization and setting (i.e., VE, VP). Notably, we
achieve state-of-the-art reconstruction quality on challenging real-world
medical inverse imaging problems, including multi-coil MRI reconstruction and
3D CT reconstruction. Moreover, our proposed method achieves more than 80 times
faster inference time than the previous state-of-the-art method.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generalized Diffusion MRI Denoising and Super-Resolution using Swin Transformers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 10, 2023 </span>    
         <span class="authors"> Amir Sadikov, Jamie Wren-Jarvis, Xinlei Pan, Lanya T. Cai, Pratik Mukherjee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05686" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion MRI is a non-invasive, in-vivo medical imaging method able to map
tissue microstructure and structural connectivity of the human brain, as well
as detect changes, such as brain development and injury, not visible by other
clinical neuroimaging techniques. However, acquiring high signal-to-noise ratio
(SNR) datasets with high angular and spatial sampling requires prohibitively
long scan times, limiting usage in many important clinical settings, especially
children, the elderly, and emergency patients with acute neurological disorders
who might not be able to cooperate with the MRI scan without conscious sedation
or general anesthesia. Here, we propose to use a Swin UNEt TRansformers (Swin
UNETR) model, trained on augmented Human Connectome Project (HCP) data and
conditioned on registered T1 scans, to perform generalized denoising and
super-resolution of diffusion MRI invariant to acquisition parameters, patient
populations, scanners, and sites. We qualitatively demonstrate super-resolution
with artificially downsampled HCP data in normal adult volunteers. Our
experiments on two other unrelated datasets, one of children with
neurodevelopmental disorders and one of traumatic brain injury patients, show
that our method demonstrates superior denoising despite wide data distribution
shifts. Further improvement can be achieved via finetuning with just one
additional subject. We apply our model to diffusion tensor (2nd order spherical
harmonic) and higher-order spherical harmonic coefficient estimation and show
results superior to current state-of-the-art methods. Our method can be used
out-of-the-box or minimally finetuned to denoise and super-resolve a wide
variety of diffusion MRI datasets. The code and model are publicly available at
https://github.com/ucsfncl/dmri-swin.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 10, 2023 </span>    
         <span class="authors"> Hongyi Yuan, Songchi Zhou, Sheng Yu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05656" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Electronic health records (EHR) contain vast biomedical knowledge and are
rich resources for developing precise medicine systems. However, due to privacy
concerns, there are limited high-quality EHR data accessible to researchers
hence hindering the advancement of methodologies. Recent research has explored
using generative modelling methods to synthesize realistic EHR data, and most
proposed methods are based on the generative adversarial network (GAN) and its
variants for EHR synthesis. Although GAN-style methods achieved
state-of-the-art performance in generating high-quality EHR data, such methods
are hard to train and prone to mode collapse. Diffusion models are recently
proposed generative modelling methods and set cutting-edge performance in image
generation. The performance of diffusion models in realistic EHR synthesis is
rarely explored. In this work, we explore whether the superior performance of
diffusion models can translate to the domain of EHR synthesis and propose a
novel EHR synthesis method named EHRDiff. Through comprehensive experiments,
EHRDiff achieves new state-of-the-art performance for the quality of synthetic
EHR data and can better protect private information in real training EHRs in
the meanwhile.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 09, 2023 </span>    
         <span class="authors"> Matthew Leigh, Debajyoti Sengupta, Guillaume Quétant, John Andrew Raine, Knut Zoch, Tobias Golling </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05376" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  hep-ph, cs.LG, hep-ex
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we present a new method to efficiently generate jets in High
Energy Physics called PC-JeDi. This method utilises score-based diffusion
models in conjunction with transformers which are well suited to the task of
generating jets as particle clouds due to their permutation equivariance.
PC-JeDi achieves competitive performance with current state-of-the-art methods
across several metrics that evaluate the quality of the generated jets.
Although slower than other models, due to the large number of forward passes
required by diffusion models, it is still substantially faster than traditional
detailed simulation. Furthermore, PC-JeDi uses conditional generation to
produce jets with a desired mass and transverse momentum for two different
particles, top quarks and gluons.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Brain-Diffuser: Natural scene reconstruction from fMRI signals using generative latent diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 09, 2023 </span>    
         <span class="authors"> Furkan Ozcelik, Rufin VanRullen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05334" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, q-bio.NC
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In neural decoding research, one of the most intriguing topics is the
reconstruction of perceived natural images based on fMRI signals. Previous
studies have succeeded in re-creating different aspects of the visuals, such as
low-level properties (shape, texture, layout) or high-level features (category
of objects, descriptive semantics of scenes) but have typically failed to
reconstruct these properties together for complex scene images. Generative AI
has recently made a leap forward with latent diffusion models capable of
generating high-complexity images. Here, we investigate how to take advantage
of this innovative technology for brain decoding. We present a two-stage scene
reconstruction framework called ``Brain-Diffuser''. In the first stage,
starting from fMRI signals, we reconstruct images that capture low-level
properties and overall layout using a VDVAE (Very Deep Variational Autoencoder)
model. In the second stage, we use the image-to-image framework of a latent
diffusion model (Versatile Diffusion) conditioned on predicted multimodal (text
and visual) features, to generate final reconstructed images. On the publicly
available Natural Scenes Dataset benchmark, our method outperforms previous
models both qualitatively and quantitatively. When applied to synthetic fMRI
patterns generated from individual ROI (region-of-interest) masks, our trained
model creates compelling ``ROI-optimal'' scenes consistent with neuroscientific
knowledge. Thus, the proposed methodology can have an impact on both applied
(e.g. brain-computer interface) and fundamental neuroscience.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 09, 2023 </span>    
         <span class="authors"> Minh-Quan Le, Tam V. Nguyen, Trung-Nghia Le, Thanh-Toan Do, Minh N. Do, Minh-Triet Tran </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05105" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Few-shot instance segmentation extends the few-shot learning paradigm to the
instance segmentation task, which tries to segment instance objects from a
query image with a few annotated examples of novel categories. Conventional
approaches have attempted to address the task via prototype learning, known as
point estimation. However, this mechanism is susceptible to noise and suffers
from bias due to a significant scarcity of data. To overcome the disadvantages
of the point estimation mechanism, we propose a novel approach, dubbed
MaskDiff, which models the underlying conditional distribution of a binary
mask, which is conditioned on an object region and $K$-shot information.
Inspired by augmentation approaches that perturb data with Gaussian noise for
populating low data density regions, we model the mask distribution with a
diffusion probabilistic model. In addition, we propose to utilize
classifier-free guided mask sampling to integrate category information into the
binary mask generation process. Without bells and whistles, our proposed method
consistently outperforms state-of-the-art methods on both base and novel
classes of the COCO dataset while simultaneously being more stable than
existing methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 09, 2023 </span>    
         <span class="authors"> Keisuke Kawano, Takuro Kutsuna, Ryoko Tokuhisa, Akihiro Nakamura, Yasushi Esaki </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05102" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 One major challenge in machine learning applications is coping with
mismatches between the datasets used in the development and those obtained in
real-world applications. These mismatches may lead to inaccurate predictions
and errors, resulting in poor product quality and unreliable systems. In this
study, we propose StyleDiff to inform developers of the differences between the
two datasets for the steady development of machine learning systems. Using
disentangled image spaces obtained from recently proposed generative models,
StyleDiff compares the two datasets by focusing on attributes in the images and
provides an easy-to-understand analysis of the differences between the
datasets. The proposed StyleDiff performs in $O (d N\log N)$, where $N$ is the
size of the datasets and $d$ is the number of attributes, enabling the
application to large datasets. We demonstrate that StyleDiff accurately detects
differences between datasets and presents them in an understandable format
using, for example, driving scenes datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 09, 2023 </span>    
         <span class="authors"> Yiqun Duan, Zheng Zhu, Xianda Guo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05021" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Monocular depth estimation is a challenging task that predicts the pixel-wise
depth from a single 2D image. Current methods typically model this problem as a
regression or classification task. We propose DiffusionDepth, a new approach
that reformulates monocular depth estimation as a denoising diffusion process.
It learns an iterative denoising process to `denoise' random depth distribution
into a depth map with the guidance of monocular visual conditions. The process
is performed in the latent space encoded by a dedicated depth encoder and
decoder. Instead of diffusing ground truth (GT) depth, the model learns to
reverse the process of diffusing the refined depth of itself into random depth
distribution. This self-diffusion formulation overcomes the difficulty of
applying generative models to sparse GT depth scenarios. The proposed approach
benefits this task by refining depth estimation step by step, which is superior
for generating accurate and highly detailed depth maps. Experimental results on
KITTI and NYU-Depth-V2 datasets suggest that a simple yet efficient diffusion
approach could reach state-of-the-art performance in both indoor and outdoor
scenarios with acceptable inference time.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 08, 2023 </span>    
         <span class="authors"> Paul Hagemann, Sophie Mildenberger, Lars Ruthotto, Gabriele Steidl, Nicole Tianjiao Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.04772" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, math.PR, stat.ML, 60H30, 62M45, 60J60, 68U10
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based diffusion models (SBDM) have recently emerged as state-of-the-art
approaches for image generation. Existing SBDMs are typically formulated in a
finite-dimensional setting, where images are considered as tensors of a finite
size. This papers develops SBDMs in the infinite-dimensional setting, that is,
we model the training data as functions supported on a rectangular domain.
Besides the quest for generating images at ever higher resolution our primary
motivation is to create a well-posed infinite-dimensional learning problem so
that we can discretize it consistently on multiple resolution levels. We
thereby hope to obtain diffusion models that generalize across different
resolution levels and improve the efficiency of the training process. We
demonstrate how to overcome two shortcomings of current SBDM approaches in the
infinite-dimensional setting. First, we modify the forward process to ensure
that the latent distribution is well-defined in the infinite-dimensional
setting using the notion of trace class operators. Second, we illustrate that
approximating the score function with an operator network, in our case Fourier
neural operators (FNOs), is beneficial for multilevel training. After deriving
the forward process in the infinite-dimensional setting and reverse processes
for finite approximations, we show their well-posedness, derive adequate
discretizations, and investigate the role of the latent distributions. We
provide first promising numerical results on two datasets, MNIST and material
structures. In particular, we show that multilevel training is feasible within
this framework.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusing Gaussian Mixtures for Generating Categorical Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 08, 2023 </span>    
         <span class="authors"> Florence Regol, Mark Coates </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.04635" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Learning a categorical distribution comes with its own set of challenges. A
successful approach taken by state-of-the-art works is to cast the problem in a
continuous domain to take advantage of the impressive performance of the
generative models for continuous data. Amongst them are the recently emerging
diffusion probabilistic models, which have the observed advantage of generating
high-quality samples. Recent advances for categorical generative models have
focused on log likelihood improvements. In this work, we propose a generative
model for categorical data based on diffusion models with a focus on
high-quality sample generation, and propose sampled-based evaluation methods.
The efficacy of our method stems from performing diffusion in the continuous
domain while having its parameterization informed by the structure of the
categorical nature of the target distribution. Our method of evaluation
highlights the capabilities and limitations of different generative models for
generating categorical data, and includes experiments on synthetic and
real-world protein datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning Enhancement From Degradation: A Diffusion Model For Fundus Image Enhancement
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 08, 2023 </span>    
         <span class="authors"> Puijin Cheng, Li Lin, Yijin Huang, Huaqing He, Wenhan Luo, Xiaoying Tang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.04603" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The quality of a fundus image can be compromised by numerous factors, many of
which are challenging to be appropriately and mathematically modeled. In this
paper, we introduce a novel diffusion model based framework, named Learning
Enhancement from Degradation (LED), for enhancing fundus images. Specifically,
we first adopt a data-driven degradation framework to learn degradation
mappings from unpaired high-quality to low-quality images. We then apply a
conditional diffusion model to learn the inverse enhancement process in a
paired manner. The proposed LED is able to output enhancement results that
maintain clinically important features with better clarity. Moreover, in the
inference phase, LED can be easily and effectively integrated with any existing
fundus image enhancement framework. We evaluate the proposed LED on several
downstream tasks with respect to various clinically-relevant metrics,
successfully demonstrating its superiority over existing state-of-the-art
methods both quantitatively and qualitatively. The source code is available at
https://github.com/QtacierP/LED.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 07, 2023 </span>    
         <span class="authors"> Cindy M. Nguyen, Eric R. Chan, Alexander W. Bergman, Gordon Wetzstein </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.04291" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Images are indispensable for the automation of high-level tasks, such as text
recognition. Low-light conditions pose a challenge for these high-level
perception stacks, which are often optimized on well-lit, artifact-free images.
Reconstruction methods for low-light images can produce well-lit counterparts,
but typically at the cost of high-frequency details critical for downstream
tasks. We propose Diffusion in the Dark (DiD), a diffusion model for low-light
image reconstruction that provides qualitatively competitive reconstructions
with that of SOTA, while preserving high-frequency details even in extremely
noisy, dark conditions. We demonstrate that DiD, without any task-specific
optimization, can outperform SOTA low-light methods in low-light text
recognition on real images, bolstering the potential of diffusion models for
ill-posed inverse problems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 07, 2023 </span>    
         <span class="authors"> David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, Eric Gu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.04248" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion models have demonstrated their proficiency for generative
sampling. However, generating good samples often requires many iterations.
Consequently, techniques such as binary time-distillation (BTD) have been
proposed to reduce the number of network calls for a fixed architecture. In
this paper, we introduce TRAnsitive Closure Time-distillation (TRACT), a new
method that extends BTD. For single step diffusion,TRACT improves FID by up to
2.4x on the same architecture, and achieves new single-step Denoising Diffusion
Implicit Models (DDIM) state-of-the-art FID (7.4 for ImageNet64, 3.8 for
CIFAR10). Finally we tease apart the method through extended ablations. The
PyTorch implementation will be released soon.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Patched Diffusion Models for Unsupervised Anomaly Detection in Brain MRI
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 07, 2023 </span>    
         <span class="authors"> Finn Behrendt, Debayan Bhattacharya, Julia Krüger, Roland Opfer, Alexander Schlaefer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.03758" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The use of supervised deep learning techniques to detect pathologies in brain
MRI scans can be challenging due to the diversity of brain anatomy and the need
for annotated data sets. An alternative approach is to use unsupervised anomaly
detection, which only requires sample-level labels of healthy brains to create
a reference representation. This reference representation can then be compared
to unhealthy brain anatomy in a pixel-wise manner to identify abnormalities. To
accomplish this, generative models are needed to create anatomically consistent
MRI scans of healthy brains. While recent diffusion models have shown promise
in this task, accurately generating the complex structure of the human brain
remains a challenge. In this paper, we propose a method that reformulates the
generation task of diffusion models as a patch-based estimation of healthy
brain anatomy, using spatial context to guide and improve reconstruction. We
evaluate our approach on data of tumors and multiple sclerosis lesions and
demonstrate a relative improvement of 25.1% compared to existing baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## 3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 06, 2023 </span>    
         <span class="authors"> Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, Jianzhu Ma </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.03543" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Rich data and powerful machine learning models allow us to design drugs for a
specific protein target \textit{in silico}. Recently, the inclusion of 3D
structures during targeted drug design shows superior performance to other
target-free models as the atomic interaction in the 3D space is explicitly
modeled. However, current 3D target-aware models either rely on the voxelized
atom densities or the autoregressive sampling process, which are not
equivariant to rotation or easily violate geometric constraints resulting in
unrealistic structures. In this work, we develop a 3D equivariant diffusion
model to solve the above challenges. To achieve target-aware molecule design,
our method learns a joint generative process of both continuous atom
coordinates and categorical atom types with a SE(3)-equivariant network.
Moreover, we show that our model can serve as an unsupervised feature extractor
to estimate the binding affinity under proper parameterization, which provides
an effective way for drug screening. To evaluate our model, we propose a
comprehensive framework to evaluate the quality of sampled molecules from
different dimensions. Empirical studies show our model could generate molecules
with more realistic 3D structures and better affinities towards the protein
targets, and improve binding affinity ranking and prediction without
retraining.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 06, 2023 </span>    
         <span class="authors"> Sitan Chen, Giannis Daras, Alexandros G. Dimakis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.03384" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.ST, stat.ML, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We develop a framework for non-asymptotic analysis of deterministic samplers
used for diffusion generative modeling. Several recent works have analyzed
stochastic samplers using tools like Girsanov's theorem and a chain rule
variant of the interpolation argument. Unfortunately, these techniques give
vacuous bounds when applied to deterministic samplers. We give a new
operational interpretation for deterministic sampling by showing that one step
along the probability flow ODE can be expressed as two steps: 1) a restoration
step that runs gradient ascent on the conditional log-likelihood at some
infinitesimally previous time, and 2) a degradation step that runs the forward
process using noise pointing back towards the current iterate. This perspective
allows us to extend denoising diffusion implicit models to general, non-linear
forward processes. We then develop the first polynomial convergence bounds for
these samplers under mild conditions on the data distribution.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## EEG Synthetic Data Generation Using Probabilistic Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 06, 2023 </span>    
         <span class="authors"> Giulio Tosato, Cesare M. Dalbagno, Francesco Fumagalli </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.06068" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.SP, cs.AI, cs.LG, q-bio.NC
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Electroencephalography (EEG) plays a significant role in the Brain Computer
Interface (BCI) domain, due to its non-invasive nature, low cost, and ease of
use, making it a highly desirable option for widespread adoption by the general
public. This technology is commonly used in conjunction with deep learning
techniques, the success of which is largely dependent on the quality and
quantity of data used for training. To address the challenge of obtaining
sufficient EEG data from individual participants while minimizing user effort
and maintaining accuracy, this study proposes an advanced methodology for data
augmentation: generating synthetic EEG data using denoising diffusion
probabilistic models. The synthetic data are generated from electrode-frequency
distribution maps (EFDMs) of emotionally labeled EEG recordings. To assess the
validity of the synthetic data generated, both a qualitative and a quantitative
comparison with real EEG data were successfully conducted. This study opens up
the possibility for an open\textendash source accessible and versatile toolbox
that can process and generate data in both time and frequency dimensions,
regardless of the number of channels involved. Finally, the proposed
methodology has potential implications for the broader field of neuroscience
research by enabling the creation of large, publicly available synthetic EEG
datasets without privacy concerns.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 04, 2023 </span>    
         <span class="authors"> Binxu Wang, John J. Vastola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.02490" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR, cs.NE, F.2.2; I.3.3; I.2.10; I.2.6
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 How do diffusion generative models convert pure noise into meaningful images?
We argue that generation involves first committing to an outline, and then to
finer and finer details. The corresponding reverse diffusion process can be
modeled by dynamics on a (time-dependent) high-dimensional landscape full of
Gaussian-like modes, which makes the following predictions: (i) individual
trajectories tend to be very low-dimensional; (ii) scene elements that vary
more within training data tend to emerge earlier; and (iii) early perturbations
substantially change image content more often than late perturbations. We show
that the behavior of a variety of trained unconditional and conditional
diffusion models like Stable Diffusion is consistent with these predictions.
Finally, we use our theory to search for the latent image manifold of diffusion
models, and propose a new way to generate interpretable image variations. Our
viewpoint suggests generation by GANs and diffusion models have unexpected
similarities.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unleashing Text-to-Image Diffusion Models for Visual Perception
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 03, 2023 </span>    
         <span class="authors"> Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, Jiwen Lu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.02153" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models (DMs) have become the new trend of generative models and
have demonstrated a powerful ability of conditional synthesis. Among those,
text-to-image diffusion models pre-trained on large-scale image-text pairs are
highly controllable by customizable prompts. Unlike the unconditional
generative models that focus on low-level attributes and details, text-to-image
diffusion models contain more high-level knowledge thanks to the
vision-language pre-training. In this paper, we propose VPD (Visual Perception
with a pre-trained Diffusion model), a new framework that exploits the semantic
information of a pre-trained text-to-image diffusion model in visual perception
tasks. Instead of using the pre-trained denoising autoencoder in a
diffusion-based pipeline, we simply use it as a backbone and aim to study how
to take full advantage of the learned knowledge. Specifically, we prompt the
denoising decoder with proper textual inputs and refine the text features with
an adapter, leading to a better alignment to the pre-trained stage and making
the visual contents interact with the text prompts. We also propose to utilize
the cross-attention maps between the visual features and the text features to
provide explicit guidance. Compared with other pre-training methods, we show
that vision-language pre-trained diffusion models can be faster adapted to
downstream visual perception tasks using the proposed VPD. Extensive
experiments on semantic segmentation, referring image segmentation and depth
estimation demonstrates the effectiveness of our method. Notably, VPD attains
0.254 RMSE on NYUv2 depth estimation and 73.3% oIoU on RefCOCO-val referring
image segmentation, establishing new records on these two benchmarks. Code is
available at https://github.com/wl-zhao/VPD
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models are Minimax Optimal Distribution Estimators
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 03, 2023 </span>    
         <span class="authors"> Kazusato Oko, Shunta Akiyama, Taiji Suzuki </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.01861" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While efficient distribution learning is no doubt behind the groundbreaking
success of diffusion modeling, its theoretical guarantees are quite limited. In
this paper, we provide the first rigorous analysis on approximation and
generalization abilities of diffusion modeling for well-known function spaces.
The highlight of this paper is that when the true density function belongs to
the Besov space and the empirical score matching loss is properly minimized,
the generated data distribution achieves the nearly minimax optimal estimation
rates in the total variation distance and in the Wasserstein distance of order
one. Furthermore, we extend our theory to demonstrate how diffusion models
adapt to low-dimensional data distributions. We expect these results advance
theoretical understandings of diffusion modeling and its ability to generate
verisimilar outputs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Deep Momentum Multi-Marginal Schrödinger Bridge
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 03, 2023 </span>    
         <span class="authors"> Tianrong Chen, Guan-Horng Liu, Molei Tao, Evangelos A. Theodorou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.01751" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 It is a crucial challenge to reconstruct population dynamics using unlabeled
samples from distributions at coarse time intervals. Recent approaches such as
flow-based models or Schr\"odinger Bridge (SB) models have demonstrated
appealing performance, yet the inferred sample trajectories either fail to
account for the underlying stochasticity or are $\underline{D}$eep
$\underline{M}$omentum Multi-Marginal $\underline{S}$chr\"odinger
$\underline{B}$ridge(DMSB), a novel computational framework that learns the
smooth measure-valued spline for stochastic systems that satisfy position
marginal constraints across time. By tailoring the celebrated Bregman Iteration
and extending the Iteration Proportional Fitting to phase space, we manage to
handle high-dimensional multi-marginal trajectory inference tasks efficiently.
Our algorithm outperforms baselines significantly, as evidenced by experiments
for synthetic datasets and a real-world single-cell RNA sequence dataset.
Additionally, the proposed approach can reasonably reconstruct the evolution of
velocity distribution, from position snapshots only, when there is a ground
truth velocity that is nevertheless inaccessible.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generative Diffusions in Augmented Spaces: A Complete Recipe
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 03, 2023 </span>    
         <span class="authors"> Kushagra Pandey, Stephan Mandt </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.01748" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based Generative Models (SGMs) have achieved state-of-the-art synthesis
results on diverse tasks. However, the current design space of the forward
diffusion process is largely unexplored and often relies on physical intuition
or simplifying assumptions. Leveraging results from the design of scalable
Bayesian posterior samplers, we present a complete recipe for constructing
forward processes in SGMs, all of which are guaranteed to converge to the
target distribution of interest. We show that several existing SGMs can be cast
as specific instantiations of this parameterization. Furthermore, building on
this recipe, we construct a novel SGM: Phase Space Langevin Diffusion (PSLD),
which performs score-based modeling in a space augmented with auxiliary
variables akin to a physical phase space. We show that PSLD outperforms
competing baselines in terms of sample quality and the speed-vs-quality
tradeoff across different samplers on various standard image synthesis
benchmarks. Moreover, we show that PSLD achieves sample quality comparable to
state-of-the-art SGMs (FID: 2.10 on unconditional CIFAR-10 generation),
providing an attractive alternative as an SGM backbone for further development.
We will publish our code and model checkpoints for reproducibility at
https://github.com/mandt-lab/PSLD.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Consistency Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 02, 2023 </span>    
         <span class="authors"> Yang Song, Prafulla Dhariwal, Mark Chen, Ilya Sutskever </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.01469" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have made significant breakthroughs in image, audio, and
video generation, but they depend on an iterative generation process that
causes slow sampling speed and caps their potential for real-time applications.
To overcome this limitation, we propose consistency models, a new family of
generative models that achieve high sample quality without adversarial
training. They support fast one-step generation by design, while still allowing
for few-step sampling to trade compute for sample quality. They also support
zero-shot data editing, like image inpainting, colorization, and
super-resolution, without requiring explicit training on these tasks.
Consistency models can be trained either as a way to distill pre-trained
diffusion models, or as standalone generative models. Through extensive
experiments, we demonstrate that they outperform existing distillation
techniques for diffusion models in one- and few-step generation. For example,
we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on
ImageNet 64x64 for one-step generation. When trained as standalone generative
models, consistency models also outperform single-step, non-adversarial
generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN
256x256.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Human Motion Diffusion as a Generative Prior
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 02, 2023 </span>    
         <span class="authors"> Yonatan Shafir, Guy Tevet, Roy Kapon, Amit H. Bermano </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.01418" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In recent months, we witness a leap forward as denoising diffusion models
were introduced to Motion Generation. Yet, the main gap in this field remains
the low availability of data. Furthermore, the expensive acquisition process of
motion biases the already modest data towards short single-person sequences.
With such a shortage, more elaborate generative tasks are left behind. In this
paper, we show that this gap can be mitigated using a pre-trained
diffusion-based model as a generative prior. We demonstrate the prior is
effective for fine-tuning, in a few-, and even a zero-shot manner. For the
zero-shot setting, we tackle the challenge of long sequence generation. We
introduce DoubleTake, an inference-time method with which we demonstrate up to
10-minute long animations of prompted intervals and their meaningful and
controlled transition, using the prior that was trained for 10-second
generations. For the few-shot setting, we consider two-person generation. Using
two fixed priors and as few as a dozen training examples, we learn a slim
communication block, ComMDM, to infuse interaction between the two resulting
motions. Finally, using fine-tuning, we train the prior to semantically
complete motions from a single prescribed joint. Then, we use our
DiffusionBlending to blend a few such models into a single one that responds
well to the combination of the individual control signals, enabling
fine-grained joint- and trajectory-level control and editing. Using an
off-the-shelf state-of-the-art (SOTA) motion diffusion model as a prior, we
evaluate our approach for the three mentioned cases and show that we
consistently outperform SOTA models that were designed and trained for those
tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Defending against Adversarial Audio via Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 02, 2023 </span>    
         <span class="authors"> Shutong Wu, Jiongxiao Wang, Wei Ping, Weili Nie, Chaowei Xiao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.01507" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.CR, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep learning models have been widely used in commercial acoustic systems in
recent years. However, adversarial audio examples can cause abnormal behaviors
for those acoustic systems, while being hard for humans to perceive. Various
methods, such as transformation-based defenses and adversarial training, have
been proposed to protect acoustic systems from adversarial attacks, but they
are less effective against adaptive attacks. Furthermore, directly applying the
methods from the image domain can lead to suboptimal results because of the
unique properties of audio data. In this paper, we propose an adversarial
purification-based defense pipeline, AudioPure, for acoustic systems via
off-the-shelf diffusion models. Taking advantage of the strong generation
ability of diffusion models, AudioPure first adds a small amount of noise to
the adversarial audio and then runs the reverse sampling step to purify the
noisy audio and recover clean audio. AudioPure is a plug-and-play method that
can be directly applied to any pretrained classifier without any fine-tuning or
re-training. We conduct extensive experiments on speech command recognition
task to evaluate the robustness of AudioPure. Our method is effective against
diverse adversarial attacks (e.g. $\mathcal{L}_2$ or
$\mathcal{L}_\infty$-norm). It outperforms the existing methods under both
strong adaptive white-box and black-box attacks bounded by $\mathcal{L}_2$ or
$\mathcal{L}_\infty$-norm (up to +20\% in robust accuracy). Besides, we also
evaluate the certified robustness for perturbations bounded by
$\mathcal{L}_2$-norm via randomized smoothing. Our pipeline achieves a higher
certified accuracy than baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Understanding the Diffusion Objective as a Weighted Integral of ELBOs
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 01, 2023 </span>    
         <span class="authors"> Diederik P. Kingma, Ruiqi Gao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.00848" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models in the literature are optimized with various objectives that
are special cases of a weighted loss, where the weighting function specifies
the weight per noise level. Uniform weighting corresponds to maximizing the
ELBO, a principled approximation of maximum likelihood. In current practice
diffusion models are optimized with non-uniform weighting due to better results
in terms of sample quality. In this work we expose a direct relationship
between the weighted loss (with any weighting) and the ELBO objective.
  We show that the weighted loss can be written as a weighted integral of
ELBOs, with one ELBO per noise level. If the weighting function is monotonic,
then the weighted loss is a likelihood-based objective: it maximizes the ELBO
under simple data augmentation, namely Gaussian noise perturbation. Our main
contribution is a deeper theoretical understanding of the diffusion objective,
but we also performed some experiments comparing monotonic with non-monotonic
weightings, finding that monotonic weighting performs competitively with the
best published results.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Continuous-Time Functional Diffusion Processes
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 01, 2023 </span>    
         <span class="authors"> Giulio Franzese, Simone Rossi, Dario Rossi, Markus Heinonen, Maurizio Filippone, Pietro Michiardi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.00800" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce functional diffusion processes (FDPs), which generalize
traditional score-based diffusion models to infinite-dimensional function
spaces. FDPs require a new mathematical framework to describe the forward and
backward dynamics, and several extensions to derive practical training
objectives. These include infinite-dimensional versions of the Girsanov
theorem, in order to be able to compute an ELBO, and of the sampling theorem,
in order to guarantee that functional evaluations in a countable set of points
are equivalent to infinite-dimensional functions. We use FDPs to build a new
breed of generative models in function spaces, which do not require specialized
network architectures, and that can work with any kind of continuous data. Our
results on synthetic and real data illustrate the advantages of FDPs in
simplifying the design requirements of diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unlimited-Size Diffusion Restoration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 01, 2023 </span>    
         <span class="authors"> Yinhuai Wang, Jiwen Yu, Runyi Yu, Jian Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.00354" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, using diffusion models for zero-shot image restoration (IR) has
become a new hot paradigm. This type of method only needs to use the
pre-trained off-the-shelf diffusion models, without any finetuning, and can
directly handle various IR tasks. The upper limit of the restoration
performance depends on the pre-trained diffusion models, which are in rapid
evolution. However, current methods only discuss how to deal with fixed-size
images, but dealing with images of arbitrary sizes is very important for
practical applications. This paper focuses on how to use those diffusion-based
zero-shot IR methods to deal with any size while maintaining the excellent
characteristics of zero-shot. A simple way to solve arbitrary size is to divide
it into fixed-size patches and solve each patch independently. But this may
yield significant artifacts since it neither considers the global semantics of
all patches nor the local information of adjacent patches. Inspired by the
Range-Null space Decomposition, we propose the Mask-Shift Restoration to
address local incoherence and propose the Hierarchical Restoration to alleviate
out-of-domain issues. Our simple, parameter-free approaches can be used not
only for image restoration but also for image generation of unlimited sizes,
with the potential to be a general tool for diffusion models. Code:
https://github.com/wyhuai/DDNM/tree/main/hq_demo
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Collage Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 01, 2023 </span>    
         <span class="authors"> Vishnu Sarukkai, Linden Li, Arden Ma, Christopher Ré, Kayvon Fatahalian </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.00262" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-conditional diffusion models generate high-quality, diverse images.
However, text is often an ambiguous specification for a desired target image,
creating the need for additional user-friendly controls for diffusion-based
image generation. We focus on having precise control over image output for
scenes with several objects. Users control image generation by defining a
collage: a text prompt paired with an ordered sequence of layers, where each
layer is an RGBA image and a corresponding text prompt. We introduce Collage
Diffusion, a collage-conditional diffusion algorithm that allows users to
control both the spatial arrangement and visual attributes of objects in the
scene, and also enables users to edit individual components of generated
images. To ensure that different parts of the input text correspond to the
various locations specified in the input collage layers, Collage Diffusion
modifies text-image cross-attention with the layers' alpha masks. To maintain
characteristics of individual collage layers that are not specified in text,
Collage Diffusion learns specialized text representations per layer. Collage
input also enables layer-based controls that provide fine-grained control over
the final output: users can control image harmonization on a layer-by-layer
basis, and they can edit individual objects in generated images while keeping
other objects fixed. Collage-conditional image generation requires harmonizing
the input collage to make objects fit together--the key challenge involves
minimizing changes in the positions and key visual attributes of objects in the
input collage while allowing other attributes of the collage to change in the
harmonization process. By leveraging the rich information present in layer
input, Collage Diffusion generates globally harmonized images that maintain
desired object locations and visual characteristics better than prior
approaches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Probabilistic Fields
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 01, 2023 </span>    
         <span class="authors"> Peiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M. Susskind, Miguel Ángel Bautista </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.00165" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models have quickly become a major approach for
generative modeling of images, 3D geometry, video and other domains. However,
to adapt diffusion generative modeling to these domains the denoising network
needs to be carefully designed for each domain independently, oftentimes under
the assumption that data lives in a Euclidean grid. In this paper we introduce
Diffusion Probabilistic Fields (DPF), a diffusion model that can learn
distributions over continuous functions defined over metric spaces, commonly
known as fields. We extend the formulation of diffusion probabilistic models to
deal with this field parametrization in an explicit way, enabling us to define
an end-to-end learning algorithm that side-steps the requirement of
representing fields with latent vectors as in previous approaches (Dupont et
al., 2022a; Du et al., 2021). We empirically show that, while using the same
denoising network, DPF effectively deals with different modalities like 2D
images and 3D geometry, in addition to modeling distributions over fields
defined on non-Euclidean metric spaces.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Monocular Depth Estimation using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 28, 2023 </span>    
         <span class="authors"> Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, David J. Fleet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.14816" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We formulate monocular depth estimation using denoising diffusion models,
inspired by their recent successes in high fidelity image generation. To that
end, we introduce innovations to address problems arising due to noisy,
incomplete depth maps in training data, including step-unrolled denoising
diffusion, an $L_1$ loss, and depth infilling during training. To cope with the
limited availability of data for supervised training, we leverage pre-training
on self-supervised image-to-image translation tasks. Despite the simplicity of
the approach, with a generic loss and architecture, our DepthGen model achieves
SOTA performance on the indoor NYU dataset, and near SOTA results on the
outdoor KITTI dataset. Further, with a multimodal posterior, DepthGen naturally
represents depth ambiguity (e.g., from transparent surfaces), and its zero-shot
performance combined with depth imputation, enable a simple but effective
text-to-3D pipeline. Project page: https://depth-gen.github.io
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 28, 2023 </span>    
         <span class="authors"> Bunlong Lay, Simon Welker, Julius Richter, Timo Gerkmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.14748" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, score-based generative models have been successfully employed for
the task of speech enhancement. A stochastic differential equation is used to
model the iterative forward process, where at each step environmental noise and
white Gaussian noise are added to the clean speech signal. While in limit the
mean of the forward process ends at the noisy mixture, in practice it stops
earlier and thus only at an approximation of the noisy mixture. This results in
a discrepancy between the terminating distribution of the forward process and
the prior used for solving the reverse process at inference. In this paper, we
address this discrepancy. To this end, we propose a forward process based on a
Brownian bridge and show that such a process leads to a reduction of the
mismatch compared to previous diffusion processes. More importantly, we show
that our approach improves in objective metrics over the baseline process with
only half of the iteration steps and having one hyperparameter less to tune.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 28, 2023 </span>    
         <span class="authors"> Hyemin Ahn, Esteve Valls Mascaro, Dongheui Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.14503" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 After many researchers observed fruitfulness from the recent diffusion
probabilistic model, its effectiveness in image generation is actively studied
these days. In this paper, our objective is to evaluate the potential of
diffusion probabilistic models for 3D human motion-related tasks. To this end,
this paper presents a study of employing diffusion probabilistic models to
predict future 3D human motion(s) from the previously observed motion. Based on
the Human 3.6M and HumanEva-I datasets, our results show that diffusion
probabilistic models are competitive for both single (deterministic) and
multiple (stochastic) 3D motion prediction tasks, after finishing a single
training process. In addition, we find out that diffusion probabilistic models
can offer an attractive compromise, since they can strike the right balance
between the likelihood and diversity of the predicted future motions. Our code
is publicly available on the project website:
https://sites.google.com/view/diffusion-motion-prediction.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Towards Enhanced Controllability of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 28, 2023 </span>    
         <span class="authors"> Wonwoong Cho, Hareesh Ravi, Midhun Harikumar, Vinh Khuc, Krishna Kumar Singh, Jingwan Lu, David I. Inouye, Ajinkya Kale </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.14368" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion models have shown remarkable capabilities in generating
realistic, high-quality and diverse images. However, the extent of
controllability during generation is underexplored. Inspired by techniques
based on GAN latent space for image manipulation, we train a diffusion model
conditioned on two latent codes, a spatial content mask and a flattened style
embedding. We rely on the inductive bias of the progressive denoising process
of diffusion models to encode pose/layout information in the spatial structure
mask and semantic/style information in the style code. We propose two generic
sampling techniques for improving controllability. We extend composable
diffusion models to allow for some dependence between conditional inputs, to
improve the quality of generations while also providing control over the amount
of guidance from each latent code and their joint distribution. We also propose
timestep dependent weight scheduling for content and style latents to further
improve the translations. We observe better controllability compared to
existing methods and show that without explicit training objectives, diffusion
models can be used for effective image manipulation and image translation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Differentially Private Diffusion Models Generate Useful Synthetic Images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 27, 2023 </span>    
         <span class="authors"> Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De, Samuel L. Smith, Olivia Wiles, Borja Balle </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.13861" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The ability to generate privacy-preserving synthetic versions of sensitive
image datasets could unlock numerous ML applications currently constrained by
data availability. Due to their astonishing image generation quality, diffusion
models are a prime candidate for generating high-quality synthetic data.
However, recent studies have found that, by default, the outputs of some
diffusion models do not preserve training data privacy. By privately
fine-tuning ImageNet pre-trained diffusion models with more than 80M
parameters, we obtain SOTA results on CIFAR-10 and Camelyon17 in terms of both
FID and the accuracy of downstream classifiers trained on synthetic data. We
decrease the SOTA FID on CIFAR-10 from 26.2 to 9.8, and increase the accuracy
from 51.0% to 88.0%. On synthetic data from Camelyon17, we achieve a downstream
accuracy of 91.1% which is close to the SOTA of 96.5% when training on the real
data. We leverage the ability of generative models to create infinite amounts
of data to maximise the downstream prediction performance, and further show how
to use synthetic data for hyperparameter tuning. Our results demonstrate that
diffusion models fine-tuned with differential privacy can produce useful and
provably private synthetic data, even in applications with significant
distribution shift between the pre-training and fine-tuning distributions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Samplers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 27, 2023 </span>    
         <span class="authors"> Francisco Vargas, Will Grathwohl, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.13834" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models are a popular class of generative models providing
state-of-the-art results in many domains. One adds gradually noise to data
using a diffusion to transform the data distribution into a Gaussian
distribution. Samples from the generative model are then obtained by simulating
an approximation of the time-reversal of this diffusion initialized by Gaussian
samples. Practically, the intractable score terms appearing in the
time-reversed process are approximated using score matching techniques. We
explore here a similar idea to sample approximately from unnormalized
probability density functions and estimate their normalizing constants. We
consider a process where the target density diffuses towards a Gaussian.
Denoising Diffusion Samplers (DDS) are obtained by approximating the
corresponding time-reversal. While score matching is not applicable in this
context, we can leverage many of the ideas introduced in generative modeling
for Monte Carlo sampling. Existing theoretical results from denoising diffusion
models also provide theoretical guarantees for DDS. We discuss the connections
between DDS, optimal control and Schr\"odinger bridges and finally demonstrate
DDS experimentally on a variety of challenging sampling tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 27, 2023 </span>    
         <span class="authors"> Jiyoung Lee, Joon Son Chung, Soo-Whan Chung </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.13700" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The goal of this work is zero-shot text-to-speech synthesis, with speaking
styles and voices learnt from facial characteristics. Inspired by the natural
fact that people can imagine the voice of someone when they look at his or her
face, we introduce a face-styled diffusion text-to-speech (TTS) model within a
unified framework learnt from visible attributes, called Face-TTS. This is the
first time that face images are used as a condition to train a TTS model.
  We jointly train cross-model biometrics and TTS models to preserve speaker
identity between face images and generated speech segments. We also propose a
speaker feature binding loss to enforce the similarity of the generated and the
ground truth speech segments in speaker embedding space. Since the biometric
information is extracted directly from the face image, our method does not
require extra fine-tuning steps to generate speech from unseen and unheard
speakers. We train and evaluate the model on the LRS3 dataset, an in-the-wild
audio-visual corpus containing background noise and diverse speaking styles.
The project page is https://facetts.github.io.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 26, 2023 </span>    
         <span class="authors"> Yifan Jiang, Han Chen, Hanseok Ko </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.13434" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, skeleton-based human action has become a hot research topic because
the compact representation of human skeletons brings new blood to this research
domain. As a result, researchers began to notice the importance of using RGB or
other sensors to analyze human action by extracting skeleton information.
Leveraging the rapid development of deep learning (DL), a significant number of
skeleton-based human action approaches have been presented with fine-designed
DL structures recently. However, a well-trained DL model always demands
high-quality and sufficient data, which is hard to obtain without costing high
expenses and human labor. In this paper, we introduce a novel data augmentation
method for skeleton-based action recognition tasks, which can effectively
generate high-quality and diverse sequential actions. In order to obtain
natural and realistic action sequences, we propose denoising diffusion
probabilistic models (DDPMs) that can generate a series of synthetic action
sequences, and their generation process is precisely guided by a
spatial-temporal transformer (ST-Trans). Experimental results show that our
method outperforms the state-of-the-art (SOTA) motion generation approaches on
different naturality and diversity metrics. It proves that its high-quality
synthetic data can also be effectively deployed to existing action recognition
models with significant performance improvement.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Model-Augmented Behavioral Cloning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 26, 2023 </span>    
         <span class="authors"> Hsiang-Chun Wang, Shang-Fu Chen, Shao-Hua Sun </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.13335" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.RO
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Imitation learning addresses the challenge of learning by observing an
expert's demonstrations without access to reward signals from the environment.
Behavioral cloning (BC) formulates imitation learning as a supervised learning
problem and learns from sampled state-action pairs. Despite its simplicity, it
often fails to capture the temporal structure of the task and the global
information of expert demonstrations. This work aims to augment BC by employing
diffusion models for modeling expert behaviors, and designing a learning
objective that leverages learned diffusion models to guide policy learning. To
this end, we propose diffusion model-augmented behavioral cloning
(Diffusion-BC) that combines our proposed diffusion model guided learning
objective with the BC objective, which complements each other. Our proposed
method outperforms baselines or achieves competitive performance in various
continuous control domains, including navigation, robot arm manipulation, and
locomotion. Ablation studies justify our design choices and investigate the
effect of balancing the BC and our proposed diffusion model objective.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Directed Diffusion: Direct Control of Object Placement through Attention Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 25, 2023 </span>    
         <span class="authors"> Wan-Duo Kurt Ma, J. P. Lewis, W. Bastiaan Kleijn, Thomas Leung </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.13153" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-guided diffusion models such as DALLE-2, IMAGEN, and Stable Diffusion
are able to generate an effectively endless variety of images given only a
short text prompt describing the desired image content. In many cases the
images are very high quality as well. However, these models often struggle to
compose scenes containing several key objects such as characters in specified
positional relationships. Unfortunately, this capability to ``direct'' the
placement of characters and objects both within and across images is crucial in
storytelling, as recognized in the literature on film and animation theory. In
this work we take a particularly straightforward approach to providing the
needed direction, by injecting ``activation'' at desired positions in the
cross-attention maps corresponding to the objects under control, while
attenuating the remainder of the map. The resulting approach is a step toward
generalizing the applicability of text-guided diffusion models beyond single
images to collections of related images, as in storybooks. To the best of our
knowledge, our Directed Diffusion method is the first diffusion technique that
provides positional control over multiple objects, while making use of an
existing pre-trained model and maintaining a coherent blend between the
positioned objects and the background. Moreover, it requires only a few lines
to implement.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising diffusion algorithm for inverse design of microstructures with fine-tuned nonlinear material properties
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 24, 2023 </span>    
         <span class="authors"> Nikolaos N. Vlassis, WaiChing Sun </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.12881" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we introduce a denoising diffusion algorithm to discover
microstructures with nonlinear fine-tuned properties. Denoising diffusion
probabilistic models are generative models that use diffusion-based dynamics to
gradually denoise images and generate realistic synthetic samples. By learning
the reverse of a Markov diffusion process, we design an artificial intelligence
to efficiently manipulate the topology of microstructures to generate a massive
number of prototypes that exhibit constitutive responses sufficiently close to
designated nonlinear constitutive responses. To identify the subset of
microstructures with sufficiently precise fine-tuned properties, a
convolutional neural network surrogate is trained to replace high-fidelity
finite element simulations to filter out prototypes outside the admissible
range. The results of this study indicate that the denoising diffusion process
is capable of creating microstructures of fine-tuned nonlinear material
properties within the latent space of the training data. More importantly, the
resulting algorithm can be easily extended to incorporate additional
topological and geometric modifications by introducing high-dimensional
structures embedded in the latent space. The algorithm is tested on the
open-source mechanical MNIST data set. Consequently, this algorithm is not only
capable of performing inverse design of nonlinear effective media but also
learns the nonlinear structure-property map to quantitatively understand the
multiscale interplay among the geometry and topology and their effective
macroscopic properties.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## To the Noise and Back: Diffusion for Shared Autonomy
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 23, 2023 </span>    
         <span class="authors"> Takuma Yoneda, Luzhe Sun, and Ge Yang, Bradly Stadie, Matthew Walter </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.12244" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.RO, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Shared autonomy is an operational concept in which a user and an autonomous
agent collaboratively control a robotic system. It provides a number of
advantages over the extremes of full-teleoperation and full-autonomy in many
settings. Traditional approaches to shared autonomy rely on knowledge of the
environment dynamics, a discrete space of user goals that is known a priori, or
knowledge of the user's policy -- assumptions that are unrealistic in many
domains. Recent works relax some of these assumptions by formulating shared
autonomy with model-free deep reinforcement learning (RL). In particular, they
no longer need knowledge of the goal space (e.g., that the goals are discrete
or constrained) or environment dynamics. However, they need knowledge of a
task-specific reward function to train the policy. Unfortunately, such reward
specification can be a difficult and brittle process. On top of that, the
formulations inherently rely on human-in-the-loop training, and that
necessitates them to prepare a policy that mimics users' behavior. In this
paper, we present a new approach to shared autonomy that employs a modulation
of the forward and reverse diffusion process of diffusion models. Our approach
does not assume known environment dynamics or the space of user goals, and in
contrast to previous work, it does not require any reward feedback, nor does it
require access to the user's policy during training. Instead, our framework
learns a distribution over a space of desired behaviors. It then employs a
diffusion model to translate the user's actions to a sample from this
distribution. Crucially, we show that it is possible to carry out this process
in a manner that preserves the user's control authority. We evaluate our
framework on a series of challenging continuous control tasks, and analyze its
ability to effectively correct user actions while maintaining their autonomy.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 23, 2023 </span>    
         <span class="authors"> Jamie Wynn, Daniyar Turmukhambetov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.12231" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive
results on novel view synthesis tasks. NeRFs learn a scene's color and density
fields by minimizing the photometric discrepancy between training views and
differentiable renders of the scene. Once trained from a sufficient set of
views, NeRFs can generate novel views from arbitrary camera positions. However,
the scene geometry and color fields are severely under-constrained, which can
lead to artifacts, especially when trained with few input views.
  To alleviate this problem we learn a prior over scene geometry and color,
using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of
the synthetic Hypersim dataset and can be used to predict the gradient of the
logarithm of a joint probability distribution of color and depth patches. We
show that, during NeRF training, these gradients of logarithms of RGBD patch
priors serve to regularize geometry and color for a scene. During NeRF
training, random RGBD patches are rendered and the estimated gradients of the
log-likelihood are backpropagated to the color and density fields. Evaluations
on LLFF, the most relevant dataset, show that our learned prior achieves
improved quality in the reconstructed geometry and improved generalization to
novel views. Evaluations on DTU show improved reconstruction quality among NeRF
methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Modeling Molecular Structures with Intrinsic Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 23, 2023 </span>    
         <span class="authors"> Gabriele Corso </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.12255" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Since its foundations, more than one hundred years ago, the field of
structural biology has strived to understand and analyze the properties of
molecules and their interactions by studying the structure that they take in 3D
space. However, a fundamental challenge with this approach has been the dynamic
nature of these particles, which forces us to model not a single but a whole
distribution of structures for every molecular system. This thesis proposes
Intrinsic Diffusion Modeling, a novel approach to this problem based on
combining diffusion generative models with scientific knowledge about the
flexibility of biological complexes. The knowledge of these degrees of freedom
is translated into the definition of a manifold over which the diffusion
process is defined. This manifold significantly reduces the dimensionality and
increases the smoothness of the generation space allowing for significantly
faster and more accurate generative processes. We demonstrate the effectiveness
of this approach on two fundamental tasks at the basis of computational
chemistry and biology: molecular conformer generation and molecular docking. In
both tasks, we construct the first deep learning method to outperform
traditional computational approaches achieving an unprecedented level of
accuracy for scalable programs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Aligned Diffusion Schrodinger Bridges
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 22, 2023 </span>    
         <span class="authors"> Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, Charlotte Bunne </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.11419" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, q-bio.QM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Schr\"odinger bridges (DSB) have recently emerged as a powerful
framework for recovering stochastic dynamics via their marginal observations at
different time points. Despite numerous successful applications, existing
algorithms for solving DSBs have so far failed to utilize the structure of
aligned data, which naturally arises in many biological phenomena. In this
paper, we propose a novel algorithmic framework that, for the first time,
solves DSBs while respecting the data alignment. Our approach hinges on a
combination of two decades-old ideas: The classical Schr\"odinger bridge theory
and Doob's $h$-transform. Compared to prior methods, our approach leads to a
simpler training procedure with lower variance, which we further augment with
principled regularization schemes. This ultimately leads to sizeable
improvements across experiments on synthetic and real data, including the tasks
of rigid protein docking and temporal evolution of cellular differentiation
processes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On Calibrating Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 21, 2023 </span>    
         <span class="authors"> Tianyu Pang, Cheng Lu, Chao Du, Min Lin, Shuicheng Yan, Zhijie Deng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10688" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion probabilistic models (DPMs) have achieved promising
results in diverse generative tasks. A typical DPM framework includes a forward
process that gradually diffuses the data distribution and a reverse process
that recovers the data distribution from time-dependent data scores. In this
work, we observe that the stochastic reverse process of data scores is a
martingale, from which concentration bounds and the optional stopping theorem
for data scores can be derived. Then, we discover a simple way for calibrating
an arbitrary pretrained DPM, with which the score matching loss can be reduced
and the lower bounds of model likelihood can consequently be increased. We
provide general calibration guidelines under various model parametrizations.
Our calibration method is performed only once and the resulting models can be
used repeatedly for sampling. We conduct experiments on multiple datasets to
empirically validate our proposal. Our code is at
https://github.com/thudzj/Calibrated-DPMs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 21, 2023 </span>    
         <span class="authors"> Luke Melas-Kyriazi, Christian Rupprecht, Andrea Vedaldi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10668" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Reconstructing the 3D shape of an object from a single RGB image is a
long-standing and highly challenging problem in computer vision. In this paper,
we propose a novel method for single-image 3D reconstruction which generates a
sparse point cloud via a conditional denoising diffusion process. Our method
takes as input a single RGB image along with its camera pose and gradually
denoises a set of 3D points, whose positions are initially sampled randomly
from a three-dimensional Gaussian distribution, into the shape of an object.
The key to our method is a geometrically-consistent conditioning process which
we call projection conditioning: at each step in the diffusion process, we
project local image features onto the partially-denoised point cloud from the
given camera pose. This projection conditioning process enables us to generate
high-resolution sparse geometries that are well-aligned with the input image,
and can additionally be used to predict point colors after shape
reconstruction. Moreover, due to the probabilistic nature of the diffusion
process, our method is naturally capable of generating multiple different
shapes consistent with a single input image. In contrast to prior work, our
approach not only performs well on synthetic benchmarks, but also gives large
qualitative improvements on complex real-world data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 21, 2023 </span>    
         <span class="authors"> Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10586" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a three-stage training strategy called dual pseudo training (DPT)
for conditional image generation and classification in semi-supervised
learning. First, a classifier is trained on partially labeled data and predicts
pseudo labels for all data. Second, a conditional generative model is trained
on all data with pseudo labels and generates pseudo images given labels.
Finally, the classifier is trained on real data augmented by pseudo images with
labels. We demonstrate large-scale diffusion models and semi-supervised
learners benefit mutually with a few labels via DPT. In particular, on the
ImageNet 256x256 generation benchmark, DPT can generate realistic, diverse, and
semantically correct images with very few labels. With two (i.e., < 0.2%) and
five (i.e., < 0.4%) labels per class, DPT achieves an FID of 3.44 and 3.37
respectively, outperforming strong diffusion models with full labels, such as
IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised
baselines substantially on ImageNet classification benchmarks with one, two,
and five labels per class, achieving state-of-the-art top-1 accuracies of 59.0
(+2.8), 69.5 (+3.0), and 73.6 (+1.2) respectively.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Probabilistic Models for Graph-Structured Prediction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 21, 2023 </span>    
         <span class="authors"> Hyosoon Jang, Sangwoo Mo, Sungsoo Ahn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10506" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper studies graph-structured prediction for supervised learning on
graphs with node-wise or edge-wise target dependencies. To solve this problem,
recent works investigated combining graph neural networks (GNNs) with
conventional structured prediction algorithms like conditional random fields.
However, in this work, we pursue an alternative direction building on the
recent successes of diffusion probabilistic models (DPMs). That is, we propose
a new framework using DPMs to make graph-structured predictions. In the fully
supervised setting, our DPM captures the target dependencies by iteratively
updating each target estimate based on the estimates of nearby targets. We also
propose a variational expectation maximization algorithm to train our DPM in
the semi-supervised setting. Extensive experiments verify that our framework
consistently outperforms existing neural structured prediction models on
inductive and transductive node classification. We also demonstrate the
competitive performance of our framework for algorithmic reasoning tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning Gradually Non-convex Image Priors Using Score Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 21, 2023 </span>    
         <span class="authors"> Erich Kobler, Thomas Pock </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10502" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, I.2.6; I.4.10
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we propose a unified framework of denoising score-based models
in the context of graduated non-convex energy minimization. We show that for
sufficiently large noise variance, the associated negative log density -- the
energy -- becomes convex. Consequently, denoising score-based models
essentially follow a graduated non-convexity heuristic. We apply this framework
to learning generalized Fields of Experts image priors that approximate the
joint density of noisy images and their associated variances. These priors can
be easily incorporated into existing optimization algorithms for solving
inverse problems and naturally implement a fast and robust graduated
non-convexity mechanism.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Infinite-Dimensional Diffusion Models for Function Spaces
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 20, 2023 </span>    
         <span class="authors"> Jakiw Pidstrigach, Youssef Marzouk, Sebastian Reich, Sven Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10130" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, math.PR, 68T99, 60Hxx
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We define diffusion-based generative models in infinite dimensions, and apply
them to the generative modeling of functions. By first formulating such models
in the infinite-dimensional limit and only then discretizing, we are able to
obtain a sampling algorithm that has \emph{dimension-free} bounds on the
distance from the sample measure to the target measure. Furthermore, we propose
a new way to perform conditional sampling in an infinite-dimensional space and
show that our approach outperforms previously suggested procedures.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 20, 2023 </span>    
         <span class="authors"> Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, Ravi Ramamoorthi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10109" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Novel view synthesis from a single image requires inferring occluded regions
of objects and scenes whilst simultaneously maintaining semantic and physical
consistency with the input. Existing approaches condition neural radiance
fields (NeRF) on local image features, projecting points to the input image
plane, and aggregating 2D features to perform volume rendering. However, under
severe occlusion, this projection fails to resolve uncertainty, resulting in
blurry renderings that lack details. In this work, we propose NerfDiff, which
addresses this issue by distilling the knowledge of a 3D-aware conditional
diffusion model (CDM) into NeRF through synthesizing and refining a set of
virtual views at test time. We further propose a novel NeRF-guided distillation
algorithm that simultaneously generates 3D consistent virtual views from the
CDM samples, and finetunes the NeRF based on the improved virtual views. Our
approach significantly outperforms existing NeRF-based and geometry-free
approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 20, 2023 </span>    
         <span class="authors"> Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Mingxuan Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10025" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While diffusion models have achieved great success in generating continuous
signals such as images and audio, it remains elusive for diffusion models in
learning discrete sequence data like natural languages. Although recent
advances circumvent this challenge of discreteness by embedding discrete tokens
as continuous surrogates, they still fall short of satisfactory generation
quality. To understand this, we first dive deep into the denoised training
protocol of diffusion-based sequence generative models and determine their
three severe problems, i.e., 1) failing to learn, 2) lack of scalability, and
3) neglecting source conditions. We argue that these problems can be boiled
down to the pitfall of the not completely eliminated discreteness in the
embedding space, and the scale of noises is decisive herein. In this paper, we
introduce DINOISER to facilitate diffusion models for sequence generation by
manipulating noises. We propose to adaptively determine the range of sampled
noise scales for counter-discreteness training; and encourage the proposed
diffused sequence learner to leverage source conditions with amplified noise
scales during inference. Experiments show that DINOISER enables consistent
improvement over the baselines of previous diffusion-based sequence generative
models on several conditional sequence modeling benchmarks thanks to both
effective training and inference strategies. Analyses further verify that
DINOISER can make better use of source conditions to govern its generative
process.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Restoration based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 20, 2023 </span>    
         <span class="authors"> Jaemoo Choi, Yesom Park, Myungjoo Kang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2303.05456" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models (DDMs) have recently attracted increasing
attention by showing impressive synthesis quality. DDMs are built on a
diffusion process that pushes data to the noise distribution and the models
learn to denoise. In this paper, we establish the interpretation of DDMs in
terms of image restoration (IR). Integrating IR literature allows us to use an
alternative objective and diverse forward processes, not confining to the
diffusion process. By imposing prior knowledge on the loss function grounded on
MAP-based estimation, we eliminate the need for the expensive sampling of DDMs.
Also, we propose a multi-scale training, which improves the performance
compared to the diffusion process, by taking advantage of the flexibility of
the forward process. Experimental results demonstrate that our model improves
the quality and efficiency of both training and inference. Furthermore, we show
the applicability of our model to inverse problems. We believe that our
framework paves the way for designing a new type of flexible general generative
model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 17, 2023 </span>    
         <span class="authors"> Giannis Daras, Yuval Dagan, Alexandros G. Dimakis, Constantinos Daskalakis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.09057" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, cs.IT, math.IT
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Imperfect score-matching leads to a shift between the training and the
sampling distribution of diffusion models. Due to the recursive nature of the
generation process, errors in previous steps yield sampling iterates that drift
away from the training distribution. Yet, the standard training objective via
Denoising Score Matching (DSM) is only designed to optimize over non-drifted
data. To train on drifted data, we propose to enforce a \emph{consistency}
property which states that predictions of the model on its own generated data
are consistent across time. Theoretically, we show that if the score is learned
perfectly on some non-drifted points (via DSM) and if the consistency property
is enforced everywhere, then the score is learned accurately everywhere.
Empirically we show that our novel training objective yields state-of-the-art
results for conditional and unconditional generation in CIFAR-10 and baseline
improvements in AFHQ and FFHQ. We open-source our code and models:
https://github.com/giannisdaras/cdm
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Text-driven Visual Synthesis with Latent Diffusion Prior
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 16, 2023 </span>    
         <span class="authors"> Ting-Hsuan Liao, Songwei Ge, Yiran Xu, Yao-Chih Lee, Badour AlBahar, Jia-Bin Huang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.08510" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 There has been tremendous progress in large-scale text-to-image synthesis
driven by diffusion models enabling versatile downstream applications such as
3D object synthesis from texts, image editing, and customized generation. We
present a generic approach using latent diffusion models as powerful image
priors for various visual synthesis tasks. Existing methods that utilize such
priors fail to use these models' full capabilities. To improve this, our core
ideas are 1) a feature matching loss between features from different layers of
the decoder to provide detailed guidance and 2) a KL divergence loss to
regularize the predicted latent features and stabilize the training. We
demonstrate the efficacy of our approach on three different applications,
text-to-3D, StyleGAN adaptation, and layered image editing. Extensive results
show our method compares favorably against baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 16, 2023 </span>    
         <span class="authors"> Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.08453" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, cs.MM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The incredible generative ability of large-scale text-to-image (T2I) models
has demonstrated strong power of learning complex structures and meaningful
semantics. However, relying solely on text prompts cannot fully take advantage
of the knowledge learned by the model, especially when flexible and accurate
controlling (e.g., color and structure) is needed. In this paper, we aim to
``dig out" the capabilities that T2I models have implicitly learned, and then
explicitly use them to control the generation more granularly. Specifically, we
propose to learn simple and lightweight T2I-Adapters to align internal
knowledge in T2I models with external control signals, while freezing the
original large T2I models. In this way, we can train various adapters according
to different conditions, achieving rich control and editing effects in the
color and structure of the generation results. Further, the proposed
T2I-Adapters have attractive properties of practical value, such as
composability and generalization ability. Extensive experiments demonstrate
that our T2I-Adapter has promising generation quality and a wide range of
applications.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Explicit Diffusion of Gaussian Mixture Model Based Image Priors
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 16, 2023 </span>    
         <span class="authors"> Martin Zach, Thomas Pock, Erich Kobler, Antonin Chambolle </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.08411" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work we tackle the problem of estimating the density $f_X$ of a
random variable $X$ by successive smoothing, such that the smoothed random
variable $Y$ fulfills $(\partial_t - \Delta_1)f_Y(\,\cdot\,, t) = 0$,
$f_Y(\,\cdot\,, 0) = f_X$. With a focus on image processing, we propose a
product/fields of experts model with Gaussian mixture experts that admits an
analytic expression for $f_Y (\,\cdot\,, t)$ under an orthogonality constraint
on the filters. This construction naturally allows the model to be trained
simultaneously over the entire diffusion horizon using empirical Bayes. We show
preliminary results on image denoising where our model leads to competitive
results while being tractable, interpretable, and having only a small number of
learnable parameters. As a byproduct, our model can be used for reliable noise
estimation, allowing blind denoising of images corrupted by heteroscedastic
noise.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 16, 2023 </span>    
         <span class="authors"> Henry Kvinge, Davis Brown, Charles Godfrey </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.09301" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Prompting has become an important mechanism by which users can more
effectively interact with many flavors of foundation model. Indeed, the last
several years have shown that well-honed prompts can sometimes unlock emergent
capabilities within such models. While there has been a substantial amount of
empirical exploration of prompting within the community, relatively few works
have studied prompting at a mathematical level. In this work we aim to take a
first step towards understanding basic geometric properties induced by prompts
in Stable Diffusion, focusing on the intrinsic dimension of internal
representations within the model. We find that choice of prompt has a
substantial impact on the intrinsic dimension of representations at both layers
of the model which we explored, but that the nature of this impact depends on
the layer being considered. For example, in certain bottleneck layers of the
model, intrinsic dimension of representations is correlated with prompt
perplexity (measured using a surrogate model), while this correlation is not
apparent in the latent layers. Our evidence suggests that intrinsic dimension
could be a useful tool for future studies of the impact of different prompts on
text-to-image models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 16, 2023 </span>    
         <span class="authors"> Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, Mu Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.08908" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Layout-to-image generation refers to the task of synthesizing photo-realistic
images based on semantic layouts. In this paper, we propose LayoutDiffuse that
adapts a foundational diffusion model pretrained on large-scale image or
text-image datasets for layout-to-image generation. By adopting a novel neural
adaptor based on layout attention and task-aware prompts, our method trains
efficiently, generates images with both high perceptual quality and layout
alignment, and needs less data. Experiments on three datasets show that our
method significantly outperforms other 10 generative models based on GANs,
VQ-VAE, and diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 16, 2023 </span>    
         <span class="authors"> Omer Bar-Tal, Lior Yariv, Yaron Lipman, Tali Dekel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.08113" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advances in text-to-image generation with diffusion models present
transformative capabilities in image quality. However, user controllability of
the generated image, and fast adaptation to new tasks still remains an open
challenge, currently mostly addressed by costly and long re-training and
fine-tuning or ad-hoc adaptations to specific image generation tasks. In this
work, we present MultiDiffusion, a unified framework that enables versatile and
controllable image generation, using a pre-trained text-to-image diffusion
model, without any further training or finetuning. At the center of our
approach is a new generation process, based on an optimization task that binds
together multiple diffusion generation processes with a shared set of
parameters or constraints. We show that MultiDiffusion can be readily applied
to generate high quality and diverse images that adhere to user-provided
controls, such as desired aspect ratio (e.g., panorama), and spatial guiding
signals, ranging from tight segmentation masks to bounding boxes. Project
webpage: https://multidiffusion.github.io
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PRedItOR: Text Guided Image Editing with Diffusion Prior
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 15, 2023 </span>    
         <span class="authors"> Hareesh Ravi, Sachin Kelkar, Midhun Harikumar, Ajinkya Kale </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.07979" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown remarkable capabilities in generating high
quality and creative images conditioned on text. An interesting application of
such models is structure preserving text guided image editing. Existing
approaches rely on text conditioned diffusion models such as Stable Diffusion
or Imagen and require compute intensive optimization of text embeddings or
fine-tuning the model weights for text guided image editing. We explore text
guided image editing with a Hybrid Diffusion Model (HDM) architecture similar
to DALLE-2. Our architecture consists of a diffusion prior model that generates
CLIP image embedding conditioned on a text prompt and a custom Latent Diffusion
Model trained to generate images conditioned on CLIP image embedding. We
discover that the diffusion prior model can be used to perform text guided
conceptual edits on the CLIP image embedding space without any finetuning or
optimization. We combine this with structure preserving edits on the image
decoder using existing approaches such as reverse DDIM to perform text guided
image editing. Our approach, PRedItOR does not require additional inputs,
fine-tuning, optimization or objectives and shows on par or better results than
baselines qualitatively and quantitatively. We provide further analysis and
understanding of the diffusion prior model and believe this opens up new
possibilities in diffusion models research.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 15, 2023 </span>    
         <span class="authors"> Hshmat Sahak, Daniel Watson, Chitwan Saharia, David Fleet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.07864" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown promising results on single-image
super-resolution and other image- to-image translation tasks. Despite this
success, they have not outperformed state-of-the-art GAN models on the more
challenging blind super-resolution task, where the input images are out of
distribution, with unknown degradations. This paper introduces SR3+, a
diffusion-based model for blind super-resolution, establishing a new
state-of-the-art. To this end, we advocate self-supervised training with a
combination of composite, parameterized degradations for self-supervised
training, and noise-conditioing augmentation during training and testing. With
these innovations, a large-scale convolutional architecture, and large-scale
datasets, SR3+ greatly outperforms SR3. It outperforms Real-ESRGAN when trained
on the same data, with a DRealSR FID score of 36.82 vs. 37.22, which further
improves to FID of 32.37 with larger models, and further still with larger
training sets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Video Probabilistic Diffusion Models in Projected Latent Space
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 15, 2023 </span>    
         <span class="authors"> Sihyun Yu, Kihyuk Sohn, Subin Kim, Jinwoo Shin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.07685" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite the remarkable progress in deep generative models, synthesizing
high-resolution and temporally coherent videos still remains a challenge due to
their high-dimensionality and complex temporal dynamics along with large
spatial variations. Recent works on diffusion models have shown their potential
to solve this challenge, yet they suffer from severe computation- and
memory-inefficiency that limit the scalability. To handle this issue, we
propose a novel generative model for videos, coined projected latent video
diffusion models (PVDM), a probabilistic diffusion model which learns a video
distribution in a low-dimensional latent space and thus can be efficiently
trained with high-resolution videos under limited resources. Specifically, PVDM
is composed of two components: (a) an autoencoder that projects a given video
as 2D-shaped latent vectors that factorize the complex cubic structure of video
pixels and (b) a diffusion model architecture specialized for our new
factorized latent space and the training/sampling procedure to synthesize
videos of arbitrary length with a single model. Experiments on popular video
generation datasets demonstrate the superiority of PVDM compared with previous
video synthesis methods; e.g., PVDM obtains the FVD score of 639.7 on the
UCF-101 long video (128 frames) generation benchmark, which improves 1773.4 of
the prior state-of-the-art.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Diffusion Models in Function Space
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 14, 2023 </span>    
         <span class="authors"> Jae Hyun Lim, Nikola B. Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, Christopher Pal, Arash Vahdat, Anima Anandkumar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.07400" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.FA, stat.ML, 46B09 (Primary), 60J22 (Secondary), I.2.6; J.2
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently emerged as a powerful framework for generative
modeling. They consist of a forward process that perturbs input data with
Gaussian white noise and a reverse process that learns a score function to
generate samples by denoising. Despite their tremendous success, they are
mostly formulated on finite-dimensional spaces, e.g. Euclidean, limiting their
applications to many domains where the data has a functional form such as in
scientific computing and 3D geometric data analysis. In this work, we introduce
a mathematically rigorous framework called Denoising Diffusion Operators (DDOs)
for training diffusion models in function space. In DDOs, the forward process
perturbs input functions gradually using a Gaussian process. The generative
process is formulated by integrating a function-valued Langevin dynamic. Our
approach requires an appropriate notion of the score for the perturbed data
distribution, which we obtain by generalizing denoising score matching to
function spaces that can be infinite-dimensional. We show that the
corresponding discretized algorithm generates accurate samples at a fixed cost
that is independent of the data resolution. We theoretically and numerically
verify the applicability of our approach on a set of problems, including
generating solutions to the Navier-Stokes equation viewed as the push-forward
distribution of forcings from a Gaussian Random Field (GRF).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 14, 2023 </span>    
         <span class="authors"> Axi Niu, Kang Zhang, Trung X. Pham, Jinqiu Sun, Yu Zhu, In So Kweon, Yanning Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.12831" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPM) have been widely adopted in
image-to-image translation to generate high-quality images. Prior attempts at
applying the DPM to image super-resolution (SR) have shown that iteratively
refining a pure Gaussian noise with a conditional image using a U-Net trained
on denoising at various-level noises can help obtain a satisfied
high-resolution image for the low-resolution one. To further improve the
performance and simplify current DPM-based super-resolution methods, we propose
a simple but non-trivial DPM-based super-resolution post-process
framework,i.e., cDPMSR. After applying a pre-trained SR model on the to-be-test
LR image to provide the conditional input, we adapt the standard DPM to conduct
conditional image generation and perform super-resolution through a
deterministic iterative denoising process. Our method surpasses prior attempts
on both qualitative and quantitative results and can generate more
photo-realistic counterparts for the low-resolution images with various
benchmark datasets including Set5, Set14, Urban100, BSD100, and Manga109. Code
will be published after accepted.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Reparameterized Discrete Diffusion Model for Text Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 13, 2023 </span>    
         <span class="authors"> Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, Aleksander Madry </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.06588" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present an approach to mitigating the risks of malicious image editing
posed by large diffusion models. The key idea is to immunize images so as to
make them resistant to manipulation by these models. This immunization relies
on injection of imperceptible adversarial perturbations designed to disrupt the
operation of the targeted diffusion models, forcing them to generate
unrealistic images. We provide two methods for crafting such perturbations, and
then demonstrate their efficacy. Finally, we discuss a policy component
necessary to make our approach fully effective and practical -- one that
involves the organizations developing diffusion models, rather than individual
users, to implement (and support) the immunization process.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Raising the Cost of Malicious AI-Powered Image Editing
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 13, 2023 </span>    
         <span class="authors"> Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, Aleksander Madry </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.06588" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present an approach to mitigating the risks of malicious image editing
posed by large diffusion models. The key idea is to immunize images so as to
make them resistant to manipulation by these models. This immunization relies
on injection of imperceptible adversarial perturbations designed to disrupt the
operation of the targeted diffusion models, forcing them to generate
unrealistic images. We provide two methods for crafting such perturbations, and
then demonstrate their efficacy. Finally, we discuss a policy component
necessary to make our approach fully effective and practical -- one that
involves the organizations developing diffusion models, rather than individual
users, to implement (and support) the immunization process.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Preconditioned Score-based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 13, 2023 </span>    
         <span class="authors"> Li Zhang, Hengyuan Ma, Xiatian Zhu, Jianfeng Feng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.06504" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
sampling process is slow due to a need for many (\eg, $2000$) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
assault this problem to the ill-conditioned issues of the Langevin dynamics and
reverse diffusion in the sampling process. Under this insight, we propose a
model-agnostic {\bf\em preconditioned diffusion sampling} (PDS) method that
leverages matrix preconditioning to alleviate the aforementioned problem. PDS
alters the sampling process of a vanilla SGM at marginal extra computation
cost, and without model retraining. Theoretically, we prove that PDS preserves
the output distribution of the SGM, no risk of inducing systematical bias to
the original sampling process. We further theoretically reveal a relation
between the parameter of PDS and the sampling iterations,easing the parameter
estimation under varying sampling iterations. Extensive experiments on various
image datasets with a variety of resolutions and diversity validate that our
PDS consistently accelerates off-the-shelf SGMs whilst maintaining the
synthesis quality. In particular, PDS can accelerate by up to $29\times$ on
more challenging high resolution (1024$\times$1024) image generation. Compared
with the latest generative models (\eg, CLD-SGM, DDIM, and Analytic-DDIM), PDS
can achieve the best sampling quality on CIFAR-10 at a FID score of 1.99. Our
code is made publicly available to foster any further research
https://github.com/fudan-zvg/PDS.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models in Bioinformatics: A New Wave of Deep Learning Revolution in Action
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 13, 2023 </span>    
         <span class="authors"> Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, Jianlin Cheng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10907" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, q-bio.QM, I.2.1; J.3
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models have emerged as one of the most powerful
generative models in recent years. They have achieved remarkable success in
many fields, such as computer vision, natural language processing (NLP), and
bioinformatics. Although there are a few excellent reviews on diffusion models
and their applications in computer vision and NLP, there is a lack of an
overview of their applications in bioinformatics. This review aims to provide a
rather thorough overview of the applications of diffusion models in
bioinformatics to aid their further development in bioinformatics and
computational biology. We start with an introduction of the key concepts and
theoretical foundations of three cornerstone diffusion modeling frameworks
(denoising diffusion probabilistic models, noise-conditioned scoring networks,
and stochastic differential equations), followed by a comprehensive description
of diffusion models employed in the different domains of bioinformatics,
including cryo-EM data enhancement, single-cell data analysis, protein design
and generation, drug and small molecule design, and protein-ligand interaction.
The review is concluded with a summary of the potential new development and
applications of diffusion models in bioinformatics.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Single Motion Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 12, 2023 </span>    
         <span class="authors"> Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H. Bermano, Daniel Cohen-Or </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.05905" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Synthesizing realistic animations of humans, animals, and even imaginary
creatures, has long been a goal for artists and computer graphics
professionals. Compared to the imaging domain, which is rich with large
available datasets, the number of data instances for the motion domain is
limited, particularly for the animation of animals and exotic creatures (e.g.,
dragons), which have unique skeletons and motion patterns. In this work, we
present a Single Motion Diffusion Model, dubbed SinMDM, a model designed to
learn the internal motifs of a single motion sequence with arbitrary topology
and synthesize motions of arbitrary length that are faithful to them. We
harness the power of diffusion models and present a denoising network designed
specifically for the task of learning from a single input motion. Our
transformer-based architecture avoids overfitting by using local attention
layers that narrow the receptive field, and encourages motion diversity by
using relative positional embedding. SinMDM can be applied in a variety of
contexts, including spatial and temporal in-betweening, motion expansion, style
transfer, and crowd animation. Our results show that SinMDM outperforms
existing methods both in quality and time-space efficiency. Moreover, while
current approaches require additional training for different applications, our
work facilitates these applications at inference time. Our code and trained
models are available at https://sinmdm.github.io/SinMDM-page.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SB: Image-to-Image Schrödinger Bridge
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 12, 2023 </span>    
         <span class="authors"> Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, Anima Anandkumar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.05872" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class of
conditional diffusion models that directly learn the nonlinear diffusion
processes between two given distributions. These diffusion bridges are
particularly useful for image restoration, as the degraded images are
structurally informative priors for reconstructing the clean images. I$^2$SB
belongs to a tractable class of Schr\"odinger bridge, the nonlinear extension
to score-based models, whose marginal distributions can be computed
analytically given boundary pairs. This results in a simulation-free framework
for nonlinear diffusions, where the I$^2$SB training becomes scalable by
adopting practical techniques used in standard diffusion models. We validate
I$^2$SB in solving various image restoration tasks, including inpainting,
super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show
that I$^2$SB surpasses standard conditional diffusion models with more
interpretable generative processes. Moreover, I$^2$SB matches the performance
of inverse methods that additionally require the knowledge of the corruption
operators. Our work opens up new algorithmic opportunities for developing
efficient nonlinear diffusion models on a large scale. scale. Project page and
codes: https://i2sb.github.io/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Adding Conditional Control to Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 10, 2023 </span>    
         <span class="authors"> Lvmin Zhang, Maneesh Agrawala </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.05543" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR, cs.HC, cs.MM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a neural network structure, ControlNet, to control pretrained
large diffusion models to support additional input conditions. The ControlNet
learns task-specific conditions in an end-to-end way, and the learning is
robust even when the training dataset is small (< 50k). Moreover, training a
ControlNet is as fast as fine-tuning a diffusion model, and the model can be
trained on a personal devices. Alternatively, if powerful computation clusters
are available, the model can scale to large amounts (millions to billions) of
data. We report that large diffusion models like Stable Diffusion can be
augmented with ControlNets to enable conditional inputs like edge maps,
segmentation maps, keypoints, etc. This may enrich the methods to control large
diffusion models and further facilitate related applications.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Star-Shaped Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 10, 2023 </span>    
         <span class="authors"> Andrey Okhotin, Dmitry Molchanov, Vladimir Arkhipkin, Grigory Bartosh, Aibek Alanov, Dmitry Vetrov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.05259" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Methods based on Denoising Diffusion Probabilistic Models (DDPM) became a
ubiquitous tool in generative modeling. However, they are mostly limited to
Gaussian and discrete diffusion processes. We propose Star-Shaped Denoising
Diffusion Probabilistic Models (SS-DDPM), a model with a non-Markovian
diffusion-like noising process. In the case of Gaussian distributions, this
model is equivalent to Markovian DDPMs. However, it can be defined and applied
with arbitrary noising distributions, and admits efficient training and
sampling algorithms for a wide range of distributions that lie in the
exponential family. We provide a simple recipe for designing diffusion-like
models with distributions like Beta, von Mises--Fisher, Dirichlet, Wishart and
others, which can be especially useful when data lies on a constrained manifold
such as the unit sphere, the space of positive semi-definite matrices, the
probabilistic simplex, etc. We evaluate the model in different settings and
find it competitive even on image data, where Beta SS-DDPM achieves results
comparable to a Gaussian DDPM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Example-Based Sampling with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 10, 2023 </span>    
         <span class="authors"> Bastien Doignies, Nicolas Bonneel, David Coeurjolly, Julie Digne, Loïs Paulin, Jean-Claude Iehl, Victor Ostromoukhov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.05116" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.GR, cs.CV, cs.LG, I.3.7; I.5.1; I.6.8; G.1.4
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Much effort has been put into developing samplers with specific properties,
such as producing blue noise, low-discrepancy, lattice or Poisson disk samples.
These samplers can be slow if they rely on optimization processes, may rely on
a wide range of numerical methods, are not always differentiable. The success
of recent diffusion models for image generation suggests that these models
could be appropriate for learning how to generate point sets from examples.
However, their convolutional nature makes these methods impractical for dealing
with scattered data such as point sets. We propose a generic way to produce 2-d
point sets imitating existing samplers from observed point sets using a
diffusion model. We address the problem of convolutional layers by leveraging
neighborhood information from an optimal transport matching to a uniform grid,
that allows us to benefit from fast convolutions on grids, and to support the
example-based learning of non-uniform sampling patterns. We demonstrate how the
differentiability of our approach can be used to optimize point sets to enforce
properties.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 09, 2023 </span>    
         <span class="authors"> Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.04867" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPMs) have demonstrated a very promising
ability in high-resolution image synthesis. However, sampling from a
pre-trained DPM usually requires hundreds of model evaluations, which is
computationally expensive. Despite recent progress in designing high-order
solvers for DPMs, there still exists room for further speedup, especially in
extremely few steps (e.g., 5~10 steps). Inspired by the predictor-corrector for
ODE solvers, we develop a unified corrector (UniC) that can be applied after
any existing DPM sampler to increase the order of accuracy without extra model
evaluations, and derive a unified predictor (UniP) that supports arbitrary
order as a byproduct. Combining UniP and UniC, we propose a unified
predictor-corrector framework called UniPC for the fast sampling of DPMs, which
has a unified analytical form for any order and can significantly improve the
sampling quality over previous methods. We evaluate our methods through
extensive experiments including both unconditional and conditional sampling
using pixel-space and latent-space DPMs. Our UniPC can achieve 3.87 FID on
CIFAR10 (unconditional) and 7.51 FID on ImageNet 256$\times$256 (conditional)
with only 10 function evaluations. Code is available at
https://github.com/wl-zhao/UniPC
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Better Diffusion Models Further Improve Adversarial Training
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 09, 2023 </span>    
         <span class="authors"> Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, Shuicheng Yan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.04638" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 It has been recognized that the data generated by the denoising diffusion
probabilistic model (DDPM) improves adversarial training. After two years of
rapid development in diffusion models, a question naturally arises: can better
diffusion models further improve adversarial training? This paper gives an
affirmative answer by employing the most recent diffusion model which has
higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID
score) compared with DDPM. Our adversarially trained models achieve
state-of-the-art performance on RobustBench using only generated data (no
external datasets). Under the $\ell_\infty$-norm threat model with
$\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on
CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous
state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm
threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10
($+4.44\%$). These results also beat previous works that use external data. Our
code is available at https://github.com/wzekai99/DM-Improves-AT.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Geometry of Score Based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 09, 2023 </span>    
         <span class="authors"> Sandesh Ghimire, Jinyang Liu, Armand Comas, Davin Hill, Aria Masoomi, Octavia Camps, Jennifer Dy </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.04411" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we look at Score-based generative models (also called diffusion
generative models) from a geometric perspective. From a new view point, we
prove that both the forward and backward process of adding noise and generating
from noise are Wasserstein gradient flow in the space of probability measures.
We are the first to prove this connection. Our understanding of Score-based
(and Diffusion) generative models have matured and become more complete by
drawing ideas from different fields like Bayesian inference, control theory,
stochastic differential equation and Schrodinger bridge. However, many open
questions and challenges remain. One problem, for example, is how to decrease
the sampling time? We demonstrate that looking from geometric perspective
enables us to answer many of these questions and provide new interpretations to
some known results. Furthermore, geometric perspective enables us to devise an
intuitive geometric solution to the problem of faster sampling. By augmenting
traditional score-based generative models with a projection step, we show that
we can generate high quality images with significantly fewer sampling-steps.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 08, 2023 </span>    
         <span class="authors"> Huan He, Shifan Zhao, Yuanzhe Xi, Joyce C Ho </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.04355" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Due to patient privacy protection concerns, machine learning research in
healthcare has been undeniably slower and limited than in other application
domains. High-quality, realistic, synthetic electronic health records (EHRs)
can be leveraged to accelerate methodological developments for research
purposes while mitigating privacy concerns associated with data sharing. The
current state-of-the-art model for synthetic EHR generation is generative
adversarial networks, which are notoriously difficult to train and can suffer
from mode collapse. Denoising Diffusion Probabilistic Models, a class of
generative models inspired by statistical thermodynamics, have recently been
shown to generate high-quality synthetic samples in certain domains. It is
unknown whether these can generalize to generation of large-scale,
high-dimensional EHRs. In this paper, we present a novel generative model based
on diffusion models that is the first successful application on electronic
health records. Our model proposes a mechanism to perform class-conditional
sampling to preserve label information. We also introduce a new sampling
strategy to accelerate the inference speed. We empirically show that our model
outperforms existing state-of-the-art synthetic EHR generation methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Geometry-Complete Diffusion for 3D Molecule Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 08, 2023 </span>    
         <span class="authors"> Alex Morehead, Jianlin Cheng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.04313" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, q-bio.BM, q-bio.QM, stat.ML, I.2.1; J.3
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) have recently taken the
field of generative modeling by storm, pioneering new state-of-the-art results
in disciplines such as computer vision and computational biology for diverse
tasks ranging from text-guided image generation to structure-guided protein
design. Along this latter line of research, methods such as those of Hoogeboom
et al. 2022 have been proposed for generating 3D molecules using equivariant
graph neural networks (GNNs) within a DDPM framework. Toward this end, we
propose GCDM, a geometry-complete diffusion model that achieves new
state-of-the-art results for 3D molecule diffusion generation and optimization
by leveraging the representation learning strengths offered by GNNs that
perform geometry-complete message-passing. Our results with GCDM also offer
preliminary insights into how physical inductive biases impact the generative
dynamics of molecular DDPMs. The source code, data, and instructions to train
new models or reproduce our results are freely available at
https://github.com/BioinfoMachineLearning/Bio-Diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Q-Diffusion: Quantizing Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 08, 2023 </span>    
         <span class="authors"> Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, Kurt Keutzer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.04304" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have achieved great success in synthesizing diverse and
high-fidelity images. However, sampling speed and memory constraints remain a
major barrier to the practical adoption of diffusion models, since the
generation process for these models can be slow due to the need for iterative
noise estimation using compute-intensive neural networks. We propose to tackle
this problem by compressing the noise estimation network to accelerate the
generation process through post-training quantization (PTQ). While existing PTQ
approaches have not been able to effectively deal with the changing output
distributions of noise estimation networks in diffusion models over multiple
time steps, we are able to formulate a PTQ method that is specifically designed
to handle the unique multi-timestep structure of diffusion models with a data
calibration scheme using data sampled from different time steps. Experimental
results show that our proposed method is able to directly quantize
full-precision diffusion models into 8-bit or 4-bit models while maintaining
comparable performance in a training-free manner, achieving a FID change of at
most 1.88. Our approach can also be applied to text-guided image generation,
and for the first time we can run stable diffusion in 4-bit weights without
losing much perceptual quality, as shown in Figure 5 and Figure 9.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 08, 2023 </span>    
         <span class="authors"> Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.04265" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce a new family of physics-inspired generative models termed PFGM++
that unifies diffusion models and Poisson Flow Generative Models (PFGM). These
models realize generative trajectories for $N$ dimensional data by embedding
paths in $N{+}D$ dimensional space while still controlling the progression with
a simple scalar norm of the $D$ additional variables. The new models reduce to
PFGM when $D{=}1$ and to diffusion models when $D{\to}\infty$. The flexibility
of choosing $D$ allows us to trade off robustness against rigidity as
increasing $D$ results in more concentrated coupling between the data and the
additional variable norms. We dispense with the biased large batch field
targets used in PFGM and instead provide an unbiased perturbation-based
objective similar to diffusion models. To explore different choices of $D$, we
provide a direct alignment method for transferring well-tuned hyperparameters
from diffusion models ($D{\to} \infty$) to any finite $D$ values. Our
experiments show that models with finite $D$ can be superior to previous
state-of-the-art diffusion models on CIFAR-10/FFHQ $64{\times}64$ datasets,
with FID scores of $1.91/2.43$ when $D{=}2048/128$. In class-conditional
setting, $D{=}2048$ yields current state-of-the-art FID of $1.74$ on CIFAR-10.
In addition, we demonstrate that models with smaller $D$ exhibit improved
robustness against modeling errors. Code is available at
https://github.com/Newbeeer/pfgmpp
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Noise2Music: Text-conditioned Music Generation with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 08, 2023 </span>    
         <span class="authors"> Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, Jesse Engel, Quoc V. Le, William Chan, Zhifeng Chen, Wei Han </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03917" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce Noise2Music, where a series of diffusion models is trained to
generate high-quality 30-second music clips from text prompts. Two types of
diffusion models, a generator model, which generates an intermediate
representation conditioned on text, and a cascader model, which generates
high-fidelity audio conditioned on the intermediate representation and possibly
the text, are trained and utilized in succession to generate high-fidelity
music. We explore two options for the intermediate representation, one using a
spectrogram and the other using audio with lower fidelity. We find that the
generated audio is not only able to faithfully reflect key elements of the text
prompt such as genre, tempo, instruments, mood, and era, but goes beyond to
ground fine-grained semantics of the prompt. Pretrained large language models
play a key role in this story -- they are used to generate paired text for the
audio of the training set and to extract embeddings of the text prompts
ingested by the diffusion models.
  Generated examples: https://google-research.github.io/noise2music
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 08, 2023 </span>    
         <span class="authors"> Hyeonho Jeong, Gihyun Kwon, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03900" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advancements in large scale text-to-image models have opened new
possibilities for guiding the creation of images through human-devised natural
language. However, while prior literature has primarily focused on the
generation of individual images, it is essential to consider the capability of
these models to ensure coherency within a sequence of images to fulfill the
demands of real-world applications such as storytelling. To address this, here
we present a novel neural pipeline for generating a coherent storybook from the
plain text of a story. Specifically, we leverage a combination of a pre-trained
Large Language Model and a text-guided Latent Diffusion Model to generate
coherent images. While previous story synthesis frameworks typically require a
large-scale text-to-image model trained on expensive image-caption pairs to
maintain the coherency, we employ simple textual inversion techniques along
with detector-based semantic image editing which allows zero-shot generation of
the coherent storybook. Experimental results show that our proposed method
outperforms state-of-the-art image editing baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Information-Theoretic Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 07, 2023 </span>    
         <span class="authors"> Xianghao Kong, Rob Brekelmans, Greg Ver Steeg </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03792" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.IT, math.IT
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models have spurred significant gains in density modeling
and image generation, precipitating an industrial revolution in text-guided AI
art generation. We introduce a new mathematical foundation for diffusion models
inspired by classic results in information theory that connect Information with
Minimum Mean Square Error regression, the so-called I-MMSE relations. We
generalize the I-MMSE relations to exactly relate the data distribution to an
optimal denoising regression problem, leading to an elegant refinement of
existing diffusion bounds. This new insight leads to several improvements for
probability distribution estimation, including theoretical justification for
diffusion model ensembling. Remarkably, our framework shows how continuous and
discrete probabilities can be learned with the same regression objective,
avoiding domain-specific generative models used in variational methods. Code to
reproduce experiments is provided at http://github.com/kxh001/ITdiffusion and
simplified demonstration code is at
http://github.com/gregversteeg/InfoDiffusionSimple.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control  
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 07, 2023 </span>    
         <span class="authors"> Jacopo Teneggi, Matt Tivnan, J Webster Stayman, Jeremias Sulam </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03791" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative modeling, informally referred to as diffusion models,
continue to grow in popularity across several important domains and tasks.
While they provide high-quality and diverse samples from empirical
distributions, important questions remain on the reliability and
trustworthiness of these sampling procedures for their responsible use in
critical scenarios. Conformal prediction is a modern tool to construct
finite-sample, distribution-free uncertainty guarantees for any black-box
predictor. In this work, we focus on image-to-image regression tasks and we
present a generalization of the Risk-Controlling Prediction Sets (RCPS)
procedure, that we term $K$-RCPS, which allows to $(i)$ provide entrywise
calibrated intervals for future samples of any diffusion model, and $(ii)$
control a certain notion of risk with respect to a ground truth image with
minimal mean interval length. Differently from existing conformal risk control
procedures, ours relies on a novel convex optimization approach that allows for
multidimensional risk control while provably minimizing the mean interval
length. We illustrate our approach on two real-world image denoising problems:
on natural images of faces as well as on computed tomography (CT) scans of the
abdomen, demonstrating state of the art performance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GraphGUIDE: interpretable and controllable conditional graph generation with discrete Bernoulli diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 07, 2023 </span>    
         <span class="authors"> Alex M. Tseng, Nathaniel Diamant, Tommaso Biancalani, Gabriele Scalia </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03790" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models achieve state-of-the-art performance in generating realistic
objects and have been successfully applied to images, text, and videos. Recent
work has shown that diffusion can also be defined on graphs, including graph
representations of drug-like molecules. Unfortunately, it remains difficult to
perform conditional generation on graphs in a way which is interpretable and
controllable. In this work, we propose GraphGUIDE, a novel framework for graph
generation using diffusion models, where edges in the graph are flipped or set
at each discrete time step. We demonstrate GraphGUIDE on several graph
datasets, and show that it enables full control over the conditional generation
of arbitrary structural properties without relying on predefined labels. Our
framework for graph diffusion can have a large impact on the interpretable
conditional generation of graphs, including the generation of drug-like
molecules with desired properties in a way which is informed by experimental
evidence.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Effective Data Augmentation With Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 07, 2023 </span>    
         <span class="authors"> Brandon Trabucco, Kyle Doherty, Max Gurinas, Ruslan Salakhutdinov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.07944" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Data augmentation is one of the most prevalent tools in deep learning,
underpinning many recent advances, including those from classification,
generative models, and representation learning. The standard approach to data
augmentation combines simple transformations like rotations and flips to
generate new images from existing ones. However, these new images lack
diversity along key semantic axes present in the data. Current augmentations
cannot alter the high-level semantic attributes, such as animal species present
in a scene, to enhance the diversity of data. We address the lack of diversity
in data augmentation with image-to-image transformations parameterized by
pre-trained text-to-image diffusion models. Our method edits images to change
their semantics using an off-the-shelf diffusion model, and generalizes to
novel visual concepts from a few labelled examples. We evaluate our approach on
few-shot image classification tasks, and on a real-world weed recognition task,
and observe an improvement in accuracy in tested domains.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Long Horizon Temperature Scaling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 07, 2023 </span>    
         <span class="authors"> Andy Shih, Dorsa Sadigh, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03686" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Temperature scaling is a popular technique for tuning the sharpness of a
model distribution. It is used extensively for sampling likely generations and
calibrating model uncertainty, and even features as a controllable parameter to
many large language models in deployment. However, autoregressive models rely
on myopic temperature scaling that greedily optimizes the next token. To
address this, we propose Long Horizon Temperature Scaling (LHTS), a novel
approach for sampling from temperature-scaled joint distributions. LHTS is
compatible with all likelihood-based models, and optimizes for the long-horizon
likelihood of samples. We derive a temperature-dependent LHTS objective, and
show that fine-tuning a model on a range of temperatures produces a single
model capable of generation with a controllable long-horizon temperature
parameter. We experiment with LHTS on image diffusion models and
character/language autoregressive models, demonstrating advantages over myopic
temperature scaling in likelihood and sample quality, and showing improvements
in accuracy on a multiple choice analogy task by $10\%$.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 07, 2023 </span>    
         <span class="authors"> Felix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha Luccioni, Kristian Kersting </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.10893" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, cs.CY, cs.HC
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative AI models have recently achieved astonishing results in quality
and are consequently employed in a fast-growing number of applications.
However, since they are highly data-driven, relying on billion-sized datasets
randomly scraped from the internet, they also suffer from degenerated and
biased human behavior, as we demonstrate. In fact, they may even reinforce such
biases. To not only uncover but also combat these undesired effects, we present
a novel strategy, called Fair Diffusion, to attenuate biases after the
deployment of generative text-to-image models. Specifically, we demonstrate
shifting a bias, based on human instructions, in any direction yielding
arbitrarily new proportions for, e.g., identity groups. As our empirical
evaluation demonstrates, this introduced control enables instructing generative
image models on fairness, with no data filtering and additional training
required.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Riemannian Flow Matching on General Geometries
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 07, 2023 </span>    
         <span class="authors"> Ricky T. Q. Chen, Yaron Lipman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03660" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose Riemannian Flow Matching (RFM), a simple yet powerful framework
for training continuous normalizing flows on manifolds. Existing methods for
generative modeling on manifolds either require expensive simulation, are
inherently unable to scale to high dimensions, or use approximations for
limiting quantities that result in biased training objectives. Riemannian Flow
Matching bypasses these limitations and offers several advantages over previous
approaches: it is simulation-free on simple geometries, does not require
divergence computation, and computes its target vector field in closed-form.
The key ingredient behind RFM is the construction of a relatively simple
premetric for defining target vector fields, which encompasses the existing
Euclidean case. To extend to general geometries, we rely on the use of spectral
decompositions to efficiently compute premetrics on the fly. Our method
achieves state-of-the-art performance on real-world non-Euclidean datasets, and
we demonstrate tractable training on general geometries, including triangular
meshes with highly non-trivial curvature and boundaries.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Graph Generation with Destination-Driven Diffusion Mixture
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 07, 2023 </span>    
         <span class="authors"> Jaehyeong Jo, Dongki Kim, Sung Ju Hwang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03596" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generation of graphs is a major challenge for real-world tasks that require
understanding the complex nature of their non-Euclidean structures. Although
diffusion models have achieved notable success in graph generation recently,
they are ill-suited for modeling the structural information of graphs since
learning to denoise the noisy samples does not explicitly capture the graph
topology. To tackle this limitation, we propose a novel generative framework
that models the topology of graphs by predicting the destination of the
diffusion process, which is the original graph that has the correct topology
information, as a weighted mean of data. Specifically, we design the generative
process as a mixture of diffusion processes conditioned on the endpoint in the
data distribution, which drives the process toward the predicted destination,
resulting in rapid convergence. We introduce new simulation-free training
objectives for predicting the destination, and further discuss the advantages
of our framework that can explicitly model the graph topology and exploit the
inductive bias of the data. Through extensive experimental validation on
general graph and 2D/3D molecule generation tasks, we show that our method
outperforms previous generative models, generating graphs with correct topology
with both continuous (e.g. 3D coordinates) and discrete (e.g. atom types)
features.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 06, 2023 </span>    
         <span class="authors"> Tiange Xiang, Mahmut Yurt, Ali B Syed, Kawin Setsompop, Akshay Chaudhari </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03018" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Magnetic resonance imaging (MRI) is a common and life-saving medical imaging
technique. However, acquiring high signal-to-noise ratio MRI scans requires
long scan times, resulting in increased costs and patient discomfort, and
decreased throughput. Thus, there is great interest in denoising MRI scans,
especially for the subtype of diffusion MRI scans that are severely
SNR-limited. While most prior MRI denoising methods are supervised in nature,
acquiring supervised training datasets for the multitude of anatomies, MRI
scanners, and scan parameters proves impractical. Here, we propose Denoising
Diffusion Models for Denoising Diffusion MRI (DDM$^2$), a self-supervised
denoising method for MRI denoising using diffusion denoising generative models.
Our three-stage framework integrates statistic-based denoising theory into
diffusion models and performs denoising through conditional generation. During
inference, we represent input noisy measurements as a sample from an
intermediate posterior distribution within the diffusion Markov chain. We
conduct experiments on 4 real-world in-vivo diffusion MRI datasets and show
that our DDM$^2$ demonstrates superior denoising performances ascertained with
clinically-relevant visual qualitative and quantitative metrics.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Structure and Content-Guided Video Synthesis with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 06, 2023 </span>    
         <span class="authors"> Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.03011" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-guided generative diffusion models unlock powerful image creation and
editing tools. While these have been extended to video generation, current
approaches that edit the content of existing footage while retaining structure
require expensive re-training for every input or rely on error-prone
propagation of image edits across frames. In this work, we present a structure
and content-guided video diffusion model that edits videos based on visual or
textual descriptions of the desired output. Conflicts between user-provided
content edits and structure representations occur due to insufficient
disentanglement between the two aspects. As a solution, we show that training
on monocular depth estimates with varying levels of detail provides control
over structure and content fidelity. Our model is trained jointly on images and
videos which also exposes explicit control of temporal consistency through a
novel guidance method. Our experiments demonstrate a wide variety of successes;
fine-grained control over output characteristics, customization based on a few
reference images, and a strong user preference towards results by our model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generative Diffusion Models on Graphs: Methods and Applications
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 06, 2023 </span>    
         <span class="authors"> Chengyi Liu, Wenqi Fan, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, Qing Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02591" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.SI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models, as a novel generative paradigm, have achieved remarkable
success in various image generation tasks such as image inpainting,
image-to-text translation, and video generation. Graph generation is a crucial
computational task on graphs with numerous real-world applications. It aims to
learn the distribution of given graphs and then generate new graphs. Given the
great success of diffusion models in image generation, increasing efforts have
been made to leverage these techniques to advance graph generation in recent
years. In this paper, we first provide a comprehensive overview of generative
diffusion models on graphs, In particular, we review representative algorithms
for three variants of graph diffusion models, i.e., Score Matching with
Langevin Dynamics (SMLD), Denoising Diffusion Probabilistic Model (DDPM), and
Score-based Generative Model (SGM). Then, we summarize the major applications
of generative diffusion models on graphs with a specific focus on molecule and
protein modeling. Finally, we discuss promising directions in generative
diffusion models on graph-structured data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Mixture of Diffusers for scene composition and high resolution image generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2023 </span>    
         <span class="authors"> Álvaro Barbero Jiménez </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02412" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, I.2.6
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion methods have been proven to be very effective to generate images
while conditioning on a text prompt. However, and although the quality of the
generated images is unprecedented, these methods seem to struggle when trying
to generate specific image compositions. In this paper we present Mixture of
Diffusers, an algorithm that builds over existing diffusion models to provide a
more detailed control over composition. By harmonizing several diffusion
processes acting on different regions of a canvas, it allows generating larger
images, where the location of each object and style is controlled by a separate
diffusion process.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Model for Generative Image Denoising
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2023 </span>    
         <span class="authors"> Yutong Xie, Minne Yuan, Bin Dong, Quanzheng Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02398" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In supervised learning for image denoising, usually the paired clean images
and noisy images are collected or synthesised to train a denoising model. L2
norm loss or other distance functions are used as the objective function for
training. It often leads to an over-smooth result with less image details. In
this paper, we regard the denoising task as a problem of estimating the
posterior distribution of clean images conditioned on noisy images. We apply
the idea of diffusion model to realize generative image denoising. According to
the noise model in denoising tasks, we redefine the diffusion process such that
it is different from the original one. Hence, the sampling of the posterior
distribution is a reverse process of dozens of steps from the noisy image. We
consider three types of noise model, Gaussian, Gamma and Poisson noise. With
the guarantee of theory, we derive a unified strategy for model training. Our
method is verified through experiments on three types of noise models and
achieves excellent performance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Eliminating Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2023 </span>    
         <span class="authors"> Zuopeng Yang, Tianshu Chu, Xin Lin, Erdun Gao, Daqing Liu, Jie Yang, Chaoyue Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02394" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The recent success of text-to-image generation diffusion models has also
revolutionized semantic image editing, enabling the manipulation of images
based on query/target texts. Despite these advancements, a significant
challenge lies in the potential introduction of prior bias in pre-trained
models during image editing, e.g., making unexpected modifications to
inappropriate regions. To this point, we present a novel Dual-Cycle Diffusion
model that addresses the issue of prior bias by generating an unbiased mask as
the guidance of image editing. The proposed model incorporates a Bias
Elimination Cycle that consists of both a forward path and an inverted path,
each featuring a Structural Consistency Cycle to ensure the preservation of
image content during the editing process. The forward path utilizes the
pre-trained model to produce the edited image, while the inverted path converts
the result back to the source image. The unbiased mask is generated by
comparing differences between the processed source image and the edited image
to ensure that both conform to the same distribution. Our experiments
demonstrate the effectiveness of the proposed method, as it significantly
improves the D-CLIP score from 0.272 to 0.283. The code will be available at
https://github.com/JohnDreamer/DualCycleDiffsion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2023 </span>    
         <span class="authors"> Zijian Zhang, Zhou Zhao, Jun Yu, Qi Tian </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02373" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently exhibited remarkable abilities to synthesize
striking image samples since the introduction of denoising diffusion
probabilistic models (DDPMs). Their key idea is to disrupt images into noise
through a fixed forward process and learn its reverse process to generate
samples from noise in a denoising way. For conditional DDPMs, most existing
practices relate conditions only to the reverse process and fit it to the
reversal of unconditional forward process. We find this will limit the
condition modeling and generation in a small time window. In this paper, we
propose a novel and flexible conditional diffusion model by introducing
conditions into the forward process. We utilize extra latent space to allocate
an exclusive diffusion trajectory for each condition based on some shifting
rules, which will disperse condition modeling to all timesteps and improve the
learning capacity of model. We formulate our method, which we call
\textbf{ShiftDDPMs}, and provide a unified point of view on existing related
methods. Extensive qualitative and quantitative experiments on image synthesis
demonstrate the feasibility and effectiveness of ShiftDDPMs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2023 </span>    
         <span class="authors"> Kexun Zhang, Xianjun Yang, William Yang Wang, Lei Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02285" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models show promising generation capability for a variety of data.
Despite their high generation quality, the inference for diffusion models is
still time-consuming due to the numerous sampling iterations required. To
accelerate the inference, we propose ReDi, a simple yet learning-free
Retrieval-based Diffusion sampling framework. From a precomputed knowledge
base, ReDi retrieves a trajectory similar to the partially generated trajectory
at an early stage of generation, skips a large portion of intermediate steps,
and continues sampling from a later step in the retrieved trajectory. We
theoretically prove that the generation performance of ReDi is guaranteed. Our
experiments demonstrate that ReDi improves the model inference efficiency by 2x
speedup. Furthermore, ReDi is able to generalize well in zero-shot cross-domain
image generation such as image stylization.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2023 </span>    
         <span class="authors"> Shiqi Sun, Shancheng Fang, Qian He, Wei Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02284" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are able to generate photorealistic images in arbitrary
scenes. However, when applying diffusion models to image translation, there
exists a trade-off between maintaining spatial structure and high-quality
content. Besides, existing methods are mainly based on test-time optimization
or fine-tuning model for each input image, which are extremely time-consuming
for practical applications. To address these issues, we propose a new approach
for flexible image translation by learning a layout-aware image condition
together with a text condition. Specifically, our method co-encodes images and
text into a new domain during the training phase. In the inference stage, we
can choose images/text or both as the conditions for each time step, which
gives users more flexible control over layout and content. Experimental
comparisons of our method with state-of-the-art methods demonstrate our model
performs best in both style image translation and semantic image translation
and took the shortest time.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SE(3) diffusion model with application to protein backbone generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2023 </span>    
         <span class="authors"> Jason Yim, Brian L. Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02277" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, q-bio.QM, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The design of novel protein structures remains a challenge in protein
engineering for applications across biomedicine and chemistry. In this line of
work, a diffusion model over rigid bodies in 3D (referred to as frames) has
shown success in generating novel, functional protein backbones that have not
been observed in nature. However, there exists no principled methodological
framework for diffusion on SE(3), the space of orientation preserving rigid
motions in R3, that operates on frames and confers the group invariance. We
address these shortcomings by developing theoretical foundations of SE(3)
invariant diffusion models on multiple frames followed by a novel framework,
FrameDiff, for learning the SE(3) equivariant score over multiple frames. We
apply FrameDiff on monomer backbone generation and find it can generate
designable monomers up to 500 amino acids without relying on a pretrained
protein structure prediction network that has been integral to previous
methods. We find our samples are capable of generalizing beyond any known
protein structure.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Divide and Compose with Score Based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2023 </span>    
         <span class="authors"> Sandesh Ghimire, Armand Comas, Davin Hill, Aria Masoomi, Octavia Camps, Jennifer Dy </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02272" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While score based generative models, or diffusion models, have found success
in image synthesis, they are often coupled with text data or image label to be
able to manipulate and conditionally generate images. Even though manipulation
of images by changing the text prompt is possible, our understanding of the
text embedding and our ability to modify it to edit images is quite limited.
Towards the direction of having more control over image manipulation and
conditional generation, we propose to learn image components in an unsupervised
manner so that we can compose those components to generate and manipulate
images in informed manner. Taking inspiration from energy based models, we
interpret different score components as the gradient of different energy
functions. We show how score based learning allows us to learn interesting
components and we can visualize them through generation. We also show how this
novel decomposition allows us to compose, generate and modify images in
interesting ways akin to dreaming. We make our code available at
https://github.com/sandeshgh/Score-based-disentanglement
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Multi-Source Diffusion Models for Simultaneous Music Generation and Separation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 04, 2023 </span>    
         <span class="authors"> Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, Emanuele Rodolà </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.02257" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we define a diffusion-based generative model capable of both
music synthesis and source separation by learning the score of the joint
probability density of sources sharing a context. Alongside the classic total
inference tasks (i.e. generating a mixture, separating the sources), we also
introduce and experiment on the partial inference task of source imputation,
where we generate a subset of the sources given the others (e.g., play a piano
track that goes well with the drums). Additionally, we introduce a novel
inference method for the separation task. We train our model on Slakh2100, a
standard dataset for musical source separation, provide qualitative results in
the generation settings, and showcase competitive quantitative results in the
separation setting. Our method is the first example of a single model that can
handle both generation and separation tasks, thus representing a step toward
general audio models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 03, 2023 </span>    
         <span class="authors"> Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, Ping Luo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.01877" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have demonstrated their powerful generative capability in
many tasks, with great potential to serve as a paradigm for offline
reinforcement learning. However, the quality of the diffusion model is limited
by the insufficient diversity of training data, which hinders the performance
of planning and the generalizability to new tasks. This paper introduces
AdaptDiffuser, an evolutionary planning method with diffusion that can
self-evolve to improve the diffusion model hence a better planner, not only for
seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the
generation of rich synthetic expert data for goal-conditioned tasks using
guidance from reward gradients. It then selects high-quality data via a
discriminator to finetune the diffusion model, which improves the
generalization ability to unseen tasks. Empirical experiments on two benchmark
environments and two carefully designed unseen tasks in KUKA industrial robot
arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For
example, AdaptDiffuser not only outperforms the previous art Diffuser by 20.8%
on Maze2D and 7.5% on MuJoCo locomotion, but also adapts better to new tasks,
e.g., KUKA pick-and-place, by 27.9% without requiring additional expert data.
More visualization results and demo videos could be found on our project page.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MorDIFF: Recognition Vulnerability and Attack Detectability of Face Morphing Attacks Created by Diffusion Autoencoders
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 03, 2023 </span>    
         <span class="authors"> Naser Damer, Meiling Fang, Patrick Siebke, Jan Niklas Kolf, Marco Huber, Fadi Boutros </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.01843" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Investigating new methods of creating face morphing attacks is essential to
foresee novel attacks and help mitigate them. Creating morphing attacks is
commonly either performed on the image-level or on the representation-level.
The representation-level morphing has been performed so far based on generative
adversarial networks (GAN) where the encoded images are interpolated in the
latent space to produce a morphed image based on the interpolated vector. Such
a process was constrained by the limited reconstruction fidelity of GAN
architectures. Recent advances in the diffusion autoencoder models have
overcome the GAN limitations, leading to high reconstruction fidelity. This
theoretically makes them a perfect candidate to perform representation-level
face morphing. This work investigates using diffusion autoencoders to create
face morphing attacks by comparing them to a wide range of image-level and
representation-level morphs. Our vulnerability analyses on four
state-of-the-art face recognition models have shown that such models are highly
vulnerable to the created attacks, the MorDIFF, especially when compared to
existing representation-level morphs. Detailed detectability analyses are also
performed on the MorDIFF, showing that they are as challenging to detect as
other morphing attacks created on the image- or representation-level. Data and
morphing script are made public: https://github.com/naserdamer/MorDIFF.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning End-to-End Channel Coding with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 03, 2023 </span>    
         <span class="authors"> Muah Kim, Rick Fritschek, Rafael F. Schaefer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.01714" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.IT, cs.LG, math.IT
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 It is a known problem that deep-learning-based end-to-end (E2E) channel
coding systems depend on a known and differentiable channel model, due to the
learning process and based on the gradient-descent optimization methods. This
places the challenge to approximate or generate the channel or its derivative
from samples generated by pilot signaling in real-world scenarios. Currently,
there are two prevalent methods to solve this problem. One is to generate the
channel via a generative adversarial network (GAN), and the other is to, in
essence, approximate the gradient via reinforcement learning methods. Other
methods include using score-based methods, variational autoencoders, or
mutual-information-based methods. In this paper, we focus on generative models
and, in particular, on a new promising method called diffusion models, which
have shown a higher quality of generation in image-based tasks. We will show
that diffusion models can be used in wireless E2E scenarios and that they work
as good as Wasserstein GANs while having a more stable training procedure and a
better generalization ability in testing.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Interventional and Counterfactual Inference with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 03, 2023 </span>    
         <span class="authors"> Muah Kim, Rick Fritschek, Rafael F. Schaefer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.01714" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.IT, cs.LG, math.IT
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 It is a known problem that deep-learning-based end-to-end (E2E) channel
coding systems depend on a known and differentiable channel model, due to the
learning process and based on the gradient-descent optimization methods. This
places the challenge to approximate or generate the channel or its derivative
from samples generated by pilot signaling in real-world scenarios. Currently,
there are two prevalent methods to solve this problem. One is to generate the
channel via a generative adversarial network (GAN), and the other is to, in
essence, approximate the gradient via reinforcement learning methods. Other
methods include using score-based methods, variational autoencoders, or
mutual-information-based methods. In this paper, we focus on generative models
and, in particular, on a new promising method called diffusion models, which
have shown a higher quality of generation in image-based tasks. We will show
that diffusion models can be used in wireless E2E scenarios and that they work
as good as Wasserstein GANs while having a more stable training procedure and a
better generalization ability in testing.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Dreamix: Video Diffusion Models are General Video Editors
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 02, 2023 </span>    
         <span class="authors"> Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, Yedid Hoshen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.01329" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-driven image and video diffusion models have recently achieved
unprecedented generation realism. While diffusion models have been successfully
applied for image editing, very few works have done so for video editing. We
present the first diffusion-based method that is able to perform text-based
motion and appearance editing of general videos. Our approach uses a video
diffusion model to combine, at inference time, the low-resolution
spatio-temporal information from the original video with new, high resolution
information that it synthesized to align with the guiding text prompt. As
obtaining high-fidelity to the original video requires retaining some of its
high-resolution information, we add a preliminary stage of finetuning the model
on the original video, significantly boosting fidelity. We propose to improve
motion editability by a new, mixed objective that jointly finetunes with full
temporal attention and with temporal attention masking. We further introduce a
new framework for image animation. We first transform the image into a coarse
video by simple image processing operations such as replication and perspective
geometric projections, and then use our general video editor to animate it. As
a further application, we can use our method for subject-driven video
generation. Extensive qualitative and numerical experiments showcase the
remarkable editing ability of our method and establish its superior performance
compared to baseline methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Are Diffusion Models Vulnerable to Membership Inference Attacks?
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 02, 2023 </span>    
         <span class="authors"> Jinhao Duan, Fei Kong, Shiqi Wang, Xiaoshuang Shi, Kaidi Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.01316" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based generative models have shown great potential for image
synthesis, but there is a lack of research on the security and privacy risks
they may pose. In this paper, we investigate the vulnerability of diffusion
models to Membership Inference Attacks (MIAs), a common privacy concern. Our
results indicate that existing MIAs designed for GANs or VAE are largely
ineffective on diffusion models, either due to inapplicable scenarios (e.g.,
requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer
distances between synthetic images and member images). To address this gap, we
propose Step-wise Error Comparing Membership Inference (SecMI), a black-box MIA
that infers memberships by assessing the matching of forward process posterior
estimation at each timestep. SecMI follows the common overfitting assumption in
MIA where member samples normally have smaller estimation errors, compared with
hold-out samples. We consider both the standard diffusion models, e.g., DDPM,
and the text-to-image diffusion models, e.g., Stable Diffusion. Experimental
results demonstrate that our methods precisely infer the membership with high
confidence on both of the two scenarios across six different datasets
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 02, 2023 </span>    
         <span class="authors"> Litu Rout, Advait Parulekar, Constantine Caramanis, Sanjay Shakkottai </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.01217" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.AI, cs.LG, math.ST, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We provide a theoretical justification for sample recovery using diffusion
based image inpainting in a linear model setting. While most inpainting
algorithms require retraining with each new mask, we prove that diffusion based
inpainting generalizes well to unseen masks without retraining. We analyze a
recently proposed popular diffusion based inpainting algorithm called RePaint
(Lugmayr et al., 2022), and show that it has a bias due to misalignment that
hampers sample recovery even in a two-state diffusion process. Motivated by our
analysis, we propose a modified RePaint algorithm we call RePaint$^+$ that
provably recovers the underlying true sample and enjoys a linear rate of
convergence. It achieves this by rectifying the misalignment error present in
drift and dispersion of the reverse process. To the best of our knowledge, this
is the first linear convergence result for a diffusion based image inpainting
algorithm.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Stable Target Field for Reduced Variance Score Estimation in Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 01, 2023 </span>    
         <span class="authors"> Yilun Xu, Shangyuan Tong, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.00670" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models generate samples by reversing a fixed forward diffusion
process. Despite already providing impressive empirical results, these
diffusion models algorithms can be further improved by reducing the variance of
the training targets in their denoising score-matching objective. We argue that
the source of such variance lies in the handling of intermediate noise-variance
scales, where multiple modes in the data affect the direction of reverse paths.
We propose to remedy the problem by incorporating a reference batch which we
use to calculate weighted conditional scores as more stable training targets.
We show that the procedure indeed helps in the challenging intermediate regime
by reducing (the trace of) the covariance of training targets. The new stable
targets can be seen as trading bias for reduced variance, where the bias
vanishes with increasing reference batch size. Empirically, we show that the
new objective improves the image quality, stability, and training speed of
various popular diffusion models across datasets with both general ODE and SDE
solvers. When used in combination with EDM, our method yields a current SOTA
FID of 1.90 with 35 network evaluations on the unconditional CIFAR-10
generation task. The code is available at https://github.com/Newbeeer/stf
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 01, 2023 </span>    
         <span class="authors"> Marloes Arts, Victor Garcia Satorras, Chin-Wei Huang, Daniel Zuegner, Marco Federici, Cecilia Clementi, Frank Noé, Robert Pinsler, Rianne van den Berg </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.00600" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Coarse-grained (CG) molecular dynamics enables the study of biological
processes at temporal and spatial scales that would be intractable at an
atomistic resolution. However, accurately learning a CG force field remains a
challenge. In this work, we leverage connections between score-based generative
models, force fields and molecular dynamics to learn a CG force field without
requiring any force inputs during training. Specifically, we train a diffusion
generative model on protein structures from molecular dynamics simulations, and
we show that its score function approximates a force field that can directly be
used to simulate CG molecular dynamics. While having a vastly simplified
training setup compared to previous work, we demonstrate that our approach
leads to improved performance across several small- to medium-sized protein
simulations, reproducing the CG equilibrium distribution, and preserving
dynamics of all-atom simulations such as protein folding events.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Flow Matching: Simulation-Free Dynamic Optimal Transport
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 01, 2023 </span>    
         <span class="authors"> Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, Yoshua Bengio </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.00482" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Continuous normalizing flows (CNFs) are an attractive generative modeling
technique, but they have thus far been held back by limitations in their
simulation-based maximum likelihood training. In this paper, we introduce a new
technique called conditional flow matching (CFM), a simulation-free training
objective for CNFs. CFM features a stable regression objective like that used
to train the stochastic flow in diffusion models but enjoys the efficient
inference of deterministic flow models. In contrast to both diffusion models
and prior CNF training algorithms, our CFM objective does not require the
source distribution to be Gaussian or require evaluation of its density. Based
on this new objective, we also introduce optimal transport CFM (OT-CFM), which
creates simpler flows that are more stable to train and lead to faster
inference, as evaluated in our experiments. Training CNFs with CFM improves
results on a variety of conditional and unconditional generation tasks such as
inferring single cell dynamics, unsupervised image translation, and
Schr\"odinger bridge inference. Code is available at
https://github.com/atong01/conditional-flow-matching .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for High-Resolution Solar Forecasts
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 01, 2023 </span>    
         <span class="authors"> Yusuke Hatanaka, Yannik Glaser, Geoff Galgon, Giuseppe Torri, Peter Sadowski </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.00170" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, physics.ao-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Forecasting future weather and climate is inherently difficult. Machine
learning offers new approaches to increase the accuracy and computational
efficiency of forecasts, but current methods are unable to accurately model
uncertainty in high-dimensional predictions. Score-based diffusion models offer
a new approach to modeling probability distributions over many dependent
variables, and in this work, we demonstrate how they provide probabilistic
forecasts of weather and climate variables at unprecedented resolution, speed,
and accuracy. We apply the technique to day-ahead solar irradiance forecasts by
generating many samples from a diffusion model trained to super-resolve
coarse-resolution numerical weather predictions to high-resolution weather
satellite observations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Salient Conditional Diffusion for Defending Against Backdoor Attacks
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 31, 2023 </span>    
         <span class="authors"> Brandon B. May, N. Joseph Tatro, Dylan Walker, Piyush Kumar, Nathan Shnidman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13862" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR, cs.CV, I.2
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a novel algorithm, Salient Conditional Diffusion (Sancdifi), a
state-of-the-art defense against backdoor attacks. Sancdifi uses a denoising
diffusion probabilistic model (DDPM) to degrade an image with noise and then
recover said image using the learned reverse diffusion. Critically, we compute
saliency map-based masks to condition our diffusion, allowing for stronger
diffusion on the most salient pixels by the DDPM. As a result, Sancdifi is
highly effective at diffusing out triggers in data poisoned by backdoor
attacks. At the same time, it reliably recovers salient features when applied
to clean data. This performance is achieved without requiring access to the
model parameters of the Trojan network, meaning Sancdifi operates as a
black-box defense.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 31, 2023 </span>    
         <span class="authors"> Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13826" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CL, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent text-to-image generative models have demonstrated an unparalleled
ability to generate diverse and creative imagery guided by a target text
prompt. While revolutionary, current state-of-the-art diffusion models may
still fail in generating images that fully convey the semantics in the given
text prompt. We analyze the publicly available Stable Diffusion model and
assess the existence of catastrophic neglect, where the model fails to generate
one or more of the subjects from the input prompt. Moreover, we find that in
some cases the model also fails to correctly bind attributes (e.g., colors) to
their corresponding subjects. To help mitigate these failure cases, we
introduce the concept of Generative Semantic Nursing (GSN), where we seek to
intervene in the generative process on the fly during inference time to improve
the faithfulness of the generated images. Using an attention-based formulation
of GSN, dubbed Attend-and-Excite, we guide the model to refine the
cross-attention units to attend to all subject tokens in the text prompt and
strengthen - or excite - their activations, encouraging the model to generate
all subjects described in the text prompt. We compare our approach to
alternative approaches and demonstrate that it conveys the desired concepts
more faithfully across a range of text prompts.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 31, 2023 </span>    
         <span class="authors"> Zihao Wang, Yingyu Yang, Maxime Sermesant, Hervé Delingette, Ona Wu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13743" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Cross-modality data translation has attracted great interest in image
computing. Deep generative models (\textit{e.g.}, GANs) show performance
improvement in tackling those problems. Nevertheless, as a fundamental
challenge in image translation, the problem of Zero-shot-Learning
Cross-Modality Data Translation with fidelity remains unanswered. This paper
proposes a new unsupervised zero-shot-learning method named Mutual Information
guided Diffusion cross-modality data translation Model (MIDiffusion), which
learns to translate the unseen source data to the target domain. The
MIDiffusion leverages a score-matching-based generative model, which learns the
prior knowledge in the target domain. We propose a differentiable
local-wise-MI-Layer ($LMI$) for conditioning the iterative denoising sampling.
The $LMI$ captures the identical cross-modality features in the statistical
domain for the diffusion guidance; thus, our method does not require retraining
when the source domain is changed, as it does not rely on any direct mapping
between the source and target domains. This advantage is critical for applying
cross-modality data translation methods in practice, as a reasonable amount of
source domain dataset is not always available for supervised training. We
empirically show the advanced performance of MIDiffusion in comparison with an
influential group of generative models, including adversarial-based and other
score-matching-based models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 31, 2023 </span>    
         <span class="authors"> Tao Yang, Yuwang Wang, Yan Lv, Nanning Zheng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13721" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, targeting to understand the underlying explainable factors
behind observations and modeling the conditional generation process on these
factors, we propose a new task, disentanglement of diffusion probabilistic
models (DPMs), to take advantage of the remarkable modeling ability of DPMs. To
tackle this task, we further devise an unsupervised approach named DisDiff. For
the first time, we achieve disentangled representation learning in the
framework of diffusion probabilistic models. Given a pre-trained DPM, DisDiff
can automatically discover the inherent factors behind the image data and
disentangle the gradient fields of DPM into sub-gradient fields, each
conditioned on the representation of each discovered factor. We propose a novel
Disentangling Loss for DisDiff to facilitate the disentanglement of the
representation and sub-gradients. The extensive experiments on synthetic and
real-world datasets demonstrate the effectiveness of DisDiff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Transport with Support: Data-Conditional Diffusion Bridges
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 31, 2023 </span>    
         <span class="authors"> Ella Tamir, Martin Trapp, Arno Solin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13636" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The dynamic Schr\"odinger bridge problem provides an appealing setting for
solving optimal transport problems by learning non-linear diffusion processes
using efficient iterative solvers. Recent works have demonstrated
state-of-the-art results (eg. in modelling single-cell embryo RNA sequences or
sampling from complex posteriors) but are limited to learning bridges with only
initial and terminal constraints. Our work extends this paradigm by proposing
the Iterative Smoothing Bridge (ISB). We integrate Bayesian filtering and
optimal control into learning the diffusion process, enabling constrained
stochastic processes governed by sparse observations at intermediate stages and
terminal constraints. We assess the effectiveness of our method on synthetic
and real-world data and show that the ISB generalises well to high-dimensional
data, is computationally efficient, and provides accurate estimates of the
marginals at intermediate and terminal times.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 31, 2023 </span>    
         <span class="authors"> Haomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Roger Zimmermann, Yuxuan Liang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13629" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Spatio-temporal graph neural networks (STGNN) have emerged as the dominant
model for spatio-temporal graph (STG) forecasting. Despite their success, they
fail to model intrinsic uncertainties within STG data, which cripples their
practicality in downstream tasks for decision-making. To this end, this paper
focuses on probabilistic STG forecasting, which is challenging due to the
difficulty in modeling uncertainties and complex ST dependencies. In this
study, we present the first attempt to generalize the popular denoising
diffusion probabilistic models to STGs, leading to a novel non-autoregressive
framework called DiffSTG, along with the first denoising network UGnet for STG
in the framework. Our approach combines the spatio-temporal learning
capabilities of STGNNs with the uncertainty measurements of diffusion models.
Extensive experiments validate that DiffSTG reduces the Continuous Ranked
Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7%
over existing methods on three real-world datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning Data Representations with Joint Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 31, 2023 </span>    
         <span class="authors"> Kamil Deja, Tomasz Trzcinski, Jakub M. Tomczak </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13622" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Joint machine learning models that allow synthesizing and classifying data
often offer uneven performance between those tasks or are unstable to train. In
this work, we depart from a set of empirical observations that indicate the
usefulness of internal representations built by contemporary deep
diffusion-based generative models not only for generating but also predicting.
We then propose to extend the vanilla diffusion model with a classifier that
allows for stable joint end-to-end training with shared parameterization
between those objectives. The resulting joint diffusion model outperforms
recent state-of-the-art hybrid methods in terms of both classification and
generation quality on all evaluated benchmarks. On top of our joint training
approach, we present how we can directly benefit from shared generative and
discriminative representations by introducing a method for visual
counterfactual explanations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Optimizing DDPM Sampling with Shortcut Fine-Tuning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 31, 2023 </span>    
         <span class="authors"> Ying Fan, Kangwook Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13362" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for
addressing the challenge of fast sampling of pretrained Denoising Diffusion
Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM
samplers through the direct minimization of Integral Probability Metrics (IPM),
instead of learning the backward diffusion process. This enables samplers to
discover an alternative and more efficient sampling shortcut, deviating from
the backward diffusion process. Inspired by a control perspective, we propose a
new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that
under certain assumptions, gradient descent of diffusion models with respect to
IPM is equivalent to performing policy gradient. To our best knowledge, this is
the first attempt to utilize reinforcement learning (RL) methods to train
diffusion models. Through empirical evaluation, we demonstrate that our
fine-tuning method can further enhance existing fast DDPM samplers, resulting
in sample quality comparable to or even surpassing that of the full-step model
across various datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ArchiSound: Audio Generation with Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 30, 2023 </span>    
         <span class="authors"> Flavio Schneider </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13267" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.CL, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The recent surge in popularity of diffusion models for image generation has
brought new attention to the potential of these models in other areas of media
generation. One area that has yet to be fully explored is the application of
diffusion models to audio generation. Audio generation requires an
understanding of multiple aspects, such as the temporal dimension, long term
structure, multiple layers of overlapping sounds, and the nuances that only
trained listeners can detect. In this work, we investigate the potential of
diffusion models for audio generation. We propose a set of models to tackle
multiple aspects, including a new method for text-conditional latent audio
diffusion with stacked 1D U-Nets, that can generate multiple minutes of music
from a textual description. For each model, we make an effort to maintain
reasonable inference speed, targeting real-time on a single consumer GPU. In
addition to trained models, we provide a collection of open source libraries
with the hope of simplifying future work in the field. Samples can be found at
https://bit.ly/audio-diffusion. Codes are at
https://github.com/archinetai/audio-diffusion-pytorch.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Extracting Training Data from Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 30, 2023 </span>    
         <span class="authors"> Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.13188" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CR, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have
attracted significant attention due to their ability to generate high-quality
synthetic images. In this work, we show that diffusion models memorize
individual images from their training data and emit them at generation time.
With a generate-and-filter pipeline, we extract over a thousand training
examples from state-of-the-art models, ranging from photographs of individual
people to trademarked company logos. We also train hundreds of diffusion models
in various settings to analyze how different modeling and data decisions affect
privacy. Overall, our results show that diffusion models are much less private
than prior generative models such as GANs, and that mitigating these
vulnerabilities may require new advances in privacy-preserving training.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ERA-Solver: Error-Robust Adams Solver for Fast Sampling of...
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 30, 2023 </span>    
         <span class="authors"> Shengmeng Li, Luping Liu, Zenghao Chai, Runnan Li, Xu Tan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.12935" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Though denoising diffusion probabilistic models (DDPMs) have achieved
remarkable generation results, the low sampling efficiency of DDPMs still
limits further applications. Since DDPMs can be formulated as diffusion
ordinary differential equations (ODEs), various fast sampling methods can be
derived from solving diffusion ODEs. However, we notice that previous sampling
methods with fixed analytical form are not robust with the error in the noise
estimated from pretrained diffusion models. In this work, we construct an
error-robust Adams solver (ERA-Solver), which utilizes the implicit Adams
numerical method that consists of a predictor and a corrector. Different from
the traditional predictor based on explicit Adams methods, we leverage a
Lagrange interpolation function as the predictor, which is further enhanced
with an error-robust strategy to adaptively select the Lagrange bases with
lower error in the estimated noise. Experiments on Cifar10, LSUN-Church, and
LSUN-Bedroom datasets demonstrate that our proposed ERA-Solver achieves 5.14,
9.42, and 9.69 Fenchel Inception Distance (FID) for image generation, with only
10 network evaluations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Bli
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 30, 2023 </span>    
         <span class="authors"> Naoki Murata, Koichi Saito, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Yuki Mitsufuji, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.12686" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Pre-trained diffusion models have been successfully used as priors in a
variety of linear inverse problems, where the goal is to reconstruct a signal
from noisy linear measurements. However, existing approaches require knowledge
of the linear operator. In this paper, we propose GibbsDDRM, an extension of
Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the
linear measurement operator is unknown. GibbsDDRM constructs a joint
distribution of the data, measurements, and linear operator by using a
pre-trained diffusion model for the data prior, and it solves the problem by
posterior sampling with an efficient variant of a Gibbs sampler. The proposed
method is problem-agnostic, meaning that a pre-trained diffusion model can be
applied to various inverse problems without fine tuning. In experiments, it
achieved high performance on both blind image deblurring and vocal
dereverberation tasks, despite the use of simple generic priors for the
underlying linear operators.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 30, 2023 </span>    
         <span class="authors"> Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.12661" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, cs.MM, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large-scale multimodal generative modeling has created milestones in
text-to-image and text-to-video generation. Its application to audio still lags
behind for two main reasons: the lack of large-scale datasets with high-quality
text-audio pairs, and the complexity of modeling long continuous audio data. In
this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that
addresses these gaps by 1) introducing pseudo prompt enhancement with a
distill-then-reprogram approach, it alleviates data scarcity with orders of
magnitude concept compositions by using language-free audios; 2) leveraging
spectrogram autoencoder to predict the self-supervised audio representation
instead of waveforms. Together with robust contrastive language-audio
pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art
results in both objective and subjective benchmark evaluation. Moreover, we
present its controllability and generalization for X-to-Audio with "No Modality
Left Behind", for the first time unlocking the ability to generate
high-definition, high-fidelity audios given a user-defined modality input.
Audio samples are available at https://Text-to-Audio.github.io
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## AudioLDM: Text-to-Audio Generation with Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 29, 2023 </span>    
         <span class="authors"> Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D. Plumbley </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.12503" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.AI, cs.MM, eess.AS, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-audio (TTA) system has recently gained attention for its ability to
synthesize general audio based on text descriptions. However, previous studies
in TTA have limited generation quality with high computational costs. In this
study, we propose AudioLDM, a TTA system that is built on a latent space to
learn the continuous audio representations from contrastive language-audio
pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs
with audio embedding while providing text embedding as a condition during
sampling. By learning the latent representations of audio signals and their
compositions without modeling the cross-modal relationship, AudioLDM is
advantageous in both generation quality and computational efficiency. Trained
on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA
performance measured by both objective and subjective metrics (e.g., frechet
distance). Moreover, AudioLDM is the first TTA system that enables various
text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion.
Our implementation and demos are available at https://audioldm.github.io.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SEGA: Instructing Diffusion using Semantic Dimensions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 28, 2023 </span>    
         <span class="authors"> Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, Kristian Kersting </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.12247" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-image diffusion models have recently received a lot of interest for
their astonishing ability to produce high-fidelity images from text only.
However, achieving one-shot generation that aligns with the user's intent is
nearly impossible, yet small changes to the input prompt often result in very
different images. This leaves the user with little semantic control. To put the
user in control, we show how to interact with the diffusion process to flexibly
steer it along semantic directions. This semantic guidance (SEGA) allows for
subtle and extensive edits, changes in composition and style, as well as
optimizing the overall artistic conception. We demonstrate SEGA's effectiveness
on a variety of tasks and provide evidence for its versatility and flexibility.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Minimizing Trajectory Curvature of ODE-based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2023 </span>    
         <span class="authors"> Sangyun Lee, Beomsu Kim, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.12003" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent ODE/SDE-based generative models, such as diffusion models, rectified
flows, and flow matching, define a generative process as a time reversal of a
fixed forward process. Even though these models show impressive performance on
large-scale datasets, numerical simulation requires multiple evaluations of a
neural network, leading to a slow sampling speed. We attribute the reason to
the high curvature of the learned generative trajectories, as it is directly
related to the truncation error of a numerical solver. Based on the
relationship between the forward process and the curvature, here we present an
efficient method of training the forward process to minimize the curvature of
generative trajectories without any ODE/SDE simulation. Experiments show that
our method achieves a lower curvature than previous models and, therefore,
decreased sampling costs while maintaining competitive performance. Code is
available at https://github.com/sangyun884/fast-ode.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2023 </span>    
         <span class="authors"> Flavio Schneider, Zhijing Jin, Bernhard Schölkopf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11757" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The recent surge in popularity of diffusion models for image generation has
brought new attention to the potential of these models in other areas of media
synthesis. One area that has yet to be fully explored is the application of
diffusion models to music generation. Music generation requires to handle
multiple aspects, including the temporal dimension, long-term structure,
multiple layers of overlapping sounds, and nuances that only trained listeners
can detect. In our work, we investigate the potential of diffusion models for
text-conditional music generation. We develop a cascading latent diffusion
approach that can generate multiple minutes of high-quality stereo music at
48kHz from textual descriptions. For each model, we make an effort to maintain
reasonable inference speed, targeting real-time on a single consumer GPU. In
addition to trained models, we provide a collection of open-source libraries
with the hope of facilitating future work in the field.
  We open-source the following: Music samples for this paper:
https://bit.ly/anonymous-mousai; all music samples for all models:
https://bit.ly/audio-diffusion; and codes:
https://github.com/archinetai/audio-diffusion-pytorch
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Input Perturbation Reduces Exposure Bias in Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2023 </span>    
         <span class="authors"> Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita Cucchiara </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11706" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Probabilistic Models have shown an impressive generation
quality, although their long sampling chain leads to high computational costs.
In this paper, we observe that a long sampling chain also leads to an error
accumulation phenomenon, which is similar to the exposure bias problem in
autoregressive text generation. Specifically, we note that there is a
discrepancy between training and testing, since the former is conditioned on
the ground truth samples, while the latter is conditioned on the previously
generated results. To alleviate this problem, we propose a very simple but
effective training regularization, consisting in perturbing the ground truth
samples to simulate the inference time prediction errors. We empirically show
that the proposed input perturbation leads to a significant improvement of the
sample quality while reducing both the training and the inference times. For
instance, on CelebA 64$\times$64, we achieve a new state-of-the-art FID score
of 1.27, while saving 37.5% of the training time. The code is publicly
available at https://github.com/forever208/DDPM-IP
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Image Restoration with Mean-Reverting Stochastic Differential Equations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2023 </span>    
         <span class="authors"> Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11699" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper presents a stochastic differential equation (SDE) approach for
general-purpose image restoration. The key construction consists in a
mean-reverting SDE that transforms a high-quality image into a degraded
counterpart as a mean state with fixed Gaussian noise. Then, by simulating the
corresponding reverse-time SDE, we are able to restore the origin of the
low-quality image without relying on any task-specific prior knowledge.
Crucially, the proposed mean-reverting SDE has a closed-form solution, allowing
us to compute the ground truth time-dependent score and learn it with a neural
network. Moreover, we propose a maximum likelihood objective to learn an
optimal reverse trajectory which stabilizes the training and improves the
restoration results. In the experiments, we show that our proposed method
achieves highly competitive performance in quantitative comparisons on image
deraining, deblurring, and denoising, setting a new state-of-the-art on two
deraining datasets. Finally, the general applicability of our approach is
further demonstrated via qualitative results on image super-resolution,
inpainting, and dehazing. Code is available at
https://github.com/Algolzw/image-restoration-sde.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A denoting diffusion model for fluid flow prediction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2023 </span>    
         <span class="authors"> Gefan Yang, Stefan Sommer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11661" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, physics.flu-dyn
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a novel denoising diffusion generative model for predicting
nonlinear fluid fields named FluidDiff. By performing a diffusion process, the
model is able to learn a complex representation of the high-dimensional dynamic
system, and then Langevin sampling is used to generate predictions for the flow
state under specified initial conditions. The model is trained with finite,
discrete fluid simulation data. We demonstrate that our model has the capacity
to model the distribution of simulated training data and that it gives accurate
predictions on the test data. Without encoded prior knowledge of the underlying
physical system, it shares competitive performance with other deep learning
models for fluid prediction, which is promising for investigation on new
computational fluid dynamics methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Accelerating Guided Diffusion Sampling with Splitting Numerical Methods
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2023 </span>    
         <span class="authors"> Suttisak Wizadwongsa, Supasorn Suwajanakorn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11558" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Guided diffusion is a technique for conditioning the output of a diffusion
model at sampling time without retraining the network for each specific task.
One drawback of diffusion models, however, is their slow sampling process.
Recent techniques can accelerate unguided sampling by applying high-order
numerical methods to the sampling process when viewed as differential
equations. On the contrary, we discover that the same techniques do not work
for guided sampling, and little has been explored about its acceleration. This
paper explores the culprit of this problem and provides a solution based on
operator splitting methods, motivated by our key finding that classical
high-order numerical methods are unsuitable for the conditional function. Our
proposed method can re-utilize the high-order methods for guided sampling and
can generate images with the same quality as a 250-step DDIM baseline using
32-58% less sampling time on ImageNet256. We also demonstrate usage on a wide
variety of conditional generation tasks, such as text-to-image generation,
colorization, inpainting, and super-resolution.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PLay: Parametrically Conditioned Layout Generation using Latent Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2023 </span>    
         <span class="authors"> Chin-Yi Cheng, Forrest Huang, Gang Li, Yang Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11529" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.HC
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Layout design is an important task in various design fields, including user
interfaces, document, and graphic design. As this task requires tedious manual
effort by designers, prior works have attempted to automate this process using
generative models, but commonly fell short of providing intuitive user controls
and achieving design objectives. In this paper, we build a conditional latent
diffusion model, PLay, that generates parametrically conditioned layouts in
vector graphic space from user-specified guidelines, which are commonly used by
designers for representing their design intents in current practices. Our
method outperforms prior works across three datasets on metrics including FID
and FD-VG, and in user test. Moreover, it brings a novel and interactive
experience to professional layout design processes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Denoising for Low-Dose-CT Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2023 </span>    
         <span class="authors"> Runyi Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11482" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Low-dose Computed Tomography (LDCT) reconstruction is an important task in
medical image analysis. Recent years have seen many deep learning based
methods, proved to be effective in this area. However, these methods mostly
follow a supervised architecture, which needs paired CT image of full dose and
quarter dose, and the solution is highly dependent on specific measurements. In
this work, we introduce Denoising Diffusion LDCT Model, dubbed as DDLM,
generating noise-free CT image using conditioned sampling. DDLM uses pretrained
model, and need no training nor tuning process, thus our proposal is in
unsupervised manner. Experiments on LDCT images have shown comparable
performance of DDLM using less inference time, surpassing other
state-of-the-art methods, proving both accurate and efficient. Implementation
code will be set to public soon.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## 3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 26, 2023 </span>    
         <span class="authors"> Biao Zhang, Jiapeng Tang, Matthias Niessner, Peter Wonka </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11445" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce 3DShape2VecSet, a novel shape representation for neural fields
designed for generative diffusion models. Our shape representation can encode
3D shapes given as surface models or point clouds, and represents them as
neural fields. The concept of neural fields has previously been combined with a
global latent vector, a regular grid of latent vectors, or an irregular grid of
latent vectors. Our new representation encodes neural fields on top of a set of
vectors. We draw from multiple concepts, such as the radial basis function
representation and the cross attention and self-attention function, to design a
learnable representation that is especially suitable for processing with
transformers. Our results show improved performance in 3D shape encoding and 3D
shape generative modeling tasks. We demonstrate a wide variety of generative
applications: unconditioned generation, category-conditioned generation,
text-conditioned generation, point-cloud completion, and image-conditioned
generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## simple diffusion: End-to-end diffusion for high resolution images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 26, 2023 </span>    
         <span class="authors"> Emiel Hoogeboom, Jonathan Heek, Tim Salimans </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11093" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Currently, applying diffusion models in pixel space of high resolution images
is difficult. Instead, existing approaches focus on diffusion in lower
dimensional spaces (latent diffusion), or have multiple super-resolution levels
of generation referred to as cascades. The downside is that these approaches
add additional complexity to the diffusion framework.
  This paper aims to improve denoising diffusion for high resolution images
while keeping the model as simple as possible. The paper is centered around the
research question: How can one train a standard denoising diffusion models on
high resolution images, and still obtain performance comparable to these
alternate approaches?
  The four main findings are: 1) the noise schedule should be adjusted for high
resolution images, 2) It is sufficient to scale only a particular part of the
architecture, 3) dropout should be added at specific locations in the
architecture, and 4) downsampling is an effective strategy to avoid high
resolution feature maps. Combining these simple yet effective techniques, we
achieve state-of-the-art on image generation among diffusion models without
sampling modifiers on ImageNet.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 26, 2023 </span>    
         <span class="authors"> Shangrong Yang, Chunyu Lin, Kang Liao, Yao Zhao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11785" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Fisheye image rectification has a long-term unresolved issue with
synthetic-to-real generalization. In most previous works, the model trained on
the synthetic images obtains unsatisfactory performance on the real-world
fisheye image. To this end, we propose a Dual Diffusion Architecture (DDA) for
the fisheye rectification with a better generalization ability. The proposed
DDA is simultaneously trained with paired synthetic fisheye images and
unlabeled real fisheye images. By gradually introducing noises, the synthetic
and real fisheye images can eventually develop into a consistent noise
distribution, improving the generalization and achieving unlabeled real fisheye
correction. The original image serves as the prior guidance in existing DDPMs
(Denoising Diffusion Probabilistic Models). However, the non-negligible
indeterminate relationship between the prior condition and the target affects
the generation performance. Especially in the rectification task, the radial
distortion can cause significant artifacts. Therefore, we provide an
unsupervised one-pass network that produces a plausible new condition to
strengthen guidance. This network can be regarded as an alternate scheme for
fast producing reliable results without iterative inference. Compared with the
state-of-the-art methods, our approach can reach superior performance in both
synthetic and real fisheye image corrections.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On the Importance of Noise Scheduling for Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 26, 2023 </span>    
         <span class="authors"> Ting Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.10972" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG, cs.MM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We empirically study the effect of noise scheduling strategies for denoising
diffusion generative models. There are three findings: (1) the noise scheduling
is crucial for the performance, and the optimal one depends on the task (e.g.,
image sizes), (2) when increasing the image size, the optimal noise scheduling
shifts towards a noisier one (due to increased redundancy in pixels), and (3)
simply scaling the input data by a factor of $b$ while keeping the noise
schedule function fixed (equivalent to shifting the logSNR by $\log b$) is a
good strategy across image sizes. This simple recipe, when combined with
recently proposed Recurrent Interface Network (RIN), yields state-of-the-art
pixel-based diffusion models for high-resolution images on ImageNet, enabling
single-stage, end-to-end generation of diverse and high-fidelity images at
1024$\times$1024 resolution (without upsampling/cascades).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Separate And Diffuse: Using a Pretrained Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 25, 2023 </span>    
         <span class="authors"> Shahar Lutati, Eliya Nachmani, Lior Wolf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.10752" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The problem of speech separation, also known as the cocktail party problem,
refers to the task of isolating a single speech signal from a mixture of speech
signals. Previous work on source separation derived an upper bound for the
source separation task in the domain of human speech. This bound is derived for
deterministic models. Recent advancements in generative models challenge this
bound. We show how the upper bound can be generalized to the case of random
generative models. Applying a diffusion model Vocoder that was pretrained to
model single-speaker voices on the output of a deterministic separation model
leads to state-of-the-art separation results. It is shown that this requires
one to combine the output of the separation model with that of the diffusion
model. In our method, a linear combination is performed, in the frequency
domain, using weights that are inferred by a learned model. We show
state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple
benchmarks. In particular, for two speakers, our method is able to surpass what
was previously considered the upper performance bound.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On the Mathematics of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 25, 2023 </span>    
         <span class="authors"> David McAllester </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11108" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, math.PR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper gives direct derivations of the differential equations and
likelihood formulas of diffusion models assuming only knowledge of Gaussian
distributions. A VAE analysis derives both forward and backward stochastic
differential equations (SDEs) as well as non-variational integral expressions
for likelihood formulas. A score-matching analysis derives the reverse
diffusion ordinary differential equation (ODE) and a family of
reverse-diffusion SDEs parameterized by noise level. The paper presents the
mathematics directly with attributions saved for a final section.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Imitating Human Behaviour with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 25, 2023 </span>    
         <span class="authors"> Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, Sam Devlin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.10677" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.AI, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have emerged as powerful generative models in the
text-to-image domain. This paper studies their application as
observation-to-action models for imitating human behaviour in sequential
environments. Human behaviour is stochastic and multimodal, with structured
correlations between action dimensions. Meanwhile, standard modelling choices
in behaviour cloning are limited in their expressiveness and may introduce bias
into the cloned policy. We begin by pointing out the limitations of these
choices. We then propose that diffusion models are an excellent fit for
imitating human behaviour, since they learn an expressive distribution over the
joint action space. We introduce several innovations to make diffusion models
suitable for sequential environments; designing suitable architectures,
investigating the role of guidance, and developing reliable sampling
strategies. Experimentally, diffusion models closely match human demonstrations
in a simulated robotic control task and a modern 3D gaming environment.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score Matching via Differentiable Physics
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 24, 2023 </span>    
         <span class="authors"> Benjamin J. Holzschuh, Simona Vegetti, Nils Thuerey </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.10250" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, physics.data-an
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models based on stochastic differential equations (SDEs) gradually
perturb a data distribution $p(\mathbf{x})$ over time by adding noise to it. A
neural network is trained to approximate the score $\nabla_\mathbf{x} \log
p_t(\mathbf{x})$ at time $t$, which can be used to reverse the corruption
process. In this paper, we focus on learning the score field that is associated
with the time evolution according to a physics operator in the presence of
natural non-deterministic physical processes like diffusion. A decisive
difference to previous methods is that the SDE underlying our approach
transforms the state of a physical system to another state at a later time. For
that purpose, we replace the drift of the underlying SDE formulation with a
differentiable simulator or a neural network approximation of the physics. We
propose different training strategies based on the so-called probability flow
ODE to fit a training set of simulation trajectories and discuss their relation
to the score matching objective. For inference, we sample plausible
trajectories that evolve towards a given end state using the reverse-time SDE
and demonstrate the competitiveness of our approach for different challenging
inverse problems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Bipartite Graph Diffusion Model for Human Interaction Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 24, 2023 </span>    
         <span class="authors"> Baptiste Chopin, Hao Tang, Mohamed Daoudi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.10134" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The generation of natural human motion interactions is a hot topic in
computer vision and computer animation. It is a challenging task due to the
diversity of possible human motion interactions. Diffusion models, which have
already shown remarkable generative capabilities in other domains, are a good
candidate for this task. In this paper, we introduce a novel bipartite graph
diffusion method (BiGraphDiff) to generate human motion interactions between
two persons. Specifically, bipartite node sets are constructed to model the
inherent geometric constraints between skeleton nodes during interactions. The
interaction graph diffusion model is transformer-based, combining some
state-of-the-art motion methods. We show that the proposed achieves new
state-of-the-art results on leading benchmarks for the human interaction
generation task.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 24, 2023 </span>    
         <span class="authors"> Fan Zhang, Naye Ji, Fuxing Gao, Yongping Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.10047" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.GR, cs.CV, cs.HC, cs.SD, eess.AS, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Speech-driven gesture synthesis is a field of growing interest in virtual
human creation. However, a critical challenge is the inherent intricate
one-to-many mapping between speech and gestures. Previous studies have explored
and achieved significant progress with generative models. Notwithstanding, most
synthetic gestures are still vastly less natural. This paper presents
DiffMotion, a novel speech-driven gesture synthesis architecture based on
diffusion models. The model comprises an autoregressive temporal encoder and a
denoising diffusion probability Module. The encoder extracts the temporal
context of the speech input and historical gestures. The diffusion module
learns a parameterized Markov chain to gradually convert a simple distribution
into a complex distribution and generates the gestures according to the
accompanied speech. Compared with baselines, objective and subjective
evaluations confirm that our approach can produce natural and diverse
gesticulation and demonstrate the benefits of diffusion-based models on
speech-driven gesture synthesis.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Membership Inference of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 24, 2023 </span>    
         <span class="authors"> Hailong Hu, Jun Pang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.09956" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent years have witnessed the tremendous success of diffusion models in
data synthesis. However, when diffusion models are applied to sensitive data,
they also give rise to severe privacy concerns. In this paper, we
systematically present the first study about membership inference attacks
against diffusion models, which aims to infer whether a sample was used to
train the model. Two attack methods are proposed, namely loss-based and
likelihood-based attacks. Our attack methods are evaluated on several
state-of-the-art diffusion models, over different datasets in relation to
privacy-sensitive data. Extensive experimental evaluations show that our
attacks can achieve remarkable performance. Furthermore, we exhaustively
investigate various factors which can affect attack performance. Finally, we
also evaluate the performance of our attack methods on diffusion models trained
with differential privacy.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 23, 2023 </span>    
         <span class="authors"> Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, Junchi Yan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.09474" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Real-world data generation often involves complex inter-dependencies among
instances, violating the IID-data hypothesis of standard learning paradigms and
posing a challenge for uncovering the geometric structures for learning desired
instance representations. To this end, we introduce an energy constrained
diffusion model which encodes a batch of instances from a dataset into
evolutionary states that progressively incorporate other instances' information
by their interactions. The diffusion process is constrained by descent criteria
w.r.t.~a principled energy function that characterizes the global consistency
of instance representations over latent structures. We provide rigorous theory
that implies closed-form optimal estimates for the pairwise diffusion strength
among arbitrary instance pairs, which gives rise to a new class of neural
encoders, dubbed as DIFFormer (diffusion-based Transformers), with two
instantiations: a simple version with linear complexity for prohibitive
instance numbers, and an advanced version for learning complex structures.
Experiments highlight the wide applicability of our model as a general-purpose
encoder backbone with superior performance in various tasks, such as node
classification on large graphs, semi-supervised image/text classification, and
spatial-temporal dynamics prediction.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## RainDiffusion:When Unsupervised Learning Meets Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 23, 2023 </span>    
         <span class="authors"> Mingqiang Wei, Yiyang Shen, Yongzhen Wang, Haoran Xie, Jing Qin, Fu Lee Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.09430" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent diffusion models show great potential in generative modeling tasks.
This motivates us to raise an intriguing question - What will happen when
unsupervised learning meets diffusion models for real-world image deraining?
Before answering it, we observe two major obstacles of diffusion models in
real-world image deraining: the need for paired training data and the limited
utilization of multi-scale rain patterns. To overcome the obstacles, we propose
RainDiffusion, the first real-world image deraining paradigm based on diffusion
models. RainDiffusion is a non-adversarial training paradigm that introduces
stable training of unpaired real-world data, rather than weakly adversarial
training, serving as a new standard bar for real-world image deraining. It
consists of two cooperative branches: Non-diffusive Translation Branch (NTB)
and Diffusive Translation Branch (DTB). NTB exploits a cycle-consistent
architecture to bypass the difficulty in unpaired training of regular diffusion
models by generating initial clean/rainy image pairs. Given initial image
pairs, DTB leverages multi-scale diffusion models to progressively refine the
desired output via diffusive generative and multi-scale priors, to obtain a
better generalization capacity of real-world image deraining. Extensive
experiments confirm the superiority of our RainDiffusion over eight
un/semi-supervised methods and show its competitive advantages over seven
fully-supervised ones.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffSDS: A language diffusion model for protein backbone inpainting under geometric conditions and constraints
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 22, 2023 </span>    
         <span class="authors"> Zhangyang Gao, Cheng Tan, Stan Z. Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.09642" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.QM, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Have you ever been troubled by the complexity and computational cost of SE(3)
protein structure modeling and been amazed by the simplicity and power of
language modeling? Recent work has shown promise in simplifying protein
structures as sequences of protein angles; therefore, language models could be
used for unconstrained protein backbone generation. Unfortunately, such
simplification is unsuitable for the constrained protein inpainting problem,
where the model needs to recover masked structures conditioned on unmasked
ones, as it dramatically increases the computing cost of geometric constraints.
To overcome this dilemma, we suggest inserting a hidden \textbf{a}tomic
\textbf{d}irection \textbf{s}pace (\textbf{ADS}) upon the language model,
converting invariant backbone angles into equivalent direction vectors and
preserving the simplicity, called Seq2Direct encoder ($\text{Enc}_{s2d}$).
Geometric constraints could be efficiently imposed on the newly introduced
direction space. A Direct2Seq decoder ($\text{Dec}_{d2s}$) with mathematical
guarantees is also introduced to develop a \textbf{SDS}
($\text{Enc}_{s2d}$+$\text{Dec}_{d2s}$) model. We apply the SDS model as the
denoising neural network during the conditional diffusion process, resulting in
a constrained generative model--\textbf{DiffSDS}. Extensive experiments show
that the plug-and-play ADS could transform the language model into a strong
structural model without loss of simplicity. More importantly, the proposed
DiffSDS outperforms previous strong baselines by a large margin on the task of
protein inpainting.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Removing Structured Noise with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 20, 2023 </span>    
         <span class="authors"> Tristan S. W. Stevens, Hans van Gorp, Faik C. Meral, Jun Seob Shin, Jason Yu, Jean-Luc Robert, Ruud J. G. van Sloun </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2302.05290" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, eess.IV, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Solving ill-posed inverse problems requires careful formulation of prior
beliefs over the signals of interest and an accurate description of their
manifestation into noisy measurements. Handcrafted signal priors based on e.g.
sparsity are increasingly replaced by data-driven deep generative models, and
several groups have recently shown that state-of-the-art score-based diffusion
models yield particularly strong performance and flexibility. In this paper, we
show that the powerful paradigm of posterior sampling with diffusion models can
be extended to include rich, structured, noise models. To that end, we propose
a joint conditional reverse diffusion process with learned scores for the noise
and signal-generating distribution. We demonstrate strong performance gains
across various inverse problems with structured noise, outperforming
competitive baselines that use normalizing flows and adversarial networks. This
opens up new opportunities and relevant practical applications of diffusion
modeling for inverse problems in the context of non-Gaussian measurement
models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionCT: Latent Diffusion Model for CT Image Standardization
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 20, 2023 </span>    
         <span class="authors"> Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.08815" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Computed tomography (CT) is one of the modalities for effective lung cancer
screening, diagnosis, treatment, and prognosis. The features extracted from CT
images are now used to quantify spatial and temporal variations in tumors.
However, CT images obtained from various scanners with customized acquisition
protocols may introduce considerable variations in texture features, even for
the same patient. This presents a fundamental challenge to downstream studies
that require consistent and reliable feature analysis. Existing CT image
harmonization models rely on GAN-based supervised or semi-supervised learning,
with limited performance. This work addresses the issue of CT image
harmonization using a new diffusion-based model, named DiffusionCT, to
standardize CT images acquired from different vendors and protocols.
DiffusionCT operates in the latent space by mapping a latent non-standard
distribution into a standard one. DiffusionCT incorporates an Unet-based
encoder-decoder, augmented by a diffusion model integrated into the bottleneck
part. The model is designed in two training phases. The encoder-decoder is
first trained, without embedding the diffusion model, to learn the latent
representation of the input data. The latent diffusion model is then trained in
the next training phase while fixing the encoder-decoder. Finally, the decoder
synthesizes a standardized image with the transformed latent representation.
The experimental results demonstrate a significant improvement in the
performance of the standardization task using DiffusionCT.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Regular Time-series Generation using SGM
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 20, 2023 </span>    
         <span class="authors"> Haksoo Lim, Minjung Kim, Sewon Park, Noseong Park </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.08518" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) are generative models that are in the
spotlight these days. Time-series frequently occurs in our daily life, e.g.,
stock data, climate data, and so on. Especially, time-series forecasting and
classification are popular research topics in the field of machine learning.
SGMs are also known for outperforming other generative models. As a result, we
apply SGMs to synthesize time-series data by learning conditional score
functions. We propose a conditional score network for the time-series
generation domain. Furthermore, we also derive the loss function between the
score matching and the denoising score matching in the time-series generation
domain. Finally, we achieve state-of-the-art results on real-world datasets in
terms of sampling diversity and quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-based Conditional ECG Generation with Structured State Space Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 19, 2023 </span>    
         <span class="authors"> Juan Miguel Lopez Alcaraz, Nils Strodthoff </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.08227" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.SP, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Synthetic data generation is a promising solution to address privacy issues
with the distribution of sensitive health data. Recently, diffusion models have
set new standards for generative models for different data modalities. Also
very recently, structured state space models emerged as a powerful modeling
paradigm to capture long-term dependencies in time series. We put forward
SSSD-ECG, as the combination of these two technologies, for the generation of
synthetic 12-lead electrocardiograms conditioned on more than 70 ECG
statements. Due to a lack of reliable baselines, we also propose conditional
variants of two state-of-the-art unconditional generative models. We thoroughly
evaluate the quality of the generated samples, by evaluating pretrained
classifiers on the generated data and by evaluating the performance of a
classifier trained only on synthetic data, where SSSD-ECG clearly outperforms
its GAN-based competitors. We demonstrate the soundness of our approach through
further experiments, including conditional class interpolation and a clinical
Turing test demonstrating the high quality of the SSSD-ECG samples across a
wide range of conditions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 19, 2023 </span>    
         <span class="authors"> Jun Yue, Leyuan Fang, Shaobo Xia, Yue Deng, Jiayi Ma </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.08072" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Color plays an important role in human visual perception, reflecting the
spectrum of objects. However, the existing infrared and visible image fusion
methods rarely explore how to handle multi-spectral/channel data directly and
achieve high color fidelity. This paper addresses the above issue by proposing
a novel method with diffusion models, termed as Dif-Fusion, to generate the
distribution of the multi-channel input data, which increases the ability of
multi-source information aggregation and the fidelity of colors. In specific,
instead of converting multi-channel images into single-channel data in existing
fusion methods, we create the multi-channel data distribution with a denoising
network in a latent space with forward and reverse diffusion process. Then, we
use the the denoising network to extract the multi-channel diffusion features
with both visible and infrared information. Finally, we feed the multi-channel
diffusion features to the multi-channel fusion module to directly generate the
three-channel fused image. To retain the texture and intensity information, we
propose multi-channel gradient loss and intensity loss. Along with the current
evaluation metrics for measuring texture and intensity fidelity, we introduce a
new evaluation metric to quantify color fidelity. Extensive experiments
indicate that our method is more effective than other state-of-the-art image
fusion methods, especially in color fidelity.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Fast Inference in Denoising Diffusion Models via MMD Finetuning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 19, 2023 </span>    
         <span class="authors"> Emanuele Aiello, Diego Valsesia, Enrico Magli </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.07969" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Models (DDMs) have become a popular tool for generating
high-quality samples from complex data distributions. These models are able to
capture sophisticated patterns and structures in the data, and can generate
samples that are highly diverse and representative of the underlying
distribution. However, one of the main limitations of diffusion models is the
complexity of sample generation, since a large number of inference timesteps is
required to faithfully capture the data distribution. In this paper, we present
MMD-DDM, a novel method for fast sampling of diffusion models. Our approach is
based on the idea of using the Maximum Mean Discrepancy (MMD) to finetune the
learned distribution with a given budget of timesteps. This allows the
finetuned model to significantly improve the speed-quality trade-off, by
substantially increasing fidelity in inference regimes with few steps or,
equivalently, by reducing the required number of steps to reach a target
fidelity, thus paving the way for a more practical adoption of diffusion models
in a wide range of applications. We evaluate our approach on unconditional
image generation with extensive experiments across the CIFAR-10, CelebA,
ImageNet and LSUN-Church datasets. Our findings show that the proposed method
is able to produce high-quality samples in a fraction of the time required by
widely-used diffusion models, and outperforms state-of-the-art techniques for
accelerated sampling. Code is available at:
https://github.com/diegovalsesia/MMD-DDM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Understanding the diffusion models by conditional expectations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 19, 2023 </span>    
         <span class="authors"> Yubin Lu, Zhongjian Wang, Guillaume Bal </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.07882" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.NA, math.NA, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper provide several mathematical analyses of the diffusion model in
machine learning. The drift term of the backwards sampling process is
represented as a conditional expectation involving the data distribution and
the forward diffusion. The training process aims to find such a drift function
by minimizing the mean-squared residue related to the conditional expectation.
Using small-time approximations of the Green's function of the forward
diffusion, we show that the analytical mean drift function in DDPM and the
score function in SGM asymptotically blow up in the final stages of the
sampling process for singular data distributions such as those concentrated on
lower-dimensional manifolds, and is therefore difficult to approximate by a
network. To overcome this difficulty, we derive a new target function and
associated loss, which remains bounded even for singular data distributions. We
illustrate the theoretical findings with several numerical examples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 19, 2023 </span>    
         <span class="authors"> Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yanwu Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.11798" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The Diffusion Probabilistic Model (DPM) has recently gained popularity in the
field of computer vision, thanks to its image generation applications, such as
Imagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated
impressive capabilities and sparked much discussion within the community.
Recent studies have also found DPM to be useful in the field of medical image
analysis, as evidenced by the strong performance of the medical image
segmentation model MedSegDiff in various tasks. While these models were
originally designed with a UNet backbone, they may also potentially benefit
from the incorporation of vision transformer techniques. However, we discovered
that simply combining these two approaches resulted in subpar performance. In
this paper, we propose a novel transformer-based conditional UNet framework, as
well as a new Spectrum-Space Transformer (SS-Former) to model the interaction
between noise and semantic features. This architectural improvement leads to a
new diffusion-based medical image segmentation method called MedSegDiff-V2,
which significantly improves the performance of MedSegDiff. We have verified
the effectiveness of MedSegDiff-V2 on eighteen organs of five segmentation
datasets with different image modalities. Our experimental results demonstrate
that MedSegDiff-V2 outperforms state-of-the-art (SOTA) methods by a
considerable margin, further proving the generalizability and effectiveness of
the proposed model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 17, 2023 </span>    
         <span class="authors"> Lars Lien Ankile, Anna Midgley, Sebastian Weisshaar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.06871" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Neural Networks are infamously sensitive to small perturbations in their
inputs, making them vulnerable to adversarial attacks. This project evaluates
the performance of Denoising Diffusion Probabilistic Models (DDPM) as a
purification technique to defend against adversarial attacks. This works by
adding noise to an adversarial example before removing it through the reverse
process of the diffusion model. We evaluate the approach on the PatchCamelyon
data set for histopathologic scans of lymph node sections and find an
improvement of the robust accuracy by up to 88\% of the original model's
accuracy, constituting a considerable improvement over the vanilla model and
our baselines. The project code is located at
https://github.com/ankile/Adversarial-Diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-based Generation, Optimization, and Planning in 3D Scenes
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 15, 2023 </span>    
         <span class="authors"> Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, Song-Chun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.06015" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce SceneDiffuser, a conditional generative model for 3D scene
understanding. SceneDiffuser provides a unified model for solving
scene-conditioned generation, optimization, and planning. In contrast to prior
works, SceneDiffuser is intrinsically scene-aware, physics-based, and
goal-oriented. With an iterative sampling strategy, SceneDiffuser jointly
formulates the scene-aware generation, physics-based optimization, and
goal-oriented planning via a diffusion-based denoising process in a fully
differentiable fashion. Such a design alleviates the discrepancies among
different modules and the posterior collapse of previous scene-conditioned
generative models. We evaluate SceneDiffuser with various 3D scene
understanding tasks, including human pose and motion generation, dexterous
grasp generation, path planning for 3D navigation, and motion planning for
robot arms. The results show significant improvements compared with previous
models, demonstrating the tremendous potential of SceneDiffuser for the broad
community of 3D scene understanding.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Neural Image Compression with a Diffusion-Based Decoder
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 13, 2023 </span>    
         <span class="authors"> Noor Fathima Ghouse, Jens Petersen, Auke Wiggers, Tianlin Xu, Guillaume Sautière </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.05489" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models have recently achieved remarkable success in
generating high quality image and video data. In this work, we build on this
class of generative models and introduce a method for lossy compression of high
resolution images. The resulting codec, which we call DIffuson-based Residual
Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal
of the rate-distortion-perception tradeoff at test time, while obtaining
competitive performance with GAN-based methods in perceptual quality.
Furthermore, while sampling from diffusion probabilistic models is notoriously
expensive, we show that in the compression setting the number of steps can be
drastically reduced.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Guiding Text-to-Image Diffusion Model Towards Grounded Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 12, 2023 </span>    
         <span class="authors"> Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.05221" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The goal of this paper is to augment a pre-trained text-to-image diffusion
model with the ability of open-vocabulary objects grounding, i.e.,
simultaneously generating images and segmentation masks for the corresponding
visual entities described in the text prompt. We make the following
contributions: (i) we insert a grounding module into the existing diffusion
model, that can be trained to align the visual and textual embedding space of
the diffusion model with only a small number of object categories; (ii) we
propose an automatic pipeline for constructing a dataset, that consists of
{image, segmentation mask, text prompt} triplets, to train the proposed
grounding module; (iii) we evaluate the performance of open-vocabulary
grounding on images generated from the text-to-image diffusion model and show
that the module can well segment the objects of categories beyond seen ones at
training time; (iv) we adopt the guided diffusion model to build a synthetic
semantic segmentation dataset, and show that training a standard segmentation
model on such dataset demonstrates competitive performance on zero-shot
segmentation(ZS3) benchmark, which opens up new opportunities for adopting the
powerful diffusion model for discriminative tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Thompson Sampling with Diffusion Generative Prior
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 12, 2023 </span>    
         <span class="authors"> Yu-Guan Hsieh, Shiva Prasad Kasiviswanathan, Branislav Kveton, Patrick Blöbaum </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.05182" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we initiate the idea of using denoising diffusion models to
learn priors for online decision making problems. Our special focus is on the
meta-learning for bandit framework, with the goal of learning a strategy that
performs well across bandit tasks of a same class. To this end, we train a
diffusion model that learns the underlying task distribution and combine
Thompson sampling with the learned prior to deal with new tasks at test time.
Our posterior sampling algorithm is designed to carefully balance between the
learned prior and the noisy observations that come from the learner's
interaction with the environment. To capture realistic bandit scenarios, we
also propose a novel diffusion model training procedure that trains even from
incomplete and/or noisy data, which could be of independent interest. Finally,
our extensive experimental evaluations clearly demonstrate the potential of the
proposed approach.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-based Data Augmentation for Skin Disease Classification
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 12, 2023 </span>    
         <span class="authors"> Mohamed Akrout, Bálint Gyepesi, Péter Holló, Adrienn Poór, Blága Kincső, Stephen Solis, Katrina Cirone, Jeremy Kawahara, Dekker Slade, Latif Abid, Máté Kovács, István Fazekas </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.04802" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite continued advancement in recent years, deep neural networks still
rely on large amounts of training data to avoid overfitting. However, labeled
training data for real-world applications such as healthcare is limited and
difficult to access given longstanding privacy, and strict data sharing
policies. By manipulating image datasets in the pixel or feature space,
existing data augmentation techniques represent one of the effective ways to
improve the quantity and diversity of training data. Here, we look to advance
augmentation techniques by building upon the emerging success of text-to-image
diffusion probabilistic models in augmenting the training samples of our
macroscopic skin disease dataset. We do so by enabling fine-grained control of
the image generation process via input text prompts. We demonstrate that this
generative data augmentation approach successfully maintains a similar
classification accuracy of the visual classifier even when trained on a fully
synthetic skin disease dataset. Similar to recent applications of generative
models, our study suggests that diffusion models are indeed effective in
generating high-quality skin images that do not sacrifice the classifier
performance, and can improve the augmentation of training datasets after
curation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models For Stronger Face Morphing Attacks
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 10, 2023 </span>    
         <span class="authors"> Zander Blasingame, Chen Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.04218" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Face morphing attacks seek to deceive a Face Recognition (FR) system by
presenting a morphed image consisting of the biometric qualities from two
different identities with the aim of triggering a false acceptance with one of
the two identities, thereby presenting a significant threat to biometric
systems. The success of a morphing attack is dependent on the ability of the
morphed image to represent the biometric characteristics of both identities
that were used to create the image. We present a novel morphing attack that
uses a Diffusion-based architecture to improve the visual fidelity of the image
and improve the ability of the morphing attack to represent characteristics
from both identities. We demonstrate the high fidelity of the proposed attack
by evaluating its visual fidelity via the Frechet Inception Distance. Extensive
experiments are conducted to measure the vulnerability of FR systems to the
proposed attack. The proposed attack is compared to two state-of-the-art
GAN-based morphing attacks along with two Landmark-based attacks. The ability
of a morphing attack detector to detect the proposed attack is measured and
compared against the other attacks. Additionally, a novel metric to measure the
relative strength between morphing attacks is introduced and evaluated.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Speech Driven Video Editing via an Audio-Conditioned Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 10, 2023 </span>    
         <span class="authors"> Dan Bigioi, Shubhajit Basak, Michał Stypułkowski, Maciej Zięba, Hugh Jordan, Rachel McDonnell, Peter Corcoran </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.04474" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Taking inspiration from recent developments in visual generative tasks using
diffusion models, we propose a method for end-to-end speech-driven video
editing using a denoising diffusion model. Given a video of a talking person,
and a separate auditory speech recording, the lip and jaw motions are
re-synchronized without relying on intermediate structural representations such
as facial landmarks or a 3D face model. We show this is possible by
conditioning a denoising diffusion model on audio mel spectral features to
generate synchronised facial motion. Proof of concept results are demonstrated
on both single-speaker and multi-speaker video editing, providing a baseline
model on the CREMA-D audiovisual data set. To the best of our knowledge, this
is the first work to demonstrate and validate the feasibility of applying
end-to-end denoising diffusion models to the task of audio-driven video
editing.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 10, 2023 </span>    
         <span class="authors"> Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, Jiwen Lu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.03786" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Talking head synthesis is a promising approach for the video production
industry. Recently, a lot of effort has been devoted in this research area to
improve the generation quality or enhance the model generalization. However,
there are few works able to address both issues simultaneously, which is
essential for practical applications. To this end, in this paper, we turn
attention to the emerging powerful Latent Diffusion Models, and model the
Talking head generation as an audio-driven temporally coherent denoising
process (DiffTalk). More specifically, instead of employing audio signals as
the single driving factor, we investigate the control mechanism of the talking
face, and incorporate reference face images and landmarks as conditions for
personality-aware generalized synthesis. In this way, the proposed DiffTalk is
capable of producing high-quality talking head videos in synchronization with
the source audio, and more importantly, it can be naturally generalized across
different identities without any further fine-tuning. Additionally, our
DiffTalk can be gracefully tailored for higher-resolution synthesis with
negligible extra computational cost. Extensive experiments show that the
proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking
head videos for generalized novel identities. For more video results, please
refer to \url{https://sstzal.github.io/DiffTalk/}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Image Denoising: The Deep Learning Revolution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 09, 2023 </span>    
         <span class="authors"> Michael Elad, Bahjat Kawar, Gregory Vaksman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.03362" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image denoising (removal of additive white Gaussian noise from an image) is
one of the oldest and most studied problems in image processing. An extensive
work over several decades has led to thousands of papers on this subject, and
to many well-performing algorithms for this task. Indeed, 10 years ago, these
achievements have led some researchers to suspect that "Denoising is Dead", in
the sense that all that can be achieved in this domain has already been
obtained. However, this turned out to be far from the truth, with the
penetration of deep learning (DL) into image processing. The era of DL brought
a revolution to image denoising, both by taking the lead in today's ability for
noise removal in images, and by broadening the scope of denoising problems
being treated. Our paper starts by describing this evolution, highlighting in
particular the tension and synergy that exist between classical approaches and
modern DL-based alternatives in design of image denoisers.
  The recent transitions in the field of image denoising go far beyond the
ability to design better denoisers. In the 2nd part of this paper we focus on
recently discovered abilities and prospects of image denoisers. We expose the
possibility of using denoisers to serve other problems, such as regularizing
general inverse problems and serving as the prime engine in diffusion-based
image synthesis. We also unveil the idea that denoising and other inverse
problems might not have a unique solution as common algorithms would have us
believe. Instead, we describe constructive ways to produce randomized and
diverse high quality results for inverse problems, all fueled by the progress
that DL brought to image denoising.
  This survey paper aims to provide a broad view of the history of image
denoising and closely related topics. Our aim is to give a better context to
recent discoveries, and to the influence of DL in our domain.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 09, 2023 </span>    
         <span class="authors"> Michael Elad, Bahjat Kawar, Gregory Vaksman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.03362" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image denoising (removal of additive white Gaussian noise from an image) is
one of the oldest and most studied problems in image processing. An extensive
work over several decades has led to thousands of papers on this subject, and
to many well-performing algorithms for this task. Indeed, 10 years ago, these
achievements have led some researchers to suspect that "Denoising is Dead", in
the sense that all that can be achieved in this domain has already been
obtained. However, this turned out to be far from the truth, with the
penetration of deep learning (DL) into image processing. The era of DL brought
a revolution to image denoising, both by taking the lead in today's ability for
noise removal in images, and by broadening the scope of denoising problems
being treated. Our paper starts by describing this evolution, highlighting in
particular the tension and synergy that exist between classical approaches and
modern DL-based alternatives in design of image denoisers.
  The recent transitions in the field of image denoising go far beyond the
ability to design better denoisers. In the 2nd part of this paper we focus on
recently discovered abilities and prospects of image denoisers. We expose the
possibility of using denoisers to serve other problems, such as regularizing
general inverse problems and serving as the prime engine in diffusion-based
image synthesis. We also unveil the idea that denoising and other inverse
problems might not have a unique solution as common algorithms would have us
believe. Instead, we describe constructive ways to produce randomized and
diverse high quality results for inverse problems, all fueled by the progress
that DL brought to image denoising.
  This survey paper aims to provide a broad view of the history of image
denoising and closely related topics. Our aim is to give a better context to
recent discoveries, and to the influence of DL in our domain.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 08, 2023 </span>    
         <span class="authors"> Yan Li, Xinjiang Lu, Yaqing Wang, Dejing Dou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.03028" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Time series forecasting has been a widely explored task of great importance
in many applications. However, it is common that real-world time series data
are recorded in a short time period, which results in a big gap between the
deep model and the limited and noisy time series. In this work, we propose to
address the time series forecasting problem with generative modeling and
propose a bidirectional variational auto-encoder (BVAE) equipped with
diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled
diffusion probabilistic model is proposed to augment the time series data
without increasing the aleatoric uncertainty and implement a more tractable
inference process with BVAE. To ensure the generated series move toward the
true target, we further propose to adapt and integrate the multiscale denoising
score matching into the diffusion process for time series forecasting. In
addition, to enhance the interpretability and stability of the prediction, we
treat the latent variable in a multivariate manner and disentangle them on top
of minimizing total correlation. Extensive experiments on synthetic and
real-world data show that D3VAE outperforms competitive algorithms with
remarkable margins. Our implementation is available at
https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 08, 2023 </span>    
         <span class="authors"> Gyutaek Oh, Jeong Eun Lee, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.03027" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Motion artifact reduction is one of the important research topics in MR
imaging, as the motion artifact degrades image quality and makes diagnosis
difficult. Recently, many deep learning approaches have been studied for motion
artifact reduction. Unfortunately, most existing models are trained in a
supervised manner, requiring paired motion-corrupted and motion-free images, or
are based on a strict motion-corruption model, which limits their use for
real-world situations. To address this issue, here we present an annealed
score-based diffusion model for MRI motion artifact reduction. Specifically, we
train a score-based model using only motion-free images, and then motion
artifacts are removed by applying forward and reverse diffusion processes
repeatedly to gradually impose a low-frequency data consistency. Experimental
results verify that the proposed method successfully reduces both simulated and
in vivo motion artifacts, outperforming the state-of-the-art deep learning
methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 06, 2023 </span>    
         <span class="authors"> Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zięba, Stavros Petridis, Maja Pantic </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.03396" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Talking face generation has historically struggled to produce head movements
and natural facial expressions without guidance from additional reference
videos. Recent developments in diffusion-based generative models allow for more
realistic and stable data synthesis and their performance on image and video
generation has surpassed that of other generative models. In this work, we
present an autoregressive diffusion model that requires only one identity image
and audio sequence to generate a video of a realistic talking human head. Our
solution is capable of hallucinating head movements, facial expressions, such
as blinks, and preserving a given background. We evaluate our model on two
different datasets, achieving state-of-the-art results on both of them.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 02, 2023 </span>    
         <span class="authors"> Dennis Eschweiler, Johannes Stegmaier </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.10227" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models have shown great potential in
generating realistic image data. We show how those models can be used to
generate realistic microscopy image data in 2D and 3D based on simulated
sketches of cellular structures. Multiple data sets are used as an inspiration
to simulate sketches of different cellular structures, allowing to generate
fully-annotated image data sets without requiring human interactions. Those
data sets are used to train segmentation approaches and demonstrate that
annotation-free segmentation of cellular structures in fluorescence microscopy
image data can be achieved, thereby leaping towards the ultimate goal of
eliminating the necessity of human annotation efforts.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 02, 2023 </span>    
         <span class="authors"> Jumin Lee, Woobin Im, Sebin Lee, Sung-Eui Yoon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.00527" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we learn a diffusion model to generate 3D data on a
scene-scale. Specifically, our model crafts a 3D scene consisting of multiple
objects, while recent diffusion research has focused on a single object. To
realize our goal, we represent a scene with discrete class labels, i.e.,
categorical distribution, to assign multiple objects into semantic categories.
Thus, we extend discrete diffusion models to learn scene-scale categorical
distributions. In addition, we validate that a latent diffusion model can
reduce computation costs for training and deploying. To the best of our
knowledge, our work is the first to apply discrete and latent diffusion for 3D
categorical data on a scene-scale. We further propose to perform semantic scene
completion (SSC) by learning a conditional distribution using our diffusion
model, where the condition is a partial observation in a sparse point cloud. In
experiments, we empirically show that our diffusion models not only generate
reasonable scenes, but also perform the scene completion task better than a
discriminative model. Our code and models are available at
https://github.com/zoomin-lee/scene-scale-diffusion
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Diffusion Based on Discrete Graph Structures for Molecular Graph Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 01, 2023 </span>    
         <span class="authors"> Han Huang, Leilei Sun, Bowen Du, Weifeng Lv </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.00427" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, q-bio.BM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Learning the underlying distribution of molecular graphs and generating
high-fidelity samples is a fundamental research problem in drug discovery and
material science. However, accurately modeling distribution and rapidly
generating novel molecular graphs remain crucial and challenging goals. To
accomplish these goals, we propose a novel Conditional Diffusion model based on
discrete Graph Structures (CDGS) for molecular graph generation. Specifically,
we construct a forward graph diffusion process on both graph structures and
inherent features through stochastic differential equations (SDE) and derive
discrete graph structures as the condition for reverse generative processes. We
present a specialized hybrid graph noise prediction model that extracts the
global context and the local node-edge dependency from intermediate graph
states. We further utilize ordinary differential equation (ODE) solvers for
efficient graph sampling, based on the semi-linear structure of the probability
flow ODE. Experiments on diverse datasets validate the effectiveness of our
framework. Particularly, the proposed method still generates high-quality
molecular graphs in a limited number of steps. Our code is provided in
https://github.com/GRAPH-0/CDGS.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 01, 2023 </span>    
         <span class="authors"> Shizhan Gong, Cheng Chen, Yuqi Gong, Nga Yan Chan, Wenao Ma, Calvin Hoi-Kwan Mak, Jill Abrigo, Qi Dou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.00409" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Brain midline shift (MLS) is one of the most critical factors to be
considered for clinical diagnosis and treatment decision-making for
intracranial hemorrhage. Existing computational methods on MLS quantification
not only require intensive labeling in millimeter-level measurement but also
suffer from poor performance due to their dependence on specific landmarks or
simplified anatomical assumptions. In this paper, we propose a novel
semi-supervised framework to accurately measure the scale of MLS from head CT
scans. We formulate the MLS measurement task as a deformation estimation
problem and solve it using a few MLS slices with sparse labels. Meanwhile, with
the help of diffusion models, we are able to use a great number of unlabeled
MLS data and 2793 non-MLS cases for representation learning and regularization.
The extracted representation reflects how the image is different from a non-MLS
image and regularization serves an important role in the sparse-to-dense
refinement of the deformation field. Our experiment on a real clinical brain
hemorrhage dataset has achieved state-of-the-art performance and can generate
interpretable deformation fields.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 30, 2022 </span>    
         <span class="authors"> Zehua Chen, Yihan Wu, Yichong Leng, Jiawei Chen, Haohe Liu, Xu Tan, Yang Cui, Ke Wang, Lei He, Sheng Zhao, Jiang Bian, Danilo Mandic </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.14518" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.CL, cs.LG, cs.SD, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Probabilistic Models (DDPMs) are emerging in
text-to-speech (TTS) synthesis because of their strong capability of generating
high-fidelity samples. However, their iterative refinement process in
high-dimensional data space results in slow inference speed, which restricts
their application in real-time systems. Previous works have explored speeding
up by minimizing the number of inference steps but at the cost of sample
quality. In this work, to improve the inference speed for DDPM-based TTS model
while achieving high sample quality, we propose ResGrad, a lightweight
diffusion model which learns to refine the output spectrogram of an existing
TTS model (e.g., FastSpeech 2) by predicting the residual between the model
output and the corresponding ground-truth speech. ResGrad has several
advantages: 1) Compare with other acceleration methods for DDPM which need to
synthesize speech from scratch, ResGrad reduces the complexity of task by
changing the generation target from ground-truth mel-spectrogram to the
residual, resulting into a more lightweight model and thus a smaller real-time
factor. 2) ResGrad is employed in the inference process of the existing TTS
model in a plug-and-play way, without re-training this model. We verify ResGrad
on the single-speaker dataset LJSpeech and two more challenging datasets with
multiple speakers (LibriTTS) and high sampling rate (VCTK). Experimental
results show that in comparison with other speed-up methods of DDPMs: 1)
ResGrad achieves better sample quality with the same inference speed measured
by real-time factor; 2) with similar speech quality, ResGrad synthesizes speech
faster than baseline methods by more than 10 times. Audio samples are available
at https://resgrad1.github.io/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 28, 2022 </span>    
         <span class="authors"> Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.14704" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent CLIP-guided 3D optimization methods, such as DreamFields and
PureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D
synthesis. However, due to scratch training and random initialization without
prior knowledge, these methods often fail to generate accurate and faithful 3D
structures that conform to the input text. In this paper, we make the first
attempt to introduce explicit 3D shape priors into the CLIP-guided 3D
optimization process. Specifically, we first generate a high-quality 3D shape
from the input text in the text-to-shape stage as a 3D shape prior. We then use
it as the initialization of a neural radiance field and optimize it with the
full prompt. To address the challenging text-to-shape generation task, we
present a simple yet effective approach that directly bridges the text and
image modalities with a powerful text-to-image diffusion model. To narrow the
style domain gap between the images synthesized by the text-to-image diffusion
model and shape renderings used to train the image-to-shape generator, we
further propose to jointly optimize a learnable text prompt and fine-tune the
text-to-image diffusion model for rendering-style image generation. Our method,
Dream3D, is capable of generating imaginative 3D content with superior visual
quality and shape accuracy compared to state-of-the-art methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Exploring Vision Transformers as Diffusion Learners
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 28, 2022 </span>    
         <span class="authors"> He Cao, Jianan Wang, Tianhe Ren, Xianbiao Qi, Yihao Chen, Yuan Yao, Lei Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.13771" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based diffusion models have captured widespread attention and funded
fast progress of recent vision generative tasks. In this paper, we focus on
diffusion model backbone which has been much neglected before. We
systematically explore vision Transformers as diffusion learners for various
generative tasks. With our improvements the performance of vanilla ViT-based
backbone (IU-ViT) is boosted to be on par with traditional U-Net-based methods.
We further provide a hypothesis on the implication of disentangling the
generative backbone as an encoder-decoder structure and show proof-of-concept
experiments verifying the effectiveness of a stronger encoder for generative
tasks with ASymmetriC ENcoder Decoder (ASCEND). Our improvements achieve
competitive results on CIFAR-10, CelebA, LSUN, CUB Bird and large-resolution
text-to-image tasks. To the best of our knowledge, we are the first to
successfully train a single diffusion model on text-to-image task beyond 64x64
resolution. We hope this will motivate people to rethink the modeling choices
and the training pipelines for diffusion-based generative models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Exploring Transformer Backbones for Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 27, 2022 </span>    
         <span class="authors"> Princy Chahal </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.14678" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present an end-to-end Transformer based Latent Diffusion model for image
synthesis. On the ImageNet class conditioned generation task we show that a
Transformer based Latent Diffusion model achieves a 14.1FID which is comparable
to the 13.1FID score of a UNet based architecture. In addition to showing the
application of Transformer models for Diffusion based image synthesis this
simplification in architecture allows easy fusion and modeling of text and
image data. The multi-head attention mechanism of Transformers enables
simplified interaction between the image and text features which removes the
requirement for crossattention mechanism in UNet based Diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffFace: Diffusion-based Face Swapping with Facial Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 27, 2022 </span>    
         <span class="authors"> Kihong Kim, Yunho Kim, Seokju Cho, Junyoung Seo, Jisu Nam, Kychul Lee, Seungryong Kim, KwangHee Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.13344" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we propose a diffusion-based face swapping framework for the
first time, called DiffFace, composed of training ID conditional DDPM, sampling
with facial guidance, and a target-preserving blending. In specific, in the
training process, the ID conditional DDPM is trained to generate face images
with the desired identity. In the sampling process, we use the off-the-shelf
facial expert models to make the model transfer source identity while
preserving target attributes faithfully. During this process, to preserve the
background of the target image and obtain the desired face swapping result, we
additionally propose a target-preserving blending strategy. It helps our model
to keep the attributes of the target face from noise while transferring the
source facial identity. In addition, without any re-training, our model can
flexibly apply additional facial guidance and adaptively control the
ID-attributes trade-off to achieve the desired results. To the best of our
knowledge, this is the first approach that applies the diffusion model in face
swapping task. Compared with previous GAN-based approaches, by taking advantage
of the diffusion model for the face swapping task, DiffFace achieves better
benefits such as training stability, high fidelity, diversity of the samples,
and controllability. Extensive experiments show that our DiffFace is comparable
or superior to the state-of-the-art methods on several standard face swapping
benchmarks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 26, 2022 </span>    
         <span class="authors"> Zijian Zhang, Zhou Zhao, Zhijie Lin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.12990" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Probabilistic Models (DPMs) have shown a powerful capacity of
generating high-quality image samples. Recently, diffusion autoencoders
(Diff-AE) have been proposed to explore DPMs for representation learning via
autoencoding. Their key idea is to jointly train an encoder for discovering
meaningful representations from images and a conditional DPM as the decoder for
reconstructing images. Considering that training DPMs from scratch will take a
long time and there have existed numerous pre-trained DPMs, we propose
\textbf{P}re-trained \textbf{D}PM \textbf{A}uto\textbf{E}ncoding
(\textbf{PDAE}), a general method to adapt existing pre-trained DPMs to the
decoders for image reconstruction, with better training efficiency and
performance than Diff-AE. Specifically, we find that the reason that
pre-trained DPMs fail to reconstruct an image from its latent variables is due
to the information loss of forward process, which causes a gap between their
predicted posterior mean and the true one. From this perspective, the
classifier-guided sampling method can be explained as computing an extra mean
shift to fill the gap, reconstructing the lost class information in samples.
These imply that the gap corresponds to the lost information of the image, and
we can reconstruct the image by filling the gap. Drawing inspiration from this,
we employ a trainable model to predict a mean shift according to encoded
representation and train it to fill as much gap as possible, in this way, the
encoder is forced to learn as much information as possible from images to help
the filling. By reusing a part of network of pre-trained DPMs and redesigning
the weighting scheme of diffusion loss, PDAE can learn meaningful
representations from images efficiently. Extensive experiments demonstrate the
effectiveness, efficiency and flexibility of PDAE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Your diffusion model secretly knows the dimension of the data manifold
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 23, 2022 </span>    
         <span class="authors"> Jan Stanczuk, Georgios Batzolis, Teo Deveney, Carola-Bibiane Schönlieb </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.12611" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we propose a novel framework for estimating the dimension of
the data manifold using a trained diffusion model. A diffusion model
approximates the score function i.e. the gradient of the log density of a
noise-corrupted version of the target distribution for varying levels of
corruption. We prove that, if the data concentrates around a manifold embedded
in the high-dimensional ambient space, then as the level of corruption
decreases, the score function points towards the manifold, as this direction
becomes the direction of maximal likelihood increase. Therefore, for small
levels of corruption, the diffusion model provides us with access to an
approximation of the normal bundle of the data manifold. This allows us to
estimate the dimension of the tangent space, thus, the intrinsic dimension of
the data manifold. To the best of our knowledge, our method is the first
estimator of the data manifold dimension based on diffusion models and it
outperforms well established statistical estimators in controlled experiments
on both Euclidean and image data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Scalable Adaptive Computation for Iterative Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 22, 2022 </span>    
         <span class="authors"> Allan Jabri, David Fleet, Ting Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.11972" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.NE
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present the Recurrent Interface Network (RIN), a neural net architecture
that allocates computation adaptively to the input according to the
distribution of information, allowing it to scale to iterative generation of
high-dimensional data. Hidden units of RINs are partitioned into the interface,
which is locally connected to inputs, and latents, which are decoupled from
inputs and can exchange information globally. The RIN block selectively reads
from the interface into latents for high-capacity processing, with incremental
updates written back to the interface. Stacking multiple blocks enables
effective routing across local and global levels. While routing adds overhead,
the cost can be amortized in recurrent computation settings where inputs change
gradually while more global context persists, such as iterative generation
using diffusion models. To this end, we propose a latent self-conditioning
technique that "warm-starts" the latents at each iteration of the generation
process. When applied to diffusion models operating directly on pixels, RINs
yield state-of-the-art image and video generation without cascades or guidance,
while being domain-agnostic and up to 10$\times$ more efficient compared to
specialized 2D and 3D U-Nets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## StoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 22, 2022 </span>    
         <span class="authors"> Jean-Marie Lemercier, Julius Richter, Simon Welker, Timo Gerkmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.11851" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown a great ability at bridging the performance gap
between predictive and generative approaches for speech enhancement. We have
shown that they may even outperform their predictive counterparts for
non-additive corruption types or when they are evaluated on mismatched
conditions. However, diffusion models suffer from a high computational burden,
mainly as they require to run a neural network for each reverse diffusion step,
whereas predictive approaches only require one pass. As diffusion models are
generative approaches they may also produce vocalizing and breathing artifacts
in adverse conditions. In comparison, in such difficult scenarios, predictive
models typically do not produce such artifacts but tend to distort the target
speech instead, thereby degrading the speech quality. In this work, we present
a stochastic regeneration approach where an estimate given by a predictive
model is provided as a guide for further diffusion. We show that the proposed
approach uses the predictive model to remove the vocalizing and breathing
artifacts while producing very high quality samples thanks to the diffusion
model, even in adverse conditions. We further show that this approach enables
to use lighter sampling schemes with fewer diffusion steps without sacrificing
quality, thus lifting the computational burden by an order of magnitude. Source
code and audio examples are available online (https://uhh.de/inf-sp-storm).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GENIE: Large Scale Pre-training for Text Generation with Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 22, 2022 </span>    
         <span class="authors"> Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, Weizhu Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.11685" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we introduce a novel dIffusion language modEl pre-training
framework for text generation, which we call GENIE. GENIE is a large-scale
pretrained diffusion language model that consists of an encoder and a
diffusion-based decoder, which can generate text by gradually transforming a
random noise sequence into a coherent text sequence. To pre-train GENIE on a
large-scale language corpus, we design a new continuous paragraph denoise
objective, which encourages the diffusion-decoder to reconstruct a clean text
paragraph from a corrupted version, while preserving the semantic and syntactic
coherence. We evaluate GENIE on four downstream text generation benchmarks,
namely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results
show that GENIE achieves comparable performance with the state-of-the-art
autoregressive models on these benchmarks, and generates more diverse text
samples. The code and models of GENIE are available at
https://github.com/microsoft/ProphetNet/tree/master/GENIE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Character-Aware Models Improve Visual Text Rendering
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 20, 2022 </span>    
         <span class="authors"> Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, Noah Constant </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.10562" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Current image generation models struggle to reliably produce well-formed
visual text. In this paper, we investigate a key contributing factor: popular
text-to-image models lack character-level input features, making it much harder
to predict a word's visual makeup as a series of glyphs. To quantify this
effect, we conduct a series of experiments comparing character-aware vs.
character-blind text encoders. In the text-only domain, we find that
character-aware models provide large gains on a novel spelling task
(WikiSpell). Applying our learnings to the visual domain, we train a suite of
image generation models, and show that character-aware variants outperform
their character-blind counterparts across a range of novel text rendering tasks
(our DrawText benchmark). Our models set a much higher state-of-the-art on
visual spelling, with 30+ point accuracy gains over competitors on rare words,
despite training on far fewer examples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Scalable Diffusion Models with Transformers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 19, 2022 </span>    
         <span class="authors"> William Peebles, Saining Xie </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.09748" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We explore a new class of diffusion models based on the transformer
architecture. We train latent diffusion models of images, replacing the
commonly-used U-Net backbone with a transformer that operates on latent
patches. We analyze the scalability of our Diffusion Transformers (DiTs)
through the lens of forward pass complexity as measured by Gflops. We find that
DiTs with higher Gflops -- through increased transformer depth/width or
increased number of input tokens -- consistently have lower FID. In addition to
possessing good scalability properties, our largest DiT-XL/2 models outperform
all prior diffusion models on the class-conditional ImageNet 512x512 and
256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Latent Diffusion for Language Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 19, 2022 </span>    
         <span class="authors"> Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, Kilian Weinberger </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.09462" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have achieved great success in modeling continuous data
modalities such as images, audio, and video, but have seen limited use in
discrete domains such as language. Recent attempts to adapt diffusion to
language have presented diffusion as an alternative to autoregressive language
generation. We instead view diffusion as a complementary method that can
augment the generative capabilities of existing pre-trained language models. We
demonstrate that continuous diffusion models can be learned in the latent space
of a pre-trained encoder-decoder model, enabling us to sample continuous latent
representations that can be decoded into natural language with the pre-trained
decoder. We show that our latent diffusion models are more effective at
sampling novel text from data distributions than a strong autoregressive
baseline and also enable controllable generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Difformer: Empowering Diffusion Model on Embedding Space for Text Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 19, 2022 </span>    
         <span class="authors"> Zhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, Linli Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.09412" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have achieved state-of-the-art synthesis quality on both
visual and audio tasks, and recent works further adapt them to textual data by
diffusing on the embedding space. In this paper, we conduct systematic studies
and analyze the challenges between the continuous data space and the embedding
space which have not been carefully explored. Firstly, the data distribution is
learnable for embeddings, which may lead to the collapse of the loss function.
Secondly, as the norm of embeddings varies between popular and rare words,
adding the same noise scale will lead to sub-optimal results. In addition, we
find the normal level of noise causes insufficient training of the model. To
address the above challenges, we propose Difformer, an embedding diffusion
model based on Transformer, which consists of three essential modules including
an additional anchor loss function, a layer normalization module for
embeddings, and a noise factor to the Gaussian noise. Experiments on two
seminal text generation tasks including machine translation and text
summarization show the superiority of Difformer over compared embedding
diffusion baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusing Surrogate Dreams of Video Scenes to Predict Video Memorability
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 19, 2022 </span>    
         <span class="authors"> Lorin Sweeney, Graham Healy, Alan F. Smeaton </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.09308" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 As part of the MediaEval 2022 Predicting Video Memorability task we explore
the relationship between visual memorability, the visual representation that
characterises it, and the underlying concept portrayed by that visual
representation. We achieve state-of-the-art memorability prediction performance
with a model trained and tested exclusively on surrogate dream images,
elevating concepts to the status of a cornerstone memorability feature, and
finding strong evidence to suggest that the intrinsic memorability of visual
content can be distilled to its underlying concept or meaning irrespective of
its specific visual representational.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Speed up the inference of diffusion models via shortcut MCMC sampling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 18, 2022 </span>    
         <span class="authors"> Gang Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2301.01206" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, 68T10, I.2.6
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models have generated high quality image synthesis
recently. However, one pain point is the notorious inference to gradually
obtain clear images with thousands of steps, which is time consuming compared
to other generative models. In this paper, we present a shortcut MCMC sampling
algorithm, which balances training and inference, while keeping the generated
data's quality. In particular, we add the global fidelity constraint with
shortcut MCMC sampling to combat the local fitting from diffusion models. We do
some initial experiments and show very promising results. Our implementation is
available at https://github.com//vividitytech/diffusion-mcmc.git.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Point-E: A System for Generating 3D Point Clouds from Complex Prompts
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 16, 2022 </span>    
         <span class="authors"> Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Mark Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.08751" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While recent work on text-conditional 3D object generation has shown
promising results, the state-of-the-art methods typically require multiple
GPU-hours to produce a single sample. This is in stark contrast to
state-of-the-art generative image models, which produce samples in a number of
seconds or minutes. In this paper, we explore an alternative method for 3D
object generation which produces 3D models in only 1-2 minutes on a single GPU.
Our method first generates a single synthetic view using a text-to-image
diffusion model, and then produces a 3D point cloud using a second diffusion
model which conditions on the generated image. While our method still falls
short of the state-of-the-art in terms of sample quality, it is one to two
orders of magnitude faster to sample from, offering a practical trade-off for
some use cases. We release our pre-trained point cloud diffusion models, as
well as evaluation code and models, at https://github.com/openai/point-e.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Probabilistic Models beat GANs on Medical Images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 14, 2022 </span>    
         <span class="authors"> Gustav Müller-Franzes, Jan Moritz Niehues, Firas Khader, Soroosh Tayebi Arasteh, Christoph Haarburger, Christiane Kuhl, Tianci Wang, Tianyu Han, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.07501" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The success of Deep Learning applications critically depends on the quality
and scale of the underlying training data. Generative adversarial networks
(GANs) can generate arbitrary large datasets, but diversity and fidelity are
limited, which has recently been addressed by denoising diffusion probabilistic
models (DDPMs) whose superiority has been demonstrated on natural images. In
this study, we propose Medfusion, a conditional latent DDPM for medical images.
We compare our DDPM-based model against GAN-based models, which constitute the
current state-of-the-art in the medical domain. Medfusion was trained and
compared with (i) StyleGan-3 on n=101,442 images from the AIROGS challenge
dataset to generate fundoscopies with and without glaucoma, (ii) ProGAN on
n=191,027 from the CheXpert dataset to generate radiographs with and without
cardiomegaly and (iii) wGAN on n=19,557 images from the CRCMS dataset to
generate histopathological images with and without microsatellite stability. In
the AIROGS, CRMCS, and CheXpert datasets, Medfusion achieved lower (=better)
FID than the GANs (11.63 versus 20.43, 30.03 versus 49.26, and 17.28 versus
84.31). Also, fidelity (precision) and diversity (recall) were higher (=better)
for Medfusion in all three datasets. Our study shows that DDPM are a superior
alternative to GANs for image synthesis in the medical domain.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SPIRiT-Diffusion: SPIRiT-driven Score-Based Generative Modeling for Vessel Wall imaging
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 14, 2022 </span>    
         <span class="authors"> Chentao Cao, Zhuo-Xu Cui, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.11274" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion model is the most advanced method in image generation and has been
successfully applied to MRI reconstruction. However, the existing methods do
not consider the characteristics of multi-coil acquisition of MRI data.
Therefore, we give a new diffusion model, called SPIRiT-Diffusion, based on the
SPIRiT iterative reconstruction algorithm. Specifically, SPIRiT-Diffusion
characterizes the prior distribution of coil-by-coil images by score matching
and characterizes the k-space redundant prior between coils based on
self-consistency. With sufficient prior constraint utilized, we achieve
superior reconstruction results on the joint Intracranial and Carotid Vessel
Wall imaging dataset.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DifFace: Blind Face Restoration with Diffused Error Contraction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 13, 2022 </span>    
         <span class="authors"> Zongsheng Yue, Chen Change Loy </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.06512" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, I.4.4
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While deep learning-based methods for blind face restoration have achieved
unprecedented success, they still suffer from two major limitations. First,
most of them deteriorate when facing complex degradations out of their training
data. Second, these methods require multiple constraints, e.g., fidelity,
perceptual, and adversarial losses, which require laborious hyper-parameter
tuning to stabilize and balance their influences. In this work, we propose a
novel method named DifFace that is capable of coping with unseen and complex
degradations more gracefully without complicated loss designs. The key of our
method is to establish a posterior distribution from the observed low-quality
(LQ) image to its high-quality (HQ) counterpart. In particular, we design a
transition distribution from the LQ image to the intermediate state of a
pre-trained diffusion model and then gradually transmit from this intermediate
state to the HQ target by recursively applying a pre-trained diffusion model.
The transition distribution only relies on a restoration backbone that is
trained with $L_2$ loss on some synthetic data, which favorably avoids the
cumbersome training process in existing methods. Moreover, the transition
distribution can contract the error of the restoration backbone and thus makes
our method more robust to unknown degradations. Comprehensive experiments show
that DifFace is superior to current state-of-the-art methods, especially in
cases with severe degradations. Code and model are available at
https://github.com/zsyOAOA/DifFace.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## HS-Diffusion: Learning a Semantic-Guided Diffusion Model for Head Swapping
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 13, 2022 </span>    
         <span class="authors"> Qinghe Wang, Lijie Liu, Miao Hua, Qian He, Pengfei Zhu, Bing Cao, Qinghua Hu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.06458" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image-based head swapping task aims to stitch a source head to another source
body flawlessly. This seldom-studied task faces two major challenges: 1)
Preserving the head and body from various sources while generating a seamless
transition region. 2) No paired head swapping dataset and benchmark so far. In
this paper, we propose a semantic-mixing diffusion model for head swapping
(HS-Diffusion) which consists of a latent diffusion model (LDM) and a semantic
layout generator. We blend the semantic layouts of source head and source body,
and then inpaint the transition region by the semantic layout generator,
achieving a coarse-grained head swapping. Semantic-mixing LDM can further
implement a fine-grained head swapping with the inpainted layout as condition
by a progressive fusion process, while preserving head and body with
high-quality reconstruction. To this end, we propose a semantic calibration
strategy for natural inpainting and a neck alignment for geometric realism.
Importantly, we construct a new image-based head swapping benchmark and design
two tailor-designed metrics (Mask-FID and Focal-FID). Extensive experiments
demonstrate the superiority of our framework. The code will be available:
https://github.com/qinghew/HS-Diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 13, 2022 </span>    
         <span class="authors"> Dohyun Kwon, Ying Fan, Kangwook Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.06359" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.NA, math.NA
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models are shown to achieve remarkable empirical
performances in various applications such as image generation and audio
synthesis. However, a theoretical understanding of score-based diffusion models
is still incomplete. Recently, Song et al. showed that the training objective
of score-based generative models is equivalent to minimizing the
Kullback-Leibler divergence of the generated distribution from the data
distribution. In this work, we show that score-based models also minimize the
Wasserstein distance between them under suitable assumptions on the model.
Specifically, we prove that the Wasserstein distance is upper bounded by the
square root of the objective function up to multiplicative constants and a
fixed constant offset. Our proof is based on a novel application of the theory
of optimal transport, which can be of independent interest to the society. Our
numerical experiments support our findings. By analyzing our upper bounds, we
provide a few techniques to obtain tighter upper bounds.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## The Stable Artist: Steering Semantics in Diffusion Latent Space
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 12, 2022 </span>    
         <span class="authors"> Manuel Brack, Patrick Schramowski, Felix Friedrich, Dominik Hintersdorf, Kristian Kersting </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.06013" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large, text-conditioned generative diffusion models have recently gained a
lot of attention for their impressive performance in generating high-fidelity
images from text alone. However, achieving high-quality results is almost
unfeasible in a one-shot fashion. On the contrary, text-guided image generation
involves the user making many slight changes to inputs in order to iteratively
carve out the envisioned image. However, slight changes to the input prompt
often lead to entirely different images being generated, and thus the control
of the artist is limited in its granularity. To provide flexibility, we present
the Stable Artist, an image editing approach enabling fine-grained control of
the image generation process. The main component is semantic guidance (SEGA)
which steers the diffusion process along variable numbers of semantic
directions. This allows for subtle edits to images, changes in composition and
style, as well as optimization of the overall artistic conception. Furthermore,
SEGA enables probing of latent spaces to gain insights into the representation
of concepts learned by the model, even complex ones such as 'carbon emission'.
We demonstrate the Stable Artist on several tasks, showcasing high-quality
image editing and composition.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diff-Font: Diffusion Model for Robust One-Shot Font Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 12, 2022 </span>    
         <span class="authors"> Haibin He, Xinyuan Chen, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao, Yu Qiao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.05895" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Font generation is a difficult and time-consuming task, especially in those
languages using ideograms that have complicated structures with a large number
of characters, such as Chinese. To solve this problem, few-shot font generation
and even one-shot font generation have attracted a lot of attention. However,
most existing font generation methods may still suffer from (i) large
cross-font gap challenge; (ii) subtle cross-font variation problem; and (iii)
incorrect generation of complicated characters. In this paper, we propose a
novel one-shot font generation method based on a diffusion model, named
Diff-Font, which can be stably trained on large datasets. The proposed model
aims to generate the entire font library by giving only one sample as the
reference. Specifically, a large stroke-wise dataset is constructed, and a
stroke-wise diffusion model is proposed to preserve the structure and the
completion of each generated character. To our best knowledge, the proposed
Diff-Font is the first work that developed diffusion models to handle the font
generation task. The well-trained Diff-Font is not only robust to font gap and
font variation, but also achieved promising performance on difficult character
generation. Compared to previous font generation methods, our model reaches
state-of-the-art performance both qualitatively and quantitatively.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffAlign : Few-shot learning using diffusion based synthesis and alignment
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 11, 2022 </span>    
         <span class="authors"> Aniket Roy, Anshul Shah, Ketul Shah, Anirban Roy, Rama Chellappa </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.05404" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We address the problem of few-shot classification where the goal is to learn
a classifier from a limited set of samples. While data-driven learning is shown
to be effective in various applications, learning from less data still remains
challenging. To address this challenge, existing approaches consider various
data augmentation techniques for increasing the number of training samples.
Pseudo-labeling is commonly used in a few-shot setup, where approximate labels
are estimated for a large set of unlabeled images. We propose DiffAlign which
focuses on generating images from class labels. Specifically, we leverage the
recent success of the generative models (e.g., DALL-E and diffusion models)
that can generate realistic images from texts. However, naive learning on
synthetic images is not adequate due to the domain gap between real and
synthetic images. Thus, we employ a maximum mean discrepancy (MMD) loss to
align the synthetic images to the real images minimizing the domain gap. We
evaluate our method on the standard few-shot classification benchmarks:
CIFAR-FS, FC100, miniImageNet, tieredImageNet and a cross-domain few-shot
classification benchmark: miniImageNet to CUB. The proposed approach
significantly outperforms the stateof-the-art in both 5-shot and 1-shot setups
on these benchmarks. Our approach is also shown to be effective in the
zero-shot classification setup
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## How to Backdoor Diffusion Models?
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 11, 2022 </span>    
         <span class="authors"> Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.05400" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are state-of-the-art deep learning empowered generative
models that are trained based on the principle of learning forward and reverse
diffusion processes via progressive noise-addition and denoising. To gain a
better understanding of the limitations and potential risks, this paper
presents the first study on the robustness of diffusion models against backdoor
attacks. Specifically, we propose BadDiffusion, a novel attack framework that
engineers compromised diffusion processes during model training for backdoor
implantation. At the inference stage, the backdoored diffusion model will
behave just like an untampered generator for regular data inputs, while falsely
generating some targeted outcome designed by the bad actor upon receiving the
implanted trigger signal. Such a critical risk can be dreadful for downstream
tasks and applications built upon the problematic model. Our extensive
experiments on various backdoor attack settings show that BadDiffusion can
consistently lead to compromised diffusion models with high utility and target
specificity. Even worse, BadDiffusion can be made cost-effective by simply
finetuning a clean pre-trained diffusion model to implant backdoors. We also
explore some possible countermeasures for risk mitigation. Our results call
attention to potential risks and possible misuse of diffusion models. Our code
is available on https://github.com/IBM/BadDiffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 09, 2022 </span>    
         <span class="authors"> Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang, Yufei Wang, Hanspeter Pfister, Bihan Wen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.04711" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, I.2.10; I.5.4
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent deep learning methods have achieved promising results in image shadow
removal. However, their restored images still suffer from unsatisfactory
boundary artifacts, due to the lack of degradation prior embedding and the
deficiency in modeling capacity. Our work addresses these issues by proposing a
unified diffusion framework that integrates both the image and degradation
priors for highly effective shadow removal. In detail, we first propose a
shadow degradation model, which inspires us to build a novel unrolling
diffusion model, dubbed ShandowDiffusion. It remarkably improves the model's
capacity in shadow removal via progressively refining the desired output with
both degradation prior and diffusive generative prior, which by nature can
serve as a new strong baseline for image restoration. Furthermore,
ShadowDiffusion progressively refines the estimated shadow mask as an auxiliary
task of the diffusion generator, which leads to more accurate and robust
shadow-free image generation. We conduct extensive experiments on three popular
public datasets, including ISTD, ISTD+, and SRD, to validate our method's
effectiveness. Compared to the state-of-the-art methods, our model achieves a
significant improvement in terms of PSNR, increasing from 31.69dB to 34.73dB
over SRD dataset.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 08, 2022 </span>    
         <span class="authors"> Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, Christian Theobalt </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.04495" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Conventional methods for human motion synthesis are either deterministic or
struggle with the trade-off between motion diversity and motion quality. In
response to these limitations, we introduce MoFusion, i.e., a new
denoising-diffusion-based framework for high-quality conditional human motion
synthesis that can generate long, temporally plausible, and semantically
accurate motions based on a range of conditioning contexts (such as music and
text). We also present ways to introduce well-known kinematic losses for motion
plausibility within the motion diffusion framework through our scheduled
weighting strategy. The learned latent space can be used for several
interactive motion editing applications -- like inbetweening, seed
conditioning, and text-based editing -- thus, providing crucial abilities for
virtual character animation and robotics. Through comprehensive quantitative
evaluations and a perceptual user study, we demonstrate the effectiveness of
MoFusion compared to the state of the art on established benchmarks in the
literature. We urge the reader to watch our supplementary video and visit
https://vcai.mpi-inf.mpg.de/projects/MoFusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SINE: SINgle Image Editing with Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 08, 2022 </span>    
         <span class="authors"> Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, Jian Ren </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.04489" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent works on diffusion models have demonstrated a strong capability for
conditioning image generation, e.g., text-guided image synthesis. Such success
inspires many efforts trying to use large-scale pre-trained diffusion models
for tackling a challenging problem--real image editing. Works conducted in this
area learn a unique textual token corresponding to several images containing
the same object. However, under many circumstances, only one image is
available, such as the painting of the Girl with a Pearl Earring. Using
existing works on fine-tuning the pre-trained diffusion models with a single
image causes severe overfitting issues. The information leakage from the
pre-trained diffusion models makes editing can not keep the same content as the
given image while creating new features depicted by the language guidance. This
work aims to address the problem of single-image editing. We propose a novel
model-based guidance built upon the classifier-free guidance so that the
knowledge from the model trained on a single image can be distilled into the
pre-trained diffusion model, enabling content creation even with one given
image. Additionally, we propose a patch-based fine-tuning that can effectively
help the model generate images of arbitrary resolution. We provide extensive
experiments to validate the design choices of our approach and show promising
editing capabilities, including changing style, content addition, and object
manipulation. The code is available for research purposes at
https://github.com/zhang-zx/SINE.git .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Guided Domain Adaptation of Image Generators
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 08, 2022 </span>    
         <span class="authors"> Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, Ahmed Elgammal </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.04473" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Can a text-to-image diffusion model be used as a training objective for
adapting a GAN generator to another domain? In this paper, we show that the
classifier-free guidance can be leveraged as a critic and enable generators to
distill knowledge from large-scale text-to-image diffusion models. Generators
can be efficiently shifted into new domains indicated by text prompts without
access to groundtruth samples from target domains. We demonstrate the
effectiveness and controllability of our method through extensive experiments.
Although not trained to minimize CLIP loss, our model achieves equally high
CLIP scores and significantly lower FID than prior work on short prompts, and
outperforms the baseline qualitatively and quantitatively on long and
complicated prompts. To our best knowledge, the proposed method is the first
attempt at incorporating large-scale pre-trained diffusion models and
distillation sampling for text-driven image generator domain adaptation and
gives a quality previously beyond possible. Moreover, we extend our work to
3D-aware style-based generators and DreamBooth guidance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Executing your Commands via Motion Diffusion in Latent Space
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 08, 2022 </span>    
         <span class="authors"> Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, Gang Yu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.04048" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We study a challenging task, conditional human motion generation, which
produces plausible human motion sequences according to various conditional
inputs, such as action classes or textual descriptors. Since human motions are
highly diverse and have a property of quite different distribution from
conditional modalities, such as textual descriptors in natural languages, it is
hard to learn a probabilistic mapping from the desired conditional modality to
the human motion sequences. Besides, the raw motion data from the motion
capture system might be redundant in sequences and contain noises; directly
modeling the joint distribution over the raw motion sequences and conditional
modalities would need a heavy computational overhead and might result in
artifacts introduced by the captured noises. To learn a better representation
of the various human motion sequences, we first design a powerful Variational
AutoEncoder (VAE) and arrive at a representative and low-dimensional latent
code for a human motion sequence. Then, instead of using a diffusion model to
establish the connections between the raw motion sequences and the conditional
inputs, we perform a diffusion process on the motion latent space. Our proposed
Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences
conforming to the given conditional inputs and substantially reduce the
computational overhead in both the training and inference stages. Extensive
experiments on various human motion generation tasks demonstrate that our MLD
achieves significant improvements over the state-of-the-art methods among
extensive human motion generation tasks, with two orders of magnitude faster
than previous diffusion models on raw motion sequences.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 07, 2022 </span>    
         <span class="authors"> Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, Tom Goldstein </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.03860" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.CY
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Cutting-edge diffusion models produce images with high quality and
customizability, enabling them to be used for commercial art and graphic design
purposes. But do diffusion models create unique works of art, or are they
replicating content directly from their training sets? In this work, we study
image retrieval frameworks that enable us to compare generated images with
training samples and detect when content has been replicated. Applying our
frameworks to diffusion models trained on multiple datasets including Oxford
flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training
set size impact rates of content replication. We also identify cases where
diffusion models, including the popular Stable Diffusion model, blatantly copy
from their training data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 07, 2022 </span>    
         <span class="authors"> Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn Wong, Baoyuan Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.04248" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.GR, cs.CV, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we introduce a simple and novel framework for one-shot
audio-driven talking head generation. Unlike prior works that require
additional driving sources for controlled synthesis in a deterministic manner,
we instead probabilistically sample all the holistic lip-irrelevant facial
motions (i.e. pose, expression, blink, gaze, etc.) to semantically match the
input audio while still maintaining both the photo-realism of audio-lip
synchronization and the overall naturalness. This is achieved by our newly
proposed audio-to-visual diffusion prior trained on top of the mapping between
audio and disentangled non-lip facial representations. Thanks to the
probabilistic nature of the diffusion prior, one big advantage of our framework
is it can synthesize diverse facial motion sequences given the same audio clip,
which is quite user-friendly for many real applications. Through comprehensive
evaluations on public benchmarks, we conclude that (1) our diffusion prior
outperforms auto-regressive prior significantly on almost all the concerned
metrics; (2) our overall system is competitive with prior works in terms of
audio-lip synchronization but can effectively sample rich and natural-looking
lip-irrelevant facial motions while still semantically harmonized with the
audio input.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## One Sample Diffusion Model in Projection Domain for Low-Dose CT Imaging
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 07, 2022 </span>    
         <span class="authors"> Bin Huang, Liu Zhang, Shiyu Lu, Boyu Lin, Weiwen Wu, Qiegen Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.03630" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Low-dose computed tomography (CT) plays a significant role in reducing the
radiation risk in clinical applications. However, lowering the radiation dose
will significantly degrade the image quality. With the rapid development and
wide application of deep learning, it has brought new directions for the
development of low-dose CT imaging algorithms. Therefore, we propose a fully
unsupervised one sample diffusion model (OSDM)in projection domain for low-dose
CT reconstruction. To extract sufficient prior information from single sample,
the Hankel matrix formulation is employed. Besides, the penalized weighted
least-squares and total variation are introduced to achieve superior image
quality. Specifically, we first train a score-based generative model on one
sinogram by extracting a great number of tensors from the structural-Hankel
matrix as the network input to capture prior distribution. Then, at the
inference stage, the stochastic differential equation solver and data
consistency step are performed iteratively to obtain the sinogram data.
Finally, the final image is obtained through the filtered back-projection
algorithm. The reconstructed results are approaching to the normal-dose
counterparts. The results prove that OSDM is practical and effective model for
reducing the artifacts and preserving the image quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Proposal of a Score Based Approach to Sampling Using Monte Carlo Estimation of Score and Oracle Access to Target Density
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 06, 2022 </span>    
         <span class="authors"> Curtis McDonald, Andrew Barron </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.03325" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, stat.ME
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score based approaches to sampling have shown much success as a generative
algorithm to produce new samples from a target density given a pool of initial
samples. In this work, we consider if we have no initial samples from the
target density, but rather $0^{th}$ and $1^{st}$ order oracle access to the log
likelihood. Such problems may arise in Bayesian posterior sampling, or in
approximate minimization of non-convex functions. Using this knowledge alone,
we propose a Monte Carlo method to estimate the score empirically as a
particular expectation of a random variable. Using this estimator, we can then
run a discrete version of the backward flow SDE to produce samples from the
target density. This approach has the benefit of not relying on a pool of
initial samples from the target density, and it does not rely on a neural
network or other black box model to estimate the score.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-SDF: Text-to-Shape via Voxelized Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 06, 2022 </span>    
         <span class="authors"> Muheng Li, Yueqi Duan, Jie Zhou, Jiwen Lu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.03293" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 With the rising industrial attention to 3D virtual modeling technology,
generating novel 3D content based on specified conditions (e.g. text) has
become a hot issue. In this paper, we propose a new generative 3D modeling
framework called Diffusion-SDF for the challenging task of text-to-shape
synthesis. Previous approaches lack flexibility in both 3D data representation
and shape generation, thereby failing to generate highly diversified 3D shapes
conforming to the given text descriptions. To address this, we propose a SDF
autoencoder together with the Voxelized Diffusion model to learn and generate
representations for voxelized signed distance fields (SDFs) of 3D shapes.
Specifically, we design a novel UinU-Net architecture that implants a
local-focused inner network inside the standard U-Net architecture, which
enables better reconstruction of patch-independent SDF representations. We
extend our approach to further text-to-shape tasks including text-conditioned
shape completion and manipulation. Experimental results show that Diffusion-SDF
generates both higher quality and more diversified 3D shapes that conform well
to given text descriptions when compared to previous approaches. Code is
available at: https://github.com/ttlmh/Diffusion-SDF
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 06, 2022 </span>    
         <span class="authors"> Congyue Deng, Chiyu "Max'' Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.03267" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 2D-to-3D reconstruction is an ill-posed problem, yet humans are good at
solving this problem due to their prior knowledge of the 3D world developed
over years. Driven by this observation, we propose NeRDi, a single-view NeRF
synthesis framework with general image priors from 2D diffusion models.
Formulating single-view reconstruction as an image-conditioned 3D generation
problem, we optimize the NeRF representations by minimizing a diffusion loss on
its arbitrary view renderings with a pretrained image diffusion model under the
input-view constraint. We leverage off-the-shelf vision-language models and
introduce a two-section language guidance as conditioning inputs to the
diffusion model. This is essentially helpful for improving multiview content
coherence as it narrows down the general image prior conditioned on the
semantic and visual features of the single-view input image. Additionally, we
introduce a geometric loss based on estimated depth maps to regularize the
underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset
show that our method can synthesize novel views with higher quality even
compared to existing methods trained on this dataset. We also demonstrate our
generalizability in zero-shot NeRF synthesis for in-the-wild images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 01, 2022 </span>    
         <span class="authors"> Zhizhuo Zhou, Shubham Tulsiani </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.00792" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose SparseFusion, a sparse view 3D reconstruction approach that
unifies recent advances in neural rendering and probabilistic image generation.
Existing approaches typically build on neural rendering with re-projected
features but fail to generate unseen regions or handle uncertainty under large
viewpoint changes. Alternate methods treat this as a (probabilistic) 2D
synthesis task, and while they can generate plausible 2D images, they do not
infer a consistent underlying 3D. However, we find that this trade-off between
3D consistency and probabilistic image generation does not need to exist. In
fact, we show that geometric consistency and generative inference can be
complementary in a mode-seeking behavior. By distilling a 3D consistent scene
representation from a view-conditioned latent diffusion model, we are able to
recover a plausible 3D representation whose renderings are both accurate and
realistic. We evaluate our approach across 51 categories in the CO3D dataset
and show that it outperforms existing methods, in both distortion and
perception metrics, for sparse-view novel view synthesis.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 01, 2022 </span>    
         <span class="authors"> Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, Greg Shakhnarovich </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.00774" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 A diffusion model learns to predict a vector field of gradients. We propose
to apply chain rule on the learned gradients, and back-propagate the score of a
diffusion model through the Jacobian of a differentiable renderer, which we
instantiate to be a voxel radiance field. This setup aggregates 2D scores at
multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D
model for 3D data generation. We identify a technical challenge of distribution
mismatch that arises in this application, and propose a novel estimation
mechanism to resolve it. We run our algorithm on several off-the-shelf
diffusion image generative models, including the recently released Stable
Diffusion trained on the large-scale LAION dataset.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## VIDM: Video Implicit Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 01, 2022 </span>    
         <span class="authors"> Kangfu Mei, Vishal M. Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.00235" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have emerged as a powerful generative method for
synthesizing high-quality and diverse set of images. In this paper, we propose
a video generation method based on diffusion models, where the effects of
motion are modeled in an implicit condition manner, i.e. one can sample
plausible video motions according to the latent feature of frames. We improve
the quality of the generated videos by proposing multiple strategies such as
sampling space truncation, robustness penalty, and positional group
normalization. Various experiments are conducted on datasets consisting of
videos with different resolutions and different number of frames. Results show
that the proposed method outperforms the state-of-the-art generative
adversarial network-based methods by a significant margin in terms of FVD
scores as well as perceptible visual quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Shape-Guided Diffusion with Inside-Outside Attention
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 01, 2022 </span>    
         <span class="authors"> Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu, Maka Karalashvili, Anna Rohrbach, Trevor Darrell </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.00210" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 When manipulating an object, existing text-to-image diffusion models often
ignore the shape of the object and generate content that is incorrectly scaled,
cut off, or replaced with background content. We propose a training-free
method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be
sensitive to shape input specified by a user or automatically inferred from
text. We use a novel Inside-Outside Attention mechanism during the inversion
and generation process to apply this shape constraint to the cross- and
self-attention maps. Our mechanism designates which spatial region is the
object (inside) vs. background (outside) then associates edits specified by
text prompts to the correct region. We demonstrate the efficacy of our method
on the shape-guided editing task, where the model must replace an object
according to a text prompt and object mask. We curate a new ShapePrompts
benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness
without a degradation in text alignment or image realism according to both
automatic metrics and annotator ratings. Our data and code will be made
available at https://shape-guided-diffusion.github.io.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion for Sampling SAT Solutions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 30, 2022 </span>    
         <span class="authors"> Karlis Freivalds, Sergejs Kozlovics </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2212.00121" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generating diverse solutions to the Boolean Satisfiability Problem (SAT) is a
hard computational problem with practical applications for testing and
functional verification of software and hardware designs. We explore the way to
generate such solutions using Denoising Diffusion coupled with a Graph Neural
Network to implement the denoising function. We find that the obtained accuracy
is similar to the currently best purely neural method and the produced SAT
solutions are highly diverse, even if the system is trained with non-random
solutions from a standard solver.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Multiresolution Textual Inversion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 30, 2022 </span>    
         <span class="authors"> Giannis Daras, Alexandros G. Dimakis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.17115" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We extend Textual Inversion to learn pseudo-words that represent a concept at
different resolutions. This allows us to generate images that use the concept
with different levels of detail and also to manipulate different resolutions
using language. Once learned, the user can generate images at different levels
of agreement to the original concept; "A photo of $S^*(0)$" produces the exact
object while the prompt "A photo of $S^*(0.8)$" only matches the rough outlines
and colors. Our framework allows us to generate images that use different
resolutions of an image (e.g. details, textures, styles) as separate
pseudo-words that can be composed in various ways. We open-soure our code in
the following URL: https://github.com/giannisdaras/multires_textual_inversion
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## High-Fidelity Guided Image Synthesis with Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 30, 2022 </span>    
         <span class="authors"> Jaskirat Singh, Stephen Gould, Liang Zheng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.17084" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Controllable image synthesis with user scribbles has gained huge public
interest with the recent advent of text-conditioned latent diffusion models.
The user scribbles control the color composition while the text prompt provides
control over the overall image semantics. However, we note that prior works in
this direction suffer from an intrinsic domain shift problem, wherein the
generated outputs often lack details and resemble simplistic representations of
the target domain. In this paper, we propose a novel guided image synthesis
framework, which addresses this problem by modeling the output image as the
solution of a constrained optimization problem. We show that while computing an
exact solution to the optimization is infeasible, an approximation of the same
can be achieved while just requiring a single pass of the reverse diffusion
process. Additionally, we show that by simply defining a cross-attention based
correspondence between the input text tokens and the user stroke-painting, the
user is also able to control the semantics of different painted regions without
requiring any conditional training or finetuning. Human user study results show
that the proposed approach outperforms the previous state-of-the-art by over
85.32% on the overall user satisfaction scores. Project page for our paper is
available at https://1jsingh.github.io/gradop.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffPose: Toward More Reliable 3D Pose Estimation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 30, 2022 </span>    
         <span class="authors"> Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hossein Rahmani, Jun Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.16940" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Monocular 3D human pose estimation is quite challenging due to the inherent
ambiguity and occlusion, which often lead to high uncertainty and
indeterminacy. On the other hand, diffusion models have recently emerged as an
effective tool for generating high-quality images from noise. Inspired by their
capability, we explore a novel pose estimation framework (DiffPose) that
formulates 3D pose estimation as a reverse diffusion process. We incorporate
novel designs into our DiffPose to facilitate the diffusion process for 3D pose
estimation: a pose-specific initialization of pose uncertainty distributions, a
Gaussian Mixture Model-based forward diffusion process, and a
context-conditioned reverse diffusion process. Our proposed DiffPose
significantly outperforms existing methods on the widely used pose estimation
benchmarks Human3.6M and MPI-INF-3DHP. Project page:
https://gongjia0208.github.io/Diffpose/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Continuous-time Discrete Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 30, 2022 </span>    
         <span class="authors"> Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, Hanjun Dai </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.16750" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based modeling through stochastic differential equations (SDEs) has
provided a new perspective on diffusion models, and demonstrated superior
performance on continuous data. However, the gradient of the log-likelihood
function, i.e., the score function, is not properly defined for discrete
spaces. This makes it non-trivial to adapt \textcolor{\cdiff}{the score-based
modeling} to categorical data. In this paper, we extend diffusion models to
discrete variables by introducing a stochastic jump process where the reverse
process denoises via a continuous-time Markov chain. This formulation admits an
analytical simulation during backward sampling. To learn the reverse process,
we extend score matching to general categorical data and show that an unbiased
estimator can be obtained via simple matching of the conditional marginal
distributions. We demonstrate the effectiveness of the proposed method on a set
of synthetic and real-world music and image benchmarks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## 3D Neural Field Generation using Triplane Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 30, 2022 </span>    
         <span class="authors"> J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, Gordon Wetzstein </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.16677" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have emerged as the state-of-the-art for image generation,
among other tasks. Here, we present an efficient diffusion-based model for
3D-aware generation of neural fields. Our approach pre-processes training data,
such as ShapeNet meshes, by converting them to continuous occupancy fields and
factoring them into a set of axis-aligned triplane feature representations.
Thus, our 3D training scenes are all represented by 2D feature planes, and we
can directly train existing 2D diffusion models on these representations to
generate 3D neural fields with high quality and diversity, outperforming
alternative approaches to 3D-aware generation. Our approach requires essential
modifications to existing triplane factorization pipelines to make the
resulting features easy to learn for the diffusion model. We demonstrate
state-of-the-art results on 3D generation on several object classes from
ShapeNet.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SinDDM: A Single Image Denoising Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 29, 2022 </span>    
         <span class="authors"> Vladimir Kulikov, Shahar Yadin, Matan Kleiner, Tomer Michaeli </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.16582" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models (DDMs) have led to staggering performance leaps in
image generation, editing and restoration. However, existing DDMs use very
large datasets for training. Here, we introduce a framework for training a DDM
on a single image. Our method, which we coin SinDDM, learns the internal
statistics of the training image by using a multi-scale diffusion process. To
drive the reverse diffusion process, we use a fully-convolutional light-weight
denoiser, which is conditioned on both the noise level and the scale. This
architecture allows generating samples of arbitrary dimensions, in a
coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality
samples, and is applicable in a wide array of tasks, including style transfer
and harmonization. Furthermore, it can be easily guided by external
supervision. Particularly, we demonstrate text-guided generation from a single
image using a pre-trained CLIP model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 29, 2022 </span>    
         <span class="authors"> Karl Holmquist, Bastian Wandt </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.16487" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Traditionally, monocular 3D human pose estimation employs a machine learning
model to predict the most likely 3D pose for a given input image. However, a
single image can be highly ambiguous and induces multiple plausible solutions
for the 2D-3D lifting step which results in overly confident 3D pose
predictors. To this end, we propose \emph{DiffPose}, a conditional diffusion
model, that predicts multiple hypotheses for a given input image. In comparison
to similar approaches, our diffusion model is straightforward and avoids
intensive hyperparameter tuning, complex network structures, mode collapse, and
unstable training. Moreover, we tackle a problem of the common two-step
approach that first estimates a distribution of 2D joint locations via
joint-wise heatmaps and consecutively approximates them based on first- or
second-moment statistics. Since such a simplification of the heatmaps removes
valid information about possibly correct, though labeled unlikely, joint
locations, we propose to represent the heatmaps as a set of 2D joint candidate
samples. To extract information about the original distribution from these
samples we introduce our \emph{embedding transformer} that conditions the
diffusion model. Experimentally, we show that DiffPose slightly improves upon
the state of the art for multi-hypothesis pose estimation for simple poses and
outperforms it by a large margin for highly ambiguous poses.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Wavelet Diffusion Models are fast and scalable Image Generators
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 29, 2022 </span>    
         <span class="authors"> Hao Phung, Quan Dao, Anh Tran </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.16152" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are rising as a powerful solution for high-fidelity image
generation, which exceeds GANs in quality in many circumstances. However, their
slow training and inference speed is a huge bottleneck, blocking them from
being used in real-time applications. A recent DiffusionGAN method
significantly decreases the models' running time by reducing the number of
sampling steps from thousands to several, but their speeds still largely lag
behind the GAN counterparts. This paper aims to reduce the speed gap by
proposing a novel wavelet-based diffusion scheme. We extract low-and-high
frequency components from both image and feature levels via wavelet
decomposition and adaptively handle these components for faster processing
while maintaining good generation quality. Furthermore, we propose to use a
reconstruction term, which effectively boosts the model training convergence.
Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets
prove our solution is a stepping-stone to offering real-time and high-fidelity
diffusion models. Our code and pre-trained checkpoints are available at
\url{https://github.com/VinAIResearch/WaveDiff.git}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Dimensionality-Varying Diffusion Process
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 29, 2022 </span>    
         <span class="authors"> Han Zhang, Ruili Feng, Zhantao Yang, Lianghua Huang, Yu Liu, Yifei Zhang, Yujun Shen, Deli Zhao, Jingren Zhou, Fan Cheng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.16032" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models, which learn to reverse a signal destruction process to
generate new data, typically require the signal at each step to have the same
dimension. We argue that, considering the spatial redundancy in image signals,
there is no need to maintain a high dimensionality in the evolution process,
especially in the early generation phase. To this end, we make a theoretical
generalization of the forward diffusion process via signal decomposition.
Concretely, we manage to decompose an image into multiple orthogonal components
and control the attenuation of each component when perturbing the image. That
way, along with the noise strength increasing, we are able to diminish those
inconsequential components and thus use a lower-dimensional signal to represent
the source, barely losing information. Such a reformulation allows to vary
dimensions in both training and inference of diffusion models. Extensive
experiments on a range of datasets suggest that our approach substantially
reduces the computational cost and achieves on-par or even better synthesis
performance compared to baseline methods. We also show that our strategy
facilitates high-resolution image synthesis and improves FID of diffusion model
trained on FFHQ at $1024\times1024$ resolution from 52.40 to 10.46. Code and
models will be made publicly available.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 28, 2022 </span>    
         <span class="authors"> Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.17091" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The proposed method, Discriminator Guidance, aims to improve sample
generation of pre-trained diffusion models. The approach introduces a
discriminator that gives explicit supervision to a denoising sample path
whether it is realistic or not. Unlike GANs, our approach does not require
joint training of score and discriminator networks. Instead, we train the
discriminator after score training, making discriminator training stable and
fast to converge. In sample generation, we add an auxiliary term to the
pre-trained score to deceive the discriminator. This term corrects the model
score to the data score at the optimal discriminator, which implies that the
discriminator helps better score estimation in a complementary way. Using our
algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83
and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).
We release the code at https://github.com/alsdudrla10/DG.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Post-training Quantization on Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 28, 2022 </span>    
         <span class="authors"> Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, Yan Yan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.15736" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion (score-based) generative models have recently achieved
significant accomplishments in generating realistic and diverse data. These
approaches define a forward diffusion process for transforming data into noise
and a backward denoising process for sampling data from noise. Unfortunately,
the generation process of current denoising diffusion models is notoriously
slow due to the lengthy iterative noise estimations, which rely on cumbersome
neural networks. It prevents the diffusion models from being widely deployed,
especially on edge devices. Previous works accelerate the generation process of
diffusion model (DM) via finding shorter yet effective sampling trajectories.
However, they overlook the cost of noise estimation with a heavy network in
every iteration. In this work, we accelerate generation from the perspective of
compressing the noise estimation network. Due to the difficulty of retraining
DMs, we exclude mainstream training-aware compression paradigms and introduce
post-training quantization (PTQ) into DM acceleration. However, the output
distributions of noise estimation networks change with time-step, making
previous PTQ methods fail in DMs since they are designed for single-time step
scenarios. To devise a DM-specific PTQ method, we explore PTQ on DM in three
aspects: quantized operations, calibration dataset, and calibration metric. We
summarize and use several observations derived from all-inclusive
investigations to formulate our method, which especially targets the unique
multi-time-step structure of DMs. Experimentally, our method can directly
quantize full-precision DMs into 8-bit models while maintaining or even
improving their performance in a training-free manner. Importantly, our method
can serve as a plug-and-play module on other fast-sampling methods, e.g., DDIM.
The code is available at https://github.com/42Shawn/PTQ4DM .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Continuous diffusion for categorical data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 28, 2022 </span>    
         <span class="authors"> Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, Rémi Leblond, Will Grathwohl, Jonas Adler </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.15089" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have quickly become the go-to paradigm for generative
modelling of perceptual signals (such as images and sound) through iterative
refinement. Their success hinges on the fact that the underlying physical
phenomena are continuous. For inherently discrete and categorical data such as
language, various diffusion-inspired alternatives have been proposed. However,
the continuous nature of diffusion models conveys many benefits, and in this
work we endeavour to preserve it. We propose CDCD, a framework for modelling
categorical data with diffusion models that are continuous both in time and
input space. We demonstrate its efficacy on several language modelling tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 28, 2022 </span>    
         <span class="authors"> Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, Xipeng Qiu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.15029" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present DiffusionBERT, a new generative masked language model based on
discrete diffusion models. Diffusion models and many pre-trained language
models have a shared training objective, i.e., denoising, making it possible to
combine the two powerful models and enjoy the best of both worlds. On the one
hand, diffusion models offer a promising training strategy that helps improve
the generation quality. On the other hand, pre-trained denoising language
models (e.g., BERT) can be used as a good initialization that accelerates
convergence. We explore training BERT to learn the reverse process of a
discrete diffusion process with an absorbing state and elucidate several
designs to improve it. First, we propose a new noise schedule for the forward
diffusion process that controls the degree of noise added at each step based on
the information of each token. Second, we investigate several designs of
incorporating the time step into BERT. Experiments on unconditional text
generation demonstrate that DiffusionBERT achieves significant improvement over
existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous
generative masked language models in terms of perplexity and BLEU score.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Probabilistic Model Made Slim
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 27, 2022 </span>    
         <span class="authors"> Xingyi Yang, Daquan Zhou, Jiashi Feng, Xinchao Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.17106" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite the recent visually-pleasing results achieved, the massive
computational cost has been a long-standing flaw for diffusion probabilistic
models (DPMs), which, in turn, greatly limits their applications on
resource-limited platforms. Prior methods towards efficient DPM, however, have
largely focused on accelerating the testing yet overlooked their huge
complexity and sizes. In this paper, we make a dedicated attempt to lighten DPM
while striving to preserve its favourable performance. We start by training a
small-sized latent diffusion model (LDM) from scratch, but observe a
significant fidelity drop in the synthetic images. Through a thorough
assessment, we find that DPM is intrinsically biased against high-frequency
generation, and learns to recover different frequency components at different
time-steps. These properties make compact networks unable to represent
frequency dynamics with accurate high-frequency estimation. Towards this end,
we introduce a customized design for slim DPM, which we term as Spectral
Diffusion (SD), for light-weight image synthesis. SD incorporates wavelet
gating in its architecture to enable frequency dynamic feature extraction at
every reverse steps, and conducts spectrum-aware distillation to promote
high-frequency recovery by inverse weighting the objective based on spectrum
magni tudes. Experimental results demonstrate that, SD achieves 8-18x
computational complexity reduction as compared to the latent diffusion models
on a series of conditional and unconditional image generation tasks while
retaining competitive image fidelity.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Traditional Classification Neural Networks are Good Generators: They are Competitive with DDPMs and GANs
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 27, 2022 </span>    
         <span class="authors"> Guangrun Wang, Philip H. S. Torr </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.14794" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, cs.MM, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Classifiers and generators have long been separated. We break down this
separation and showcase that conventional neural network classifiers can
generate high-quality images of a large number of categories, being comparable
to the state-of-the-art generative models (e.g., DDPMs and GANs). We achieve
this by computing the partial derivative of the classification loss function
with respect to the input to optimize the input to produce an image. Since it
is widely known that directly optimizing the inputs is similar to targeted
adversarial attacks incapable of generating human-meaningful images, we propose
a mask-based stochastic reconstruction module to make the gradients
semantic-aware to synthesize plausible images. We further propose a
progressive-resolution technique to guarantee fidelity, which produces
photorealistic images. Furthermore, we introduce a distance metric loss and a
non-trivial distribution loss to ensure classification neural networks can
synthesize diverse and high-fidelity images. Using traditional neural network
classifiers, we can generate good-quality images of 256$\times$256 resolution
on ImageNet. Intriguingly, our method is also applicable to text-to-image
generation by regarding image-text foundation models as generalized
classifiers.
  Proving that classifiers have learned the data distribution and are ready for
image generation has far-reaching implications, for classifiers are much easier
to train than generative models like DDPMs and GANs. We don't even need to
train classification models because tons of public ones are available for
download. Also, this holds great potential for the interpretability and
robustness of classifiers. Project page is at
\url{https://classifier-as-generator.github.io/}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 25, 2022 </span>    
         <span class="authors"> German Barquero, Sergio Escalera, Cristina Palmero </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.14304" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Stochastic human motion prediction (HMP) has generally been tackled with
generative adversarial networks and variational autoencoders. Most prior works
aim at predicting highly diverse movements in terms of the skeleton joints'
dispersion. This has led to methods predicting fast and motion-divergent
movements, which are often unrealistic and incoherent with past motion. Such
methods also neglect contexts that need to anticipate diverse low-range
behaviors, or actions, with subtle joint displacements. To address these
issues, we present BeLFusion, a model that, for the first time, leverages
latent diffusion models in HMP to sample from a latent space where behavior is
disentangled from pose and motion. As a result, diversity is encouraged from a
behavioral perspective. Thanks to our behavior coupler's ability to transfer
sampled behavior to ongoing motion, BeLFusion's predictions display a variety
of behaviors that are significantly more realistic than the state of the art.
To support it, we introduce two metrics, the Area of the Cumulative Motion
Distribution, and the Average Pairwise Distance Error, which are correlated to
our definition of realism according to a qualitative study with 126
participants. Finally, we prove BeLFusion's generalization power in a new
cross-dataset scenario for stochastic HMP.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Latent Space Diffusion Models of Cryo-EM Structures
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 25, 2022 </span>    
         <span class="authors"> Karsten Kreis, Tim Dockhorn, Zihao Li, Ellen Zhong </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.14169" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.QM, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Cryo-electron microscopy (cryo-EM) is unique among tools in structural
biology in its ability to image large, dynamic protein complexes. Key to this
ability is image processing algorithms for heterogeneous cryo-EM
reconstruction, including recent deep learning-based approaches. The
state-of-the-art method cryoDRGN uses a Variational Autoencoder (VAE) framework
to learn a continuous distribution of protein structures from single particle
cryo-EM imaging data. While cryoDRGN can model complex structural motions, the
Gaussian prior distribution of the VAE fails to match the aggregate approximate
posterior, which prevents generative sampling of structures especially for
multi-modal distributions (e.g. compositional heterogeneity). Here, we train a
diffusion model as an expressive, learnable prior in the cryoDRGN framework.
Our approach learns a high-quality generative model over molecular
conformations directly from cryo-EM imaging data. We show the ability to sample
from the model on two synthetic and two real datasets, where samples accurately
follow the data distribution unlike samples from the VAE prior distribution. We
also demonstrate how the diffusion model prior can be leveraged for fast latent
space traversal and interpolation between states of interest. By learning an
accurate model of the data distribution, our method unlocks tools in generative
modeling, sampling, and distribution analysis for heterogeneous cryo-EM
ensembles.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## 3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 25, 2022 </span>    
         <span class="authors"> Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.14108" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-guided diffusion models have shown superior performance in image/video
generation and editing. While few explorations have been performed in 3D
scenarios. In this paper, we discuss three fundamental and interesting problems
on this topic. First, we equip text-guided diffusion models to achieve
$\textbf{3D-consistent generation}$. Specifically, we integrate a NeRF-like
neural field to generate low-resolution coarse results for a given camera view.
Such results can provide 3D priors as condition information for the following
diffusion process. During denoising diffusion, we further enhance the 3D
consistency by modeling cross-view correspondences with a novel two-stream
(corresponding to two different views) asynchronous diffusion process. Second,
we study $\textbf{3D local editing}$ and propose a two-step solution that can
generate 360$^{\circ}$ manipulated results by editing an object from a single
view. Step 1, we propose to perform 2D local editing by blending the predicted
noises. Step 2, we conduct a noise-to-text inversion process that maps 2D
blended noises into the view-independent text embedding space. Once the
corresponding text embedding is obtained, 360$^{\circ}$ images can be
generated. Last but not least, we extend our model to perform \textbf{one-shot
novel view synthesis} by fine-tuning on a single image, firstly showing the
potential of leveraging text guidance for novel view synthesis. Extensive
experiments and various applications show the prowess of our 3DDesigner. The
project page is available at https://3ddesigner-diffusion.github.io/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionSDF: Conditional Generative Modeling of Signed Distance Functions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 24, 2022 </span>    
         <span class="authors"> Gene Chou, Yuval Bahat, Felix Heide </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.13757" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Probabilistic diffusion models have achieved state-of-the-art results for
image synthesis, inpainting, and text-to-image tasks. However, they are still
in the early stages of generating complex 3D shapes. This work proposes
Diffusion-SDF, a generative model for shape completion, single-view
reconstruction, and reconstruction of real-scanned point clouds. We use neural
signed distance functions (SDFs) as our 3D representation to parameterize the
geometry of various signals (e.g., point clouds, 2D images) through neural
networks. Neural SDFs are implicit functions and diffusing them amounts to
learning the reversal of their neural network weights, which we solve using a
custom modulation module. Extensive experiments show that our method is capable
of both realistic unconditional generation and conditional generation from
partial inputs. This work expands the domain of diffusion models from learning
2D, explicit representations, to 3D, implicit representations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Sketch-Guided Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 24, 2022 </span>    
         <span class="authors"> Andrey Voynov, Kfir Aberman, Daniel Cohen-Or </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.13752" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-Image models have introduced a remarkable leap in the evolution of
machine learning, demonstrating high-quality synthesis of images from a given
text-prompt. However, these powerful pretrained models still lack control
handles that can guide spatial properties of the synthesized images. In this
work, we introduce a universal approach to guide a pretrained text-to-image
diffusion model, with a spatial map from another domain (e.g., sketch) during
inference time. Unlike previous works, our method does not require to train a
dedicated model or a specialized encoder for the task. Our key idea is to train
a Latent Guidance Predictor (LGP) - a small, per-pixel, Multi-Layer Perceptron
(MLP) that maps latent features of noisy images to spatial maps, where the deep
features are extracted from the core Denoising Diffusion Probabilistic Model
(DDPM) network. The LGP is trained only on a few thousand images and
constitutes a differential guiding map predictor, over which the loss is
computed and propagated back to push the intermediate images to agree with the
spatial map. The per-pixel training offers flexibility and locality which
allows the technique to perform well on out-of-domain sketches, including
free-hand style drawings. We take a particular focus on the sketch-to-image
translation task, revealing a robust and expressive way to generate images that
follow the guidance of a sketch of arbitrary style or domain. Project page:
sketch-guided-diffusion.github.io
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Fast Sampling of Diffusion Models via Operator Learning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 24, 2022 </span>    
         <span class="authors"> Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, Anima Anandkumar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.13449" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have found widespread adoption in various areas. However,
their sampling process is slow because it requires hundreds to thousands of
network evaluations to emulate a continuous process defined by differential
equations. In this work, we use neural operators, an efficient method to solve
the probability flow differential equations, to accelerate the sampling process
of diffusion models. Compared to other fast sampling methods that have a
sequential nature, we are the first to propose parallel decoding method that
generates images with only one model forward pass. We propose \textit{diffusion
model sampling with neural operator} (DSNO) that maps the initial condition,
i.e., Gaussian distribution, to the continuous-time solution trajectory of the
reverse diffusion process. To model the temporal correlations along the
trajectory, we introduce temporal convolution layers that are parameterized in
the Fourier space into the given diffusion model backbone. We show our method
achieves state-of-the-art FID of 4.12 for CIFAR-10 and 8.35 for ImageNet-64 in
the one-model-evaluation setting.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Shifted Diffusion for Text-to-image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 24, 2022 </span>    
         <span class="authors"> Yufan Zhou, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou Chen, Jinhui Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.15388" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Corgi, a novel method for text-to-image generation. Corgi is based
on our proposed shifted diffusion model, which achieves better image embedding
generation from input text. Unlike the baseline diffusion model used in DALL-E
2, our method seamlessly encodes prior knowledge of the pre-trained CLIP model
in its diffusion process by designing a new initialization distribution and a
new transition step of the diffusion. Compared to the strong DALL-E 2 baseline,
our method performs better in generating image embedding from the text in terms
of both efficiency and effectiveness, resulting in better text-to-image
generation. Extensive large-scale experiments are conducted and evaluated in
terms of both quantitative measures and human evaluation, indicating a stronger
generation ability of our method compared to existing ones. Furthermore, our
model enables semi-supervised and language-free training for text-to-image
generation, where only part or none of the images in the training dataset have
an associated caption. Trained with only 1.7% of the images being captioned,
our semi-supervised model obtains FID results comparable to DALL-E 2 on
zero-shot text-to-image generation evaluated on MS-COCO. Corgi also achieves
new state-of-the-art results across different datasets on downstream
language-free text-to-image generation tasks, outperforming the previous
method, Lafite, by a large margin.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improving dermatology classifiers across populations using images generated by large diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 23, 2022 </span>    
         <span class="authors"> Luke W. Sagers, James A. Diao, Matthew Groh, Pranav Rajpurkar, Adewole S. Adamson, Arjun K. Manrai </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.13352" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Dermatological classification algorithms developed without sufficiently
diverse training data may generalize poorly across populations. While
intentional data collection and annotation offer the best means for improving
representation, new computational approaches for generating training data may
also aid in mitigating the effects of sampling bias. In this paper, we show
that DALL$\cdot$E 2, a large-scale text-to-image diffusion model, can produce
photorealistic images of skin disease across skin types. Using the Fitzpatrick
17k dataset as a benchmark, we demonstrate that augmenting training data with
DALL$\cdot$E 2-generated synthetic images improves classification of skin
disease overall and especially for underrepresented groups.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 23, 2022 </span>    
         <span class="authors"> Mohammad Amin Shabani, Sepidehsadat Hosseini, Yasutaka Furukawa </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.13287" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The paper presents a novel approach for vector-floorplan generation via a
diffusion model, which denoises 2D coordinates of room/door corners with two
inference objectives: 1) a single-step noise as the continuous quantity to
precisely invert the continuous forward process; and 2) the final 2D coordinate
as the discrete quantity to establish geometric incident relationships such as
parallelism, orthogonality, and corner-sharing. Our task is graph-conditioned
floorplan generation, a common workflow in floorplan design. We represent a
floorplan as 1D polygonal loops, each of which corresponds to a room or a door.
Our diffusion model employs a Transformer architecture at the core, which
controls the attention masks based on the input graph-constraint and directly
generates vector-graphics floorplans via a discrete and continuous denoising
process. We have evaluated our approach on RPLAN dataset. The proposed approach
makes significant improvements in all the metrics against the state-of-the-art
with significant margins, while being capable of generating non-Manhattan
structures and controlling the exact number of corners per room. A project
website with supplementary video and document is here
https://aminshabani.github.io/housediffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Paint by Example: Exemplar-based Image Editing with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 23, 2022 </span>    
         <span class="authors"> Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.13227" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Language-guided image editing has achieved great success recently. In this
paper, for the first time, we investigate exemplar-guided image editing for
more precise control. We achieve this goal by leveraging self-supervised
training to disentangle and re-organize the source image and the exemplar.
However, the naive approach will cause obvious fusing artifacts. We carefully
analyze it and propose an information bottleneck and strong augmentations to
avoid the trivial solution of directly copying and pasting the exemplar image.
Meanwhile, to ensure the controllability of the editing process, we design an
arbitrary shape mask for the exemplar image and leverage the classifier-free
guidance to increase the similarity to the exemplar image. The whole framework
involves a single forward of the diffusion model without any iterative
optimization. We demonstrate that our method achieves an impressive performance
and enables controllable editing on in-the-wild images with high fidelity.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Tetrahedral Diffusion Models for 3D Shape Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 23, 2022 </span>    
         <span class="authors"> Nikolai Kalischek, Torben Peters, Jan D. Wegner, Konrad Schindler </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.13220" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, probabilistic denoising diffusion models (DDMs) have greatly
advanced the generative power of neural networks. DDMs, inspired by
non-equilibrium thermodynamics, have not only been used for 2D image
generation, but can also readily be applied to 3D point clouds. However,
representing 3D shapes as point clouds has a number of drawbacks, most obvious
perhaps that they have no notion of topology or connectivity. Here, we explore
an alternative route and introduce tetrahedral diffusion models, an extension
of DDMs to tetrahedral partitions of 3D space. The much more structured 3D
representation with space-filling tetrahedra makes it possible to guide and
regularize the diffusion process and to apply it to colorized assets. To
manipulate the proposed representation, we develop tetrahedral convolutions,
down- and up-sampling kernels. With those operators, 3D shape generation
amounts to learning displacement vectors and signed distance values on the
tetrahedral grid. Our experiments confirm that Tetrahedral Diffusion yields
plausible, visually pleasing and diverse 3D shapes, is able to handle surface
attributes like color, and can be guided at test time to manipulate the
resulting shapes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Inversion-Based Creativity Transfer with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 23, 2022 </span>    
         <span class="authors"> Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, Changsheng Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.13203" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The artistic style within a painting is the means of expression, which
includes not only the painting material, colors, and brushstrokes, but also the
high-level attributes including semantic elements, object shapes, etc. Previous
arbitrary example-guided artistic image generation methods often fail to
control shape changes or convey elements. The pre-trained text-to-image
synthesis diffusion probabilistic models have achieved remarkable quality, but
it often requires extensive textual descriptions to accurately portray
attributes of a particular painting. We believe that the uniqueness of an
artwork lies precisely in the fact that it cannot be adequately explained with
normal language. Our key idea is to learn artistic style directly from a single
painting and then guide the synthesis without providing complex textual
descriptions. Specifically, we assume style as a learnable textual description
of a painting. We propose an inversion-based style transfer method (InST),
which can efficiently and accurately learn the key information of an image,
thus capturing and transferring the artistic style of a painting. We
demonstrate the quality and efficiency of our method on numerous paintings of
various artists and styles. Code and models are available at
https://github.com/zyxElsa/InST.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 22, 2022 </span>    
         <span class="authors"> Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.12572" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large-scale text-to-image generative models have been a revolutionary
breakthrough in the evolution of generative AI, allowing us to synthesize
diverse images that convey highly complex visual concepts. However, a pivotal
challenge in leveraging such models for real-world content creation tasks is
providing users with control over the generated content. In this paper, we
present a new framework that takes text-to-image synthesis to the realm of
image-to-image translation -- given a guidance image and a target text prompt,
our method harnesses the power of a pre-trained text-to-image diffusion model
to generate a new image that complies with the target text, while preserving
the semantic layout of the source image. Specifically, we observe and
empirically demonstrate that fine-grained control over the generated structure
can be achieved by manipulating spatial features and their self-attention
inside the model. This results in a simple and effective approach, where
features extracted from the guidance image are directly injected into the
generation process of the target image, requiring no training or fine-tuning
and applicable for both real or generated guidance images. We demonstrate
high-quality results on versatile text-guided image translation tasks,
including translating sketches, rough drawings and animations into realistic
images, changing of the class and appearance of objects in a given image, and
modifications of global qualities such as lighting and color.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Person Image Synthesis via Denoising Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 22, 2022 </span>    
         <span class="authors"> Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Jorma Laaksonen, Mubarak Shah, Fahad Shahbaz Khan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.12500" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The pose-guided person image generation task requires synthesizing
photorealistic images of humans in arbitrary poses. The existing approaches use
generative adversarial networks that do not necessarily maintain realistic
textures or need dense correspondences that struggle to handle complex
deformations and severe occlusions. In this work, we show how denoising
diffusion models can be applied for high-fidelity person image synthesis with
strong sample diversity and enhanced mode coverage of the learnt data
distribution. Our proposed Person Image Diffusion Model (PIDM) disintegrates
the complex transfer problem into a series of simpler forward-backward
denoising steps. This helps in learning plausible source-to-target
transformation trajectories that result in faithful textures and undistorted
appearance details. We introduce a 'texture diffusion module' based on
cross-attention to accurately model the correspondences between appearance and
pose information available in source and target images. Further, we propose
'disentangled classifier-free guidance' to ensure close resemblance between the
conditional inputs and the synthesized output in terms of both pose and
appearance information. Our extensive results on two large-scale benchmarks and
a user study demonstrate the photorealism of our proposed approach under
challenging scenarios. We also show how our generated images can help in
downstream tasks. Our code and models will be publicly released.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## EDICT: Exact Diffusion Inversion via Coupled Transformations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 22, 2022 </span>    
         <span class="authors"> Bram Wallace, Akash Gokul, Nikhil Naik </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.12446" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Finding an initial noise vector that produces an input image when fed into
the diffusion process (known as inversion) is an important problem in denoising
diffusion models (DDMs), with applications for real image editing. The
state-of-the-art approach for real image editing with inversion uses denoising
diffusion implicit models (DDIMs) to deterministically noise the image to the
intermediate state along the path that the denoising would follow given the
original conditioning. However, DDIM inversion for real images is unstable as
it relies on local linearization assumptions, which result in the propagation
of errors, leading to incorrect image reconstruction and loss of content. To
alleviate these problems, we propose Exact Diffusion Inversion via Coupled
Transformations (EDICT), an inversion method that draws inspiration from affine
coupling layers. EDICT enables mathematically exact inversion of real and
model-generated images by maintaining two coupled noise vectors which are used
to invert each other in an alternating fashion. Using Stable Diffusion, a
state-of-the-art latent diffusion model, we demonstrate that EDICT successfully
reconstructs real images with high fidelity. On complex image datasets like
MS-COCO, EDICT reconstruction significantly outperforms DDIM, improving the
mean square error of reconstruction by a factor of two. Using noise vectors
inverted from real images, EDICT enables a wide range of image edits--from
local and global semantic edits to image stylization--while maintaining
fidelity to the original image structure. EDICT requires no model
training/finetuning, prompt tuning, or extra data and can be combined with any
pretrained DDM. Code is available at https://github.com/salesforce/EDICT.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SinDiffusion: Learning a Diffusion Model from a Single Natural Image
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 22, 2022 </span>    
         <span class="authors"> Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.12445" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present SinDiffusion, leveraging denoising diffusion models to capture
internal distribution of patches from a single natural image. SinDiffusion
significantly improves the quality and diversity of generated samples compared
with existing GAN-based approaches. It is based on two core designs. First,
SinDiffusion is trained with a single model at a single scale instead of
multiple models with progressive growing of scales which serves as the default
setting in prior work. This avoids the accumulation of errors, which cause
characteristic artifacts in generated results. Second, we identify that a
patch-level receptive field of the diffusion network is crucial and effective
for capturing the image's patch statistics, therefore we redesign the network
structure of the diffusion model. Coupling these two designs enables us to
generate photorealistic and diverse images from a single image. Furthermore,
SinDiffusion can be applied to various applications, i.e., text-guided image
generation, and image outpainting, due to the inherent capability of diffusion
models. Extensive experiments on a wide range of images demonstrate the
superiority of our proposed method for modeling the patch distribution.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Can denoising diffusion probabilistic models generate realistic astrophysical fields?
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 22, 2022 </span>    
         <span class="authors"> Nayantara Mudur, Douglas P. Finkbeiner </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.12444" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  astro-ph.CO, astro-ph.GA, astro-ph.IM, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models have emerged as alternatives to generative
adversarial networks (GANs) and normalizing flows for tasks involving learning
and sampling from complex image distributions. In this work we investigate the
ability of these models to generate fields in two astrophysical contexts: dark
matter mass density fields from cosmological simulations and images of
interstellar dust. We examine the fidelity of the sampled cosmological fields
relative to the true fields using three different metrics, and identify
potential issues to address. We demonstrate a proof-of-concept application of
the model trained on dust in denoising dust images. To our knowledge, this is
the first application of this class of models to the interstellar medium.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 22, 2022 </span>    
         <span class="authors"> Jiaming Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Stewart He, K. Aditya Mohan, Ulugbek S. Kamilov, Hyojin Kim </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.12340" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Limited-Angle Computed Tomography (LACT) is a non-destructive evaluation
technique used in a variety of applications ranging from security to medicine.
The limited angle coverage in LACT is often a dominant source of severe
artifacts in the reconstructed images, making it a challenging inverse problem.
We present DOLCE, a new deep model-based framework for LACT that uses a
conditional diffusion model as an image prior. Diffusion models are a recent
class of deep generative models that are relatively easy to train due to their
implementation as image denoisers. DOLCE can form high-quality images from
severely under-sampled data by integrating data-consistency updates with the
sampling updates of a diffusion model, which is conditioned on the transformed
limited-angle data. We show through extensive experimentation on several
challenging real LACT datasets that, the same pre-trained DOLCE model achieves
the SOTA performance on drastically different types of images. Additionally, we
show that, unlike standard LACT reconstruction methods, DOLCE naturally enables
the quantification of the reconstruction uncertainty by generating multiple
samples consistent with the measured data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffDreamer: Consistent Single-view Perpetual View Generation with Conditional Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 22, 2022 </span>    
         <span class="authors"> Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool, Gordon Wetzstein </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.12131" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Scene extrapolation -- the idea of generating novel views by flying into a
given image -- is a promising, yet challenging task. For each predicted frame,
a joint inpainting and 3D refinement problem has to be solved, which is ill
posed and includes a high level of ambiguity. Moreover, training data for
long-range scenes is difficult to obtain and usually lacks sufficient views to
infer accurate camera poses. We introduce DiffDreamer, an unsupervised
framework capable of synthesizing novel views depicting a long camera
trajectory while training solely on internet-collected images of nature scenes.
Utilizing the stochastic nature of the guided denoising steps, we train the
diffusion models to refine projected RGBD images but condition the denoising
steps on multiple past and future frames for inference. We demonstrate that
image-conditioned diffusion models can effectively perform long-range scene
extrapolation while preserving consistency significantly better than prior
GAN-based methods. DiffDreamer is a powerful and efficient solution for scene
extrapolation, producing impressive results despite limited supervision.
Project page: https://primecai.github.io/diffdreamer.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SinFusion: Training Diffusion Models on a Single Image or Video
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 21, 2022 </span>    
         <span class="authors"> Yaniv Nikankin, Niv Haim, Michal Irani </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.11743" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models exhibited tremendous progress in image and video generation,
exceeding GANs in quality and diversity. However, they are usually trained on
very large datasets and are not naturally adapted to manipulate a given input
image or video. In this paper we show how this can be resolved by training a
diffusion model on a single input image or video. Our image/video-specific
diffusion model (SinFusion) learns the appearance and dynamics of the single
image or video, while utilizing the conditioning capabilities of diffusion
models. It can solve a wide array of image/video-specific manipulation tasks.
In particular, our model can learn from few frames the motion and dynamics of a
single input video. It can then generate diverse new video samples of the same
dynamic scene, extrapolate short videos into long ones (both forward and
backward in time) and perform video upsampling. Most of these tasks are not
realizable by current video-specific generation methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 21, 2022 </span>    
         <span class="authors"> Ajay Jain, Amber Xie, Pieter Abbeel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.11319" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown impressive results in text-to-image synthesis.
Using massive datasets of captioned images, diffusion models learn to generate
raster images of highly diverse objects and scenes. However, designers
frequently use vector representations of images like Scalable Vector Graphics
(SVGs) for digital icons or art. Vector graphics can be scaled to any size, and
are compact. We show that a text-conditioned diffusion model trained on pixel
representations of images can be used to generate SVG-exportable vector
graphics. We do so without access to large datasets of captioned SVGs. By
optimizing a differentiable vector graphics rasterizer, our method,
VectorFusion, distills abstract semantic knowledge out of a pretrained
diffusion model. Inspired by recent text-to-3D work, we learn an SVG consistent
with a caption using Score Distillation Sampling. To accelerate generation and
improve fidelity, VectorFusion also initializes from an image sample.
Experiments show greater quality than prior work, and demonstrate a range of
styles including pixel art and sketches. See our project webpage at
https://ajayj.com/vectorfusion .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 21, 2022 </span>    
         <span class="authors"> Luping Liu, Yi Ren, Xize Cheng, Zhou Zhao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.11255" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Out-of-distribution (OOD) detection is an important task to ensure the
reliability and safety of deep learning and the discriminator models outperform
others for now. However, the feature extraction of the discriminator models
must compress the data and lose certain information, leaving room for bad cases
and malicious attacks. In this paper, we provide a new assumption that the
discriminator models are more sensitive to some subareas of the input space and
such perceptron bias causes bad cases and overconfidence areas. Under this
assumption, we design new detection methods and indicator scores. For detection
methods, we introduce diffusion models (DMs) into OOD detection. We find that
the diffusion denoising process (DDP) of DMs also functions as a novel form of
asymmetric interpolation, which is suitable to enhance the input and reduce the
overconfidence areas. For indicator scores, we find that the features of the
discriminator models of OOD inputs occur sharp changes under DDP and use the
norm of this dynamic change as our indicator scores. Therefore, we develop a
new framework to combine the discriminator and generation models to do OOD
detection under our new assumption. The discriminator models provide proper
detection spaces and the generation models reduce the overconfidence problem.
According to our experiments on CIFAR10 and CIFAR100, our methods get
competitive results with state-of-the-art methods. Our implementation is
available at https://github.com/luping-liu/DiffOOD.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Investigating Prompt Engineering in Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 21, 2022 </span>    
         <span class="authors"> Sam Witteveen, Martin Andrews </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.15462" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.CL
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 With the spread of the use of Text2Img diffusion models such as DALL-E 2,
Imagen, Mid Journey and Stable Diffusion, one challenge that artists face is
selecting the right prompts to achieve the desired artistic output. We present
techniques for measuring the effect that specific words and phrases in prompts
have, and (in the Appendix) present guidance on the selection of prompts to
produce desired effects.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-Based Scene Graph to Image Generation with Masked Contrastive Pre-Training
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 21, 2022 </span>    
         <span class="authors"> Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui, Bernard Ghanem, Ming-Hsuan Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.11138" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generating images from graph-structured inputs, such as scene graphs, is
uniquely challenging due to the difficulty of aligning nodes and connections in
graphs with objects and their relations in images. Most existing methods
address this challenge by using scene layouts, which are image-like
representations of scene graphs designed to capture the coarse structures of
scene images. Because scene layouts are manually crafted, the alignment with
images may not be fully optimized, causing suboptimal compliance between the
generated images and the original scene graphs. To tackle this issue, we
propose to learn scene graph embeddings by directly optimizing their alignment
with images. Specifically, we pre-train an encoder to extract both global and
local information from scene graphs that are predictive of the corresponding
images, relying on two loss functions: masked autoencoding loss and contrastive
loss. The former trains embeddings by reconstructing randomly masked image
regions, while the latter trains embeddings to discriminate between compliant
and non-compliant images according to the scene graph. Given these embeddings,
we build a latent diffusion model to generate images from scene graphs. The
resulting method, called SGDiff, allows for the semantic manipulation of
generated images by modifying scene graph nodes and connections. On the Visual
Genome and COCO-Stuff datasets, we demonstrate that SGDiff outperforms
state-of-the-art methods, as measured by both the Inception Score and Fr\'echet
Inception Distance (FID) metrics. We will release our source code and trained
models at https://github.com/YangLing0818/SGDiff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MagicVideo: Efficient Video Generation With Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 20, 2022 </span>    
         <span class="authors"> Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, Jiashi Feng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.11018" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present an efficient text-to-video generation framework based on latent
diffusion models, termed MagicVideo. MagicVideo can generate smooth video clips
that are concordant with the given text descriptions. Due to a novel and
efficient 3D U-Net design and modeling video distributions in a low-dimensional
space, MagicVideo can synthesize video clips with 256x256 spatial resolution on
a single GPU card, which takes around 64x fewer computations than the Video
Diffusion Models (VDM) in terms of FLOPs. In specific, unlike existing works
that directly train video models in the RGB space, we use a pre-trained VAE to
map video clips into a low-dimensional latent space and learn the distribution
of videos' latent codes via a diffusion model. Besides, we introduce two new
designs to adapt the U-Net denoiser trained on image tasks to video data: a
frame-wise lightweight adaptor for the image-to-video distribution adjustment
and a directed temporal attention module to capture temporal dependencies
across frames. Thus, we can exploit the informative weights of convolution
operators from a text-to-image model for accelerating video training. To
ameliorate the pixel dithering in the generated videos, we also propose a novel
VideoVAE auto-encoder for better RGB reconstruction. We conduct extensive
experiments and demonstrate that MagicVideo can generate high-quality video
clips with either realistic or imaginary content. Refer to
\url{https://magicvideo.github.io/#} for more examples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## IC3D: Image-Conditioned 3D Diffusion for Shape Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 20, 2022 </span>    
         <span class="authors"> Cristian Sbrolli, Paolo Cudrano, Matteo Frosi, Matteo Matteucci </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.10865" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In the last years, Denoising Diffusion Probabilistic Models (DDPMs) obtained
state-of-the-art results in many generative tasks, outperforming GANs and other
classes of generative models. In particular, they reached impressive results in
various image generation sub-tasks, among which conditional generation tasks
such as text-guided image synthesis. Given the success of DDPMs in 2D
generation, they have more recently been applied to 3D shape generation,
outperforming previous approaches and reaching state-of-the-art results.
However, these existing 3D DDPM works make little or no use of guidance, mainly
being unconditional or class-conditional. In this work, we present IC3D, an
Image-Conditioned 3D Diffusion model that generates 3D shapes by image
guidance. To guide our DDPM, we introduce CISP (Contrastive Image-Shape
Pre-training), a model jointly embedding images and shapes by contrastive
pre-training, inspired by the literature on text-to-image DDPMs. Our generative
diffusion model outperforms the state-of-the-art in 3D generation quality and
diversity. Furthermore, despite IC3D generative nature, we show that its
generated shapes are preferred by human evaluators to a SoTA single-view 3D
reconstruction model in terms of quality and coherence to the query image by
running a side-by-side human evaluation. Ablation studies show the importance
of CISP for learning structural integrity properties, crucial for realistic
generation. Such biases yield a regular embedding space and allow for
interpolation and conditioning on out-of-distribution images, while also making
IC3D capable of generating coherent but diverse completions of occluded views
and enabling its adoption in controlled real-life applications.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 20, 2022 </span>    
         <span class="authors"> Xiangming Meng, Yoshiyuki Kabashima </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.12343" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.IT, math.IT, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We consider the ubiquitous linear inverse problems with additive Gaussian
noise and propose an unsupervised general-purpose sampling approach called
diffusion model based posterior sampling (DMPS) to reconstruct the unknown
signal from noisy linear measurements. Specifically, the prior of the unknown
signal is implicitly modeled by one pre-trained diffusion model (DM). In
posterior sampling, to address the intractability of exact noise-perturbed
likelihood score, a simple yet effective noise-perturbed pseudo-likelihood
score is introduced under the uninformative prior assumption. While DMPS
applies to any kind of DM with proper modifications, we focus on the ablated
diffusion model (ADM) as one specific example and evaluate its efficacy on a
variety of linear inverse problems such as image super-resolution, denoising,
deblurring, colorization. Experimental results demonstrate that, for both
in-distribution and out-of-distribution samples, DMPS achieves highly
competitive or even better performances on various tasks while being 3 times
faster than the leading competitor. The code to reproduce the results is
available at https://github.com/mengxiangming/dmps.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## NVDiff: Graph Generation through the Diffusion of Node Vectors
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 19, 2022 </span>    
         <span class="authors"> Xiaohui Chen, Yukun Li, Aonan Zhang, Li-ping Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.10794" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Learning to generate graphs is challenging as a graph is a set of pairwise
connected, unordered nodes encoding complex combinatorial structures. Recently,
several works have proposed graph generative models based on normalizing flows
or score-based diffusion models. However, these models need to generate nodes
and edges in parallel from the same process, whose dimensionality is
unnecessarily high. We propose NVDiff, which takes the VGAE structure and uses
a score-based generative model (SGM) as a flexible prior to sample node
vectors. By modeling only node vectors in the latent space, NVDiff
significantly reduces the dimension of the diffusion process and thus improves
sampling speed. Built on the NVDiff framework, we introduce an attention-based
score network capable of capturing both local and global contexts of graphs.
Experiments indicate that NVDiff significantly reduces computations and can
model much larger graphs than competing methods. At the same time, it achieves
superior or competitive performances over various datasets compared to previous
methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Parallel Diffusion Models of Operator and Image for Blind Inverse Problems
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 19, 2022 </span>    
         <span class="authors"> Hyungjin Chung, Jeongsol Kim, Sehui Kim, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.10656" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion model-based inverse problem solvers have demonstrated
state-of-the-art performance in cases where the forward operator is known (i.e.
non-blind). However, the applicability of the method to blind inverse problems
has yet to be explored. In this work, we show that we can indeed solve a family
of blind inverse problems by constructing another diffusion prior for the
forward operator. Specifically, parallel reverse diffusion guided by gradients
from the intermediate stages enables joint optimization of both the forward
operator parameters as well as the image, such that both are jointly estimated
at the end of the parallel reverse diffusion procedure. We show the efficacy of
our method on two representative tasks -- blind deblurring, and imaging through
turbulence -- and show that our method yields state-of-the-art performance,
while also being flexible to be applicable to general blind inverse problems
when we know the functional forms.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Magic3D: High-Resolution Text-to-3D Content Creation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 18, 2022 </span>    
         <span class="authors"> Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.10440" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 DreamFusion has recently demonstrated the utility of a pre-trained
text-to-image diffusion model to optimize Neural Radiance Fields (NeRF),
achieving remarkable text-to-3D synthesis results. However, the method has two
inherent limitations: (a) extremely slow optimization of NeRF and (b)
low-resolution image space supervision on NeRF, leading to low-quality 3D
models with a long processing time. In this paper, we address these limitations
by utilizing a two-stage optimization framework. First, we obtain a coarse
model using a low-resolution diffusion prior and accelerate with a sparse 3D
hash grid structure. Using the coarse representation as the initialization, we
further optimize a textured 3D mesh model with an efficient differentiable
renderer interacting with a high-resolution latent diffusion model. Our method,
dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is
2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also
achieving higher resolution. User studies show 61.7% raters to prefer our
approach over DreamFusion. Together with the image-conditioned generation
capabilities, we provide users with new ways to control 3D synthesis, opening
up new avenues to various creative applications.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Structure-Guided Diffusion Model for Large-Hole Diverse Image Completion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 18, 2022 </span>    
         <span class="authors"> Daichi Horita, Jiaolong Yang, Dong Chen, Yuki Koyama, Kiyoharu Aizawa </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.10437" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diverse image completion, a problem of generating various ways of filling
incomplete regions (i.e. holes) of an image, has made remarkable success.
However, managing input images with large holes is still a challenging problem
due to the corruption of semantically important structures. In this paper, we
tackle this problem by incorporating explicit structural guidance. We propose a
structure-guided diffusion model (SGDM) for the large-hole diverse completion
problem. Our proposed SGDM consists of a structure generator and a texture
generator, which are both diffusion probabilistic models (DMs). The structure
generator generates an edge image representing a plausible structure within the
holes, which is later used to guide the texture generation process. To jointly
train these two generators, we design a strategy that combines optimal Bayesian
denoising and a momentum framework. In addition to the quality improvement,
auxiliary edge images generated by the structure generator can be manually
edited to allow user-guided image editing. Our experiments using datasets of
faces (CelebA-HQ) and natural scenes (Places) show that our method achieves a
comparable or superior trade-off between visual quality and diversity compared
to other state-of-the-art methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Patch-Based Denoising Diffusion Probabilistic Model for Sparse-View CT Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 18, 2022 </span>    
         <span class="authors"> Wenjun Xia, Wenxiang Cong, Ge Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.10388" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.LG, eess.SP, physics.med-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Sparse-view computed tomography (CT) can be used to reduce radiation dose
greatly but is suffers from severe image artifacts. Recently, the deep learning
based method for sparse-view CT reconstruction has attracted a major attention.
However, neural networks often have a limited ability to remove the artifacts
when they only work in the image domain. Deep learning-based sinogram
processing can achieve a better anti-artifact performance, but it inevitably
requires feature maps of the whole image in a video memory, which makes
handling large-scale or three-dimensional (3D) images rather challenging. In
this paper, we propose a patch-based denoising diffusion probabilistic model
(DDPM) for sparse-view CT reconstruction. A DDPM network based on patches
extracted from fully sampled projection data is trained and then used to
inpaint down-sampled projection data. The network does not require paired
full-sampled and down-sampled data, enabling unsupervised learning. Since the
data processing is patch-based, the deep learning workflow can be distributed
in parallel, overcoming the memory problem of large-scale data. Our experiments
show that the proposed method can effectively suppress few-view artifacts while
faithfully preserving textural details.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Invariant Learning via Diffusion Dreamed Distribution Shifts
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 18, 2022 </span>    
         <span class="authors"> Priyatham Kattakinda, Alexander Levine, Soheil Feizi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.10370" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Though the background is an important signal for image classification, over
reliance on it can lead to incorrect predictions when spurious correlations
between foreground and background are broken at test time. Training on a
dataset where these correlations are unbiased would lead to more robust models.
In this paper, we propose such a dataset called Diffusion Dreamed Distribution
Shifts (D3S). D3S consists of synthetic images generated through
StableDiffusion using text prompts and image guides obtained by pasting a
sample foreground image onto a background template image. Using this scalable
approach we generate 120K images of objects from all 1000 ImageNet classes in
10 diverse backgrounds. Due to the incredible photorealism of the diffusion
model, our images are much closer to natural images than previous synthetic
datasets. D3S contains a validation set of more than 17K images whose labels
are human-verified in an MTurk study. Using the validation set, we evaluate
several popular DNN image classifiers and find that the classification
performance of models generally suffers on our background diverse images. Next,
we leverage the foreground & background labels in D3S to learn a foreground
(background) representation that is invariant to changes in background
(foreground) by penalizing the mutual information between the foreground
(background) features and the background (foreground) labels. Linear
classifiers trained on these features to predict foreground (background) from
foreground (background) have high accuracies at 82.9% (93.8%), while
classifiers that predict these labels from background and foreground have a
much lower accuracy of 2.4% and 45.6% respectively. This suggests that our
foreground and background features are well disentangled. We further test the
efficacy of these representations by training classifiers on a task with strong
spurious correlations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 17, 2022 </span>    
         <span class="authors"> Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J. Mitra, Paul Guerrero </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.09869" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models currently achieve state-of-the-art performance for both
conditional and unconditional image generation. However, so far, image
diffusion models do not support tasks required for 3D understanding, such as
view-consistent 3D generation or single-view object reconstruction. In this
paper, we present RenderDiffusion, the first diffusion model for 3D generation
and inference, trained using only monocular 2D supervision. Central to our
method is a novel image denoising architecture that generates and renders an
intermediate three-dimensional representation of a scene in each denoising
step. This enforces a strong inductive structure within the diffusion process,
providing a 3D consistent representation while only requiring 2D supervision.
The resulting 3D representation can be rendered from any view. We evaluate
RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive
performance for generation of 3D scenes and inference of 3D scenes from 2D
images. Additionally, our diffusion-based approach allows us to use 2D
inpainting to edit 3D scenes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conffusion: Confidence Intervals for Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 17, 2022 </span>    
         <span class="authors"> Eliahu Horwitz, Yedid Hoshen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.09795" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have become the go-to method for many generative tasks,
particularly for image-to-image generation tasks such as super-resolution and
inpainting. Current diffusion-based methods do not provide statistical
guarantees regarding the generated results, often preventing their use in
high-stakes situations. To bridge this gap, we construct a confidence interval
around each generated pixel such that the true value of the pixel is guaranteed
to fall within the interval with a probability set by the user. Since diffusion
models parametrize the data distribution, a straightforward way of constructing
such intervals is by drawing multiple samples and calculating their bounds.
However, this method has several drawbacks: i) slow sampling speeds ii)
suboptimal bounds iii) requires training a diffusion model per task. To
mitigate these shortcomings we propose Conffusion, wherein we fine-tune a
pre-trained diffusion model to predict interval bounds in a single forward
pass. We show that Conffusion outperforms the baseline method while being three
orders of magnitude faster.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Null-text Inversion for Editing Real Images using Guided Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 17, 2022 </span>    
         <span class="authors"> Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.09794" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent text-guided diffusion models provide powerful image generation
capabilities. Currently, a massive effort is given to enable the modification
of these images using text only as means to offer intuitive and versatile
editing. To edit a real image using these state-of-the-art tools, one must
first invert the image with a meaningful text prompt into the pretrained
model's domain. In this paper, we introduce an accurate inversion technique and
thus facilitate an intuitive text-based modification of the image. Our proposed
inversion consists of two novel key components: (i) Pivotal inversion for
diffusion models. While current methods aim at mapping random noise samples to
a single input image, we use a single pivotal noise vector for each timestamp
and optimize around it. We demonstrate that a direct inversion is inadequate on
its own, but does provide a good anchor for our optimization. (ii) NULL-text
optimization, where we only modify the unconditional textual embedding that is
used for classifier-free guidance, rather than the input text embedding. This
allows for keeping both the model weights and the conditional embedding intact
and hence enables applying prompt-based editing while avoiding the cumbersome
tuning of the model's weights. Our Null-text inversion, based on the publicly
available Stable Diffusion model, is extensively evaluated on a variety of
images and prompt editing, showing high-fidelity editing of real images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionDet: Diffusion Model for Object Detection
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 17, 2022 </span>    
         <span class="authors"> Shoufa Chen, Peize Sun, Yibing Song, Ping Luo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.09788" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose DiffusionDet, a new framework that formulates object detection as
a denoising diffusion process from noisy boxes to object boxes. During training
stage, object boxes diffuse from ground-truth boxes to random distribution, and
the model learns to reverse this noising process. In inference, the model
refines a set of randomly generated boxes to the output results in a
progressive way. The extensive evaluations on the standard benchmarks,
including MS-COCO and LVIS, show that DiffusionDet achieves favorable
performance compared to previous well-established detectors. Our work brings
two important findings in object detection. First, random boxes, although
drastically different from pre-defined anchors or learned queries, are also
effective object candidates. Second, object detection, one of the
representative perception tasks, can be solved by a generative way. Our code is
available at https://github.com/ShoufaChen/DiffusionDet.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Listen, denoise, action! Audio-driven motion synthesis with diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 17, 2022 </span>    
         <span class="authors"> Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.09707" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.GR, cs.HC, cs.SD, eess.AS, 68T07, G.3; I.2.6; I.3.7; J.5
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have experienced a surge of interest as highly expressive
yet efficiently trainable probabilistic models. We show that these models are
an excellent fit for synthesising human motion that co-occurs with audio, e.g.,
dancing and co-speech gesticulation, since motion is complex and highly
ambiguous given audio, calling for a probabilistic description. Specifically,
we adapt the DiffWave architecture to model 3D pose sequences, putting
Conformers in place of dilated convolutions for improved modelling power. We
also demonstrate control over motion style, using classifier-free guidance to
adjust the strength of the stylistic expression. Experiments on gesture and
dance generation confirm that the proposed method achieves top-of-the-line
motion quality, with distinctive styles whose expression can be made more or
less pronounced. We also synthesise path-driven locomotion using the same model
architecture. Finally, we generalise the guidance procedure to obtain
product-of-expert ensembles of diffusion models and demonstrate how these may
be used for, e.g., style interpolation, a contribution we believe is of
independent interest. See
https://www.speech.kth.se/research/listen-denoise-action/ for video examples,
data, and code.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 17, 2022 </span>    
         <span class="authors"> Yiwei Guo, Chenpeng Du, Xie Chen, Kai Yu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.09496" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.AI, cs.HC, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Although current neural text-to-speech (TTS) models are able to generate
high-quality speech, intensity controllable emotional TTS is still a
challenging task. Most existing methods need external optimizations for
intensity calculation, leading to suboptimal results or degraded quality. In
this paper, we propose EmoDiff, a diffusion-based TTS model where emotion
intensity can be manipulated by a proposed soft-label guidance technique
derived from classifier guidance. Specifically, instead of being guided with a
one-hot vector for the specified emotion, EmoDiff is guided with a soft label
where the value of the specified emotion and \textit{Neutral} is set to
$\alpha$ and $1-\alpha$ respectively. The $\alpha$ here represents the emotion
intensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can
precisely control the emotion intensity while maintaining high voice quality.
Moreover, diverse speech with specified emotion intensity can be generated by
sampling in the reverse denoising process.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Any-speaker Adaptive Text-To-Speech Synthesis with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 17, 2022 </span>    
         <span class="authors"> Minki Kang, Dongchan Min, Sung Ju Hwang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.09383" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.AI, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 There has been a significant progress in Text-To-Speech (TTS) synthesis
technology in recent years, thanks to the advancement in neural generative
modeling. However, existing methods on any-speaker adaptive TTS have achieved
unsatisfactory performance, due to their suboptimal accuracy in mimicking the
target speakers' styles. In this work, we present Grad-StyleSpeech, which is an
any-speaker adaptive TTS framework that is based on a diffusion model that can
generate highly natural speech with extremely high similarity to target
speakers' voice, given a few seconds of reference speech. Grad-StyleSpeech
significantly outperforms recent speaker-adaptive TTS baselines on English
benchmarks. Audio samples are available at
https://nardien.github.io/grad-stylespeech-demo.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Fast Graph Generative Model via Spectral Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 16, 2022 </span>    
         <span class="authors"> Tianze Luo, Zhanfeng Mo, Sinno Jialin Pan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.08892" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generating graph-structured data is a challenging problem, which requires
learning the underlying distribution of graphs. Various models such as graph
VAE, graph GANs, and graph diffusion models have been proposed to generate
meaningful and reliable graphs, among which the diffusion models have achieved
state-of-the-art performance. In this paper, we argue that running full-rank
diffusion SDEs on the whole graph adjacency matrix space hinders diffusion
models from learning graph topology generation, and hence significantly
deteriorates the quality of generated graph data. To address this limitation,
we propose an efficient yet effective Graph Spectral Diffusion Model (GSDM),
which is driven by low-rank diffusion SDEs on the graph spectrum space. Our
spectral diffusion model is further proven to enjoy a substantially stronger
theoretical guarantee than standard diffusion models. Extensive experiments
across various datasets demonstrate that, our proposed GSDM turns out to be the
SOTA model, by exhibiting both significantly higher generation quality and much
less computational consumption than the baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Versatile Diffusion: Text, Images and Variations All in One Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 15, 2022 </span>    
         <span class="authors"> Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, Humphrey Shi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.08332" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advances in diffusion models have set an impressive milestone in many
generation tasks, and trending works such as DALL-E2, Imagen, and Stable
Diffusion have attracted great interest. Despite the rapid landscape changes,
recent new approaches focus on extensions and performance rather than capacity,
thus requiring separate models for separate tasks. In this work, we expand the
existing single-flow diffusion pipeline into a multi-task multimodal network,
dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image,
image-to-text, and variations in one unified model. The pipeline design of VD
instantiates a unified multi-flow diffusion framework, consisting of sharable
and swappable layer modules that enable the crossmodal generality beyond images
and text. Through extensive experiments, we demonstrate that VD successfully
achieves the following: a) VD outperforms the baseline approaches and handles
all its base tasks with competitive quality; b) VD enables novel extensions
such as disentanglement of style and semantics, dual- and multi-context
blending, etc.; c) The success of our multi-flow multimodal framework over
images and text may inspire further diffusion-based universal AI research. Our
code and models are open-sourced at
https://github.com/SHI-Labs/Versatile-Diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ShadowDiffusion: Diffusion-based Shadow Removal using Classifier-driven Attention and Structure Preservation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 15, 2022 </span>    
         <span class="authors"> Yeying Jin, Wenhan Yang, Wei Ye, Yuan Yuan, Robby T. Tan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.08089" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Removing soft and self shadows that lack clear boundaries from a single image
is still challenging. Self shadows are shadows that are cast on the object
itself. Most existing methods rely on binary shadow masks, without considering
the ambiguous boundaries of soft and self shadows. In this paper, we present
DeS3, a method that removes hard, soft and self shadows based on the self-tuned
ViT feature similarity and color convergence. Our novel ViT similarity loss
utilizes features extracted from a pre-trained Vision Transformer. This loss
helps guide the reverse diffusion process towards recovering scene structures.
We also introduce a color convergence loss to constrain the surface colors in
the reverse inference process to avoid any color shifts. Our DeS3 is able to
differentiate shadow regions from the underlying objects, as well as shadow
regions from the object casting the shadow. This capability enables DeS3 to
better recover the structures of objects even when they are partially occluded
by shadows. Different from existing methods that rely on constraints during the
training phase, we incorporate the ViT similarity and color convergence loss
during the sampling stage. This enables our DeS3 model to effectively integrate
its strong modeling capabilities with input-specific knowledge in a self-tuned
manner. Our method outperforms state-of-the-art methods on the SRD, AISTD,
LRSS, USR and UIUC datasets, removing hard, soft, and self shadows robustly.
Specifically, our method outperforms the SOTA method by 20% of the RMSE of the
whole image on the SRD dataset.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 15, 2022 </span>    
         <span class="authors"> Qihua Zhou, Ruibin Li, Song Guo, Peiran Dong, Yi Liu, Jingcai Guo, Zhenda Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.08428" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG, cs.MM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent years have witnessed the dramatic growth of Internet video traffic,
where the video bitstreams are often compressed and delivered in low quality to
fit the streamer's uplink bandwidth. To alleviate the quality degradation, it
comes the rise of Neural-enhanced Video Streaming (NVS), which shows great
prospects for recovering low-quality videos by mostly deploying neural
super-resolution (SR) on the media server. Despite its benefit, we reveal that
current mainstream works with SR enhancement have not achieved the desired
rate-distortion trade-off between bitrate saving and quality restoration, due
to: (1) overemphasizing the enhancement on the decoder side while omitting the
co-design of encoder, (2) limited generative capacity to recover high-fidelity
perceptual details, and (3) optimizing the compression-and-restoration pipeline
from the resolution perspective solely, without considering color bit-depth.
Aiming at overcoming these limitations, we are the first to conduct an
encoder-decoder (i.e., codec) synergy by leveraging the inherent
visual-generative property of diffusion models. Specifically, we present the
Codec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly
reduce streaming delivery bitrates while holding pretty higher restoration
capacity over existing methods. First, CaDM improves the encoder's compression
efficiency by simultaneously reducing resolution and color bit-depth of video
frames. Second, CaDM empowers the decoder with high-quality enhancement by
making the denoising diffusion restoration aware of encoder's resolution-color
conditions. Evaluation on public cloud services with OpenMMLab benchmarks shows
that CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common
video standards and achieves much better recovery quality (e.g., FID of 0.61)
over state-of-the-art neural-enhancing methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 15, 2022 </span>    
         <span class="authors"> Adham Elarabawy, Harish Kamath, Samuel Denton </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.07825" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 With the rise of large, publicly-available text-to-image diffusion models,
text-guided real image editing has garnered much research attention recently.
Existing methods tend to either rely on some form of per-instance or per-task
fine-tuning and optimization, require multiple novel views, or they inherently
entangle preservation of real image identity, semantic coherence, and
faithfulness to text guidance. In this paper, we propose an optimization-free
and zero fine-tuning framework that applies complex and non-rigid edits to a
single real image via a text prompt, avoiding all the pitfalls described above.
Using widely-available generic pre-trained text-to-image diffusion models, we
demonstrate the ability to modulate pose, scene, background, style, color, and
even racial identity in an extremely flexible manner through a single target
text detailing the desired edit. Furthermore, our method, which we name
$\textit{Direct Inversion}$, proposes multiple intuitively configurable
hyperparameters to allow for a wide range of types and extents of real image
edits. We prove our method's efficacy in producing high-quality, diverse,
semantically coherent, and faithful real image edits through applying it on a
variety of inputs for a multitude of tasks. We also formalize our method in
well-established theory, detail future experiments for further improvement, and
compare against state-of-the-art attempts.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for Medical Image Analysis: A Comprehensive Survey
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 14, 2022 </span>    
         <span class="authors"> Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, Dorit Merhof </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.07804" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models, a class of generative models, have garnered
immense interest lately in various deep-learning problems. A diffusion
probabilistic model defines a forward diffusion stage where the input data is
gradually perturbed over several steps by adding Gaussian noise and then learns
to reverse the diffusion process to retrieve the desired noise-free data from
noisy data samples. Diffusion models are widely appreciated for their strong
mode coverage and quality of the generated samples despite their known
computational burdens. Capitalizing on the advances in computer vision, the
field of medical imaging has also observed a growing interest in diffusion
models. To help the researcher navigate this profusion, this survey intends to
provide a comprehensive overview of diffusion models in the discipline of
medical image analysis. Specifically, we introduce the solid theoretical
foundation and fundamental concepts behind diffusion models and the three
generic diffusion modelling frameworks: diffusion probabilistic models,
noise-conditioned score networks, and stochastic differential equations. Then,
we provide a systematic taxonomy of diffusion models in the medical domain and
propose a multi-perspective categorization based on their application, imaging
modality, organ of interest, and algorithms. To this end, we cover extensive
applications of diffusion models in the medical domain. Furthermore, we
emphasize the practical use case of some selected approaches, and then we
discuss the limitations of the diffusion models in the medical domain and
propose several directions to fulfill the demands of this field. Finally, we
gather the overviewed studies with their available open-source implementations
at
https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Extreme Generative Image Compression by Learning Text Embedding from Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 14, 2022 </span>    
         <span class="authors"> Zhihong Pan, Xin Zhou, Hao Tian </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.07793" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Transferring large amount of high resolution images over limited bandwidth is
an important but very challenging task. Compressing images using extremely low
bitrates (<0.1 bpp) has been studied but it often results in low quality images
of heavy artifacts due to the strong constraint in the number of bits available
for the compressed data. It is often said that a picture is worth a thousand
words but on the other hand, language is very powerful in capturing the essence
of an image using short descriptions. With the recent success of diffusion
models for text-to-image generation, we propose a generative image compression
method that demonstrates the potential of saving an image as a short text
embedding which in turn can be used to generate high-fidelity images which is
equivalent to the original one perceptually. For a given image, its
corresponding text embedding is learned using the same optimization process as
the text-to-image diffusion model itself, using a learnable text embedding as
input after bypassing the original transformer. The optimization is applied
together with a learning compression model to achieve extreme compression of
low bitrates <0.1 bpp. Based on our experiments measured by a comprehensive set
of image quality metrics, our method outperforms the other state-of-the-art
deep learning methods in terms of both perceptual quality and diversity.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Arbitrary Style Guidance for Enhanced Diffusion-Based Text-to-Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 14, 2022 </span>    
         <span class="authors"> Zhihong Pan, Xin Zhou, Hao Tian </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.07751" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based text-to-image generation models like GLIDE and DALLE-2 have
gained wide success recently for their superior performance in turning complex
text inputs into images of high quality and wide diversity. In particular, they
are proven to be very powerful in creating graphic arts of various formats and
styles. Although current models supported specifying style formats like oil
painting or pencil drawing, fine-grained style features like color
distributions and brush strokes are hard to specify as they are randomly picked
from a conditional distribution based on the given text input. Here we propose
a novel style guidance method to support generating images using arbitrary
style guided by a reference image. The generation method does not require a
separate style transfer model to generate desired styles while maintaining
image quality in generated content as controlled by the text input.
Additionally, the guidance method can be applied without a style reference,
denoted as self style guidance, to generate images of more diverse styles.
Comprehensive experiments prove that the proposed method remains robust and
effective in a wide range of conditions, including diverse graphic art forms,
image content types and diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Models for Out-of-Distribution Detection
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 14, 2022 </span>    
         <span class="authors"> Mark S. Graham, Walter H. L. Pinaya, Petru-Daniel Tudosiu, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.07740" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Out-of-distribution detection is crucial to the safe deployment of machine
learning systems. Currently, unsupervised out-of-distribution detection is
dominated by generative-based approaches that make use of estimates of the
likelihood or other measurements from a generative model. Reconstruction-based
methods offer an alternative approach, in which a measure of reconstruction
error is used to determine if a sample is out-of-distribution. However,
reconstruction-based approaches are less favoured, as they require careful
tuning of the model's information bottleneck - such as the size of the latent
dimension - to produce good results. In this work, we exploit the view of
denoising diffusion probabilistic models (DDPM) as denoising autoencoders where
the bottleneck is controlled externally, by means of the amount of noise
applied. We propose to use DDPMs to reconstruct an input that has been noised
to a range of noise levels, and use the resulting multi-dimensional
reconstruction error to classify out-of-distribution inputs. We validate our
approach both on standard computer-vision datasets and on higher dimension
medical datasets. Our approach outperforms not only reconstruction-based
methods, but also state-of-the-art generative-based approaches. Code is
available at https://github.com/marksgraham/ddpm-ood.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 14, 2022 </span>    
         <span class="authors"> Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.07600" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-guided image generation has progressed rapidly in recent years,
inspiring major breakthroughs in text-guided shape generation. Recently, it has
been shown that using score distillation, one can successfully text-guide a
NeRF model to generate a 3D object. We adapt the score distillation to the
publicly available, and computationally efficient, Latent Diffusion Models,
which apply the entire diffusion process in a compact latent space of a
pretrained autoencoder. As NeRFs operate in image space, a naive solution for
guiding them with latent score distillation would require encoding to the
latent space at each guidance step. Instead, we propose to bring the NeRF to
the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we
show that while Text-to-3D models can generate impressive results, they are
inherently unconstrained and may lack the ability to guide or enforce a
specific 3D structure. To assist and direct the 3D generation, we propose to
guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines
the coarse structure of the desired object. Then, we present means to integrate
such a constraint directly into a Latent-NeRF. This unique combination of text
and shape guidance allows for increased control over the generation process. We
also show that latent score distillation can be successfully applied directly
on 3D meshes. This allows for generating high-quality textures on a given
geometry. Our experiments validate the power of our different forms of guidance
and the efficiency of using latent rendering. Implementation is available at
https://github.com/eladrich/latent-nerf
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Fast Text-Conditional Discrete Denoising on Vector-Quantized Latent Spaces
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 14, 2022 </span>    
         <span class="authors"> Dominic Rampas, Pablo Pernias, Marc Aubreville </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.07292" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advancements in the domain of text-to-image synthesis have culminated
in a multitude of enhancements pertaining to quality, fidelity, and diversity.
Contemporary techniques enable the generation of highly intricate visuals which
rapidly approach near-photorealistic quality. Nevertheless, as progress is
achieved, the complexity of these methodologies increases, consequently
intensifying the comprehension barrier between individuals within the field and
those external to it.
  In an endeavor to mitigate this disparity, we propose a streamlined approach
for text-to-image generation, which encompasses both the training paradigm and
the sampling process. Despite its remarkable simplicity, our method yields
aesthetically pleasing images with few sampling iterations, allows for
intriguing ways for conditioning the model, and imparts advantages absent in
state-of-the-art techniques. To demonstrate the efficacy of this approach in
achieving outcomes comparable to existing works, we have trained a one-billion
parameter text-conditional model, which we refer to as "Paella". In the
interest of fostering future exploration in this field, we have made our source
code and models publicly accessible for the research community.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## TIER-A: Denoising Learning Framework for Information Extraction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 13, 2022 </span>    
         <span class="authors"> Yongkang Li, Ming Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.11527" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.AI, cs.IR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 With the development of deep neural language models, great progress has been
made in information extraction recently. However, deep learning models often
overfit on noisy data points, leading to poor performance. In this work, we
examine the role of information entropy in the overfitting process and draw a
key insight that overfitting is a process of overconfidence and entropy
decreasing. Motivated by such properties, we propose a simple yet effective
co-regularization joint-training framework TIER-A, Aggregation Joint-training
Framework with Temperature Calibration and Information Entropy Regularization.
Our framework consists of several neural models with identical structures.
These models are jointly trained and we avoid overfitting by introducing
temperature and information entropy regularization. Extensive experiments on
two widely-used but noisy datasets, TACRED and CoNLL03, demonstrate the
correctness of our assumption and the effectiveness of our framework.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DriftRec: Adapting diffusion models to blind image restoration tasks
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 12, 2022 </span>    
         <span class="authors"> Simon Welker, Henry N. Chapman, Timo Gerkmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.06757" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we utilize the high-fidelity generation abilities of diffusion
models to solve blind image restoration tasks, using JPEG artifact removal at
high compression levels as an example. We propose an elegant modification of
the forward stochastic differential equation of diffusion models to adapt them
to restoration tasks and name our method DriftRec. Comparing DriftRec against
an $L_2$ regression baseline with the same network architecture and a
state-of-the-art technique for JPEG reconstruction, we show that our approach
can escape both baselines' tendency to generate blurry images, and recovers the
distribution of clean images significantly more faithfully while only requiring
a dataset of clean/corrupted image pairs and no knowledge about the corruption
operation. By utilizing the idea that the distributions of clean and corrupted
images are much closer to each other than to a Gaussian prior, our approach
requires only low levels of added noise, and thus needs comparatively few
sampling steps even without further optimizations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## HumanDiffusion: a Coarse-to-Fine Alignment Diffusion Framework for Controllable Text-Driven Person Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 11, 2022 </span>    
         <span class="authors"> Kaiduo Zhang, Muyi Sun, Jianxin Sun, Binghao Zhao, Kunbo Zhang, Zhenan Sun, Tieniu Tan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.06235" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-driven person image generation is an emerging and challenging task in
cross-modality image generation. Controllable person image generation promotes
a wide range of applications such as digital human interaction and virtual
try-on. However, previous methods mostly employ single-modality information as
the prior condition (e.g. pose-guided person image generation), or utilize the
preset words for text-driven human synthesis. Introducing a sentence composed
of free words with an editable semantic pose map to describe person appearance
is a more user-friendly way. In this paper, we propose HumanDiffusion, a
coarse-to-fine alignment diffusion framework, for text-driven person image
generation. Specifically, two collaborative modules are proposed, the Stylized
Memory Retrieval (SMR) module for fine-grained feature distillation in data
processing and the Multi-scale Cross-modality Alignment (MCA) module for
coarse-to-fine feature alignment in diffusion. These two modules guarantee the
alignment quality of the text and image, from image-level to feature-level,
from low-resolution to high-resolution. As a result, HumanDiffusion realizes
open-vocabulary person image generation with desired semantic poses. Extensive
experiments conducted on DeepFashion demonstrate the superiority of our method
compared with previous approaches. Moreover, better results could be obtained
for complicated person images with various details and uncommon poses.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## StructDiffusion: Object-Centric Diffusion for Semantic Rearrangement of Novel Objects
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 08, 2022 </span>    
         <span class="authors"> Weiyu Liu, Yilun Du, Tucker Hermans, Sonia Chernova, Chris Paxton </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.04604" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.RO, cs.AI, cs.CL, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Robots operating in human environments must be able to rearrange objects into
semantically-meaningful configurations, even if these objects are previously
unseen. In this work, we focus on the problem of building physically-valid
structures without step-by-step instructions. We propose StructDiffusion, which
combines a diffusion model and an object-centric transformer to construct
structures given partial-view point clouds and high-level language goals, such
as "set the table". Our method can perform multiple challenging
language-conditioned multi-step 3D planning tasks using one model.
StructDiffusion even improves the success rate of assembling physically-valid
structures out of unseen objects by on average 16% over an existing multi-modal
transformer model trained on specific structures. We show experiments on
held-out objects in both simulation and on real-world rearrangement tasks.
Importantly, we show how integrating both a diffusion model and a
collision-discriminator model allows for improved generalization over other
methods when rearranging previously-unseen objects. For videos and additional
results, see our website: https://structdiffusion.github.io/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffPhase: Generative Diffusion-based STFT Phase Retrieval
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 08, 2022 </span>    
         <span class="authors"> Tal Peer, Simon Welker, Timo Gerkmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.04332" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models have been recently used in a variety of tasks,
including speech enhancement and synthesis. As a generative approach, diffusion
models have been shown to be especially suitable for imputation problems, where
missing data is generated based on existing data. Phase retrieval is inherently
an imputation problem, where phase information has to be generated based on the
given magnitude. In this work we build upon previous work in the speech domain,
adapting a speech enhancement diffusion model specifically for STFT phase
retrieval. Evaluation using speech quality and intelligibility metrics shows
the diffusion approach is well-suited to the phase retrieval task, with
performance surpassing both classical and modern methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Self-conditioned Embedding Diffusion for Text Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 08, 2022 </span>    
         <span class="authors"> Robin Strudel, Corentin Tallec, Florent Altché, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, Rémi Leblond </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.04236" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unsupervised vocal dereverberation with diffusion-based generative models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 08, 2022 </span>    
         <span class="authors"> Koichi Saito, Naoki Murata, Toshimitsu Uesaka, Chieh-Hsin Lai, Yuhta Takida, Takao Fukui, Yuki Mitsufuji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.04124" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Removing reverb from reverberant music is a necessary technique to clean up
audio for downstream music manipulations. Reverberation of music contains two
categories, natural reverb, and artificial reverb. Artificial reverb has a
wider diversity than natural reverb due to its various parameter setups and
reverberation types. However, recent supervised dereverberation methods may
fail because they rely on sufficiently diverse and numerous pairs of
reverberant observations and retrieved data for training in order to be
generalizable to unseen observations during inference. To resolve these
problems, we propose an unsupervised method that can remove a general kind of
artificial reverb for music without requiring pairs of data for training. The
proposed method is based on diffusion models, where it initializes the unknown
reverberation operator with a conventional signal processing technique and
simultaneously refines the estimate with the help of diffusion models. We show
through objective and perceptual evaluations that our method outperforms the
current leading vocal dereverberation benchmarks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## From Denoising Diffusions to Denoising Markov Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 07, 2022 </span>    
         <span class="authors"> Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.03595" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusions are state-of-the-art generative models exhibiting
remarkable empirical performance. They work by diffusing the data distribution
into a Gaussian distribution and then learning to reverse this noising process
to obtain synthetic datapoints. The denoising diffusion relies on
approximations of the logarithmic derivatives of the noised data densities
using score matching. Such models can also be used to perform approximate
posterior simulation when one can only sample from the prior and likelihood. We
propose a unifying framework generalising this approach to a wide class of
spaces and leading to an original extension of score matching. We illustrate
the resulting models on various applications.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Medical Diffusion -- Denoising Diffusion Probabilistic Models for 3D Medical Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 07, 2022 </span>    
         <span class="authors"> Firas Khader, Gustav Mueller-Franzes, Soroosh Tayebi Arasteh, Tianyu Han, Christoph Haarburger, Maximilian Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina Baessler, Sebastian Foersch, Johannes Stegmaier, Christiane Kuhl, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.03364" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advances in computer vision have shown promising results in image
generation. Diffusion probabilistic models in particular have generated
realistic images from textual input, as demonstrated by DALL-E 2, Imagen and
Stable Diffusion. However, their use in medicine, where image data typically
comprises three-dimensional volumes, has not been systematically evaluated.
Synthetic images may play a crucial role in privacy preserving artificial
intelligence and can also be used to augment small datasets. Here we show that
diffusion probabilistic models can synthesize high quality medical imaging
data, which we show for Magnetic Resonance Images (MRI) and Computed Tomography
(CT) images. We provide quantitative measurements of their performance through
a reader study with two medical experts who rated the quality of the
synthesized images in three categories: Realistic image appearance, anatomical
correctness and consistency between slices. Furthermore, we demonstrate that
synthetic images can be used in a self-supervised pre-training and improve the
performance of breast segmentation models when data is scarce (dice score 0.91
vs. 0.95 without vs. with synthetic data). The code is publicly available on
GitHub: https://github.com/FirasGit/medicaldiffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Modeling Temporal Data as Continuous Functions with Process Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 04, 2022 </span>    
         <span class="authors"> Marin Biloš, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Stephan Günnemann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.02590" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Temporal data such as time series can be viewed as discretized measurements
of the underlying function. To build a generative model for such data we have
to model the stochastic process that governs it. We propose a solution by
defining the denoising diffusion model in the function space which also allows
us to naturally handle irregularly-sampled observations. The forward process
gradually adds noise to functions, preserving their continuity, while the
learned reverse process removes the noise and returns functions as new samples.
To this end, we define suitable noise sources and introduce novel denoising and
score-matching models. We show how our method can be used for multivariate
probabilistic forecasting and imputation, and how our model can be interpreted
as a neural process.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Cold Diffusion for Speech Enhancement
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 04, 2022 </span>    
         <span class="authors"> Hao Yen, François G. Germain, Gordon Wichern, Jonathan Le Roux </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.02527" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently shown promising results for difficult
enhancement tasks such as the conditional and unconditional restoration of
natural images and audio signals. In this work, we explore the possibility of
leveraging a recently proposed advanced iterative diffusion model, namely cold
diffusion, to recover clean speech signals from noisy signals. The unique
mathematical properties of the sampling process from cold diffusion could be
utilized to restore high-quality samples from arbitrary degradations. Based on
these properties, we propose an improved training algorithm and objective to
help the model generalize better during the sampling process. We verify our
proposed framework by investigating two model architectures. Experimental
results on benchmark speech enhancement dataset VoiceBank-DEMAND demonstrate
the strong performance of the proposed approach compared to representative
discriminative models and diffusion-based enhancement models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 04, 2022 </span>    
         <span class="authors"> Lukas Struppek, Dominik Hintersdorf, Kristian Kersting </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.02408" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CR, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While text-to-image synthesis currently enjoys great popularity among
researchers and the general public, the security of these models has been
neglected so far. Many text-guided image generation models rely on pre-trained
text encoders from external sources, and their users trust that the retrieved
models will behave as promised. Unfortunately, this might not be the case. We
introduce backdoor attacks against text-guided generative models and
demonstrate that their text encoders pose a major tampering risk. Our attacks
only slightly alter an encoder so that no suspicious model behavior is apparent
for image generations with clean prompts. By then inserting a single character
trigger into the prompt, e.g., a non-Latin character or emoji, the adversary
can trigger the model to either generate images with pre-defined attributes or
images following a hidden, potentially malicious description. We empirically
demonstrate the high effectiveness of our attacks on Stable Diffusion and
highlight that the injection process of a single backdoor takes less than two
minutes. Besides phrasing our approach solely as an attack, it can also force
an encoder to forget phrases related to certain concepts, such as nudity or
violence, and help to make image generation safer.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Analysing Diffusion-based Generative Approaches versus Discriminative Approaches for Speech Restoration
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 04, 2022 </span>    
         <span class="authors"> Jean-Marie Lemercier, Julius Richter, Simon Welker, Timo Gerkmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.02397" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based generative models have had a high impact on the computer
vision and speech processing communities these past years. Besides data
generation tasks, they have also been employed for data restoration tasks like
speech enhancement and dereverberation. While discriminative models have
traditionally been argued to be more powerful e.g. for speech enhancement,
generative diffusion approaches have recently been shown to narrow this
performance gap considerably. In this paper, we systematically compare the
performance of generative diffusion models and discriminative approaches on
different speech restoration tasks. For this, we extend our prior contributions
on diffusion-based speech enhancement in the complex time-frequency domain to
the task of bandwith extension. We then compare it to a discriminatively
trained neural network with the same network architecture on three restoration
tasks, namely speech denoising, dereverberation and bandwidth extension. We
observe that the generative approach performs globally better than its
discriminative counterpart on all tasks, with the strongest benefit for
non-additive distortion models, like in dereverberation and bandwidth
extension. Code and audio examples can be found online at
https://uhh.de/inf-sp-sgmsemultitask
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 03, 2022 </span>    
         <span class="authors"> Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, Jun-Yan Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.02048" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 During image editing, existing deep generative models tend to re-synthesize
the entire output from scratch, including the unedited regions. This leads to a
significant waste of computation, especially for minor editing operations. In
this work, we present Spatially Sparse Inference (SSI), a general-purpose
technique that selectively performs computation for edited regions and
accelerates various generative models, including both conditional GANs and
diffusion models. Our key observation is that users tend to gradually change
the input image. This motivates us to cache and reuse the feature maps of the
original image. Given an edited image, we sparsely apply the convolutional
filters to the edited regions while reusing the cached features for the
unedited areas. Based on our algorithm, we further propose Sparse Incremental
Generative Engine (SIGE) to convert the computation reduction to latency
reduction on off-the-shelf hardware. With about $1\%$-area edits, our method
reduces the computation of DDPM by $7.5\times$, Stable Diffusion by
$8.2\times$, and GauGAN by $18\times$ while preserving the visual fidelity.
With SIGE, we accelerate the inference time of DDPM by $3.0\times$ on NVIDIA
RTX 3090 and $6.6\times$ on Apple M1 Pro CPU, Stable Diffusion by $7.2\times$
on 3090, and GauGAN by $5.6\times$ on 3090 and $14\times$ on M1 Pro CPU.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Evaluating a Synthetic Image Dataset Generated with Stable Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 03, 2022 </span>    
         <span class="authors"> Andreas Stöckl </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.01777" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We generate synthetic images with the "Stable Diffusion" image generation
model using the Wordnet taxonomy and the definitions of concepts it contains.
This synthetic image database can be used as training data for data
augmentation in machine learning applications, and it is used to investigate
the capabilities of the Stable Diffusion model.
  Analyses show that Stable Diffusion can produce correct images for a large
number of concepts, but also a large variety of different representations. The
results show differences depending on the test concepts considered and problems
with very specific concepts. These evaluations were performed using a vision
transformer model for image classification.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Convergence in KL Divergence of the Inexact Langevin Algorithm with Application to Score-based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2022 </span>    
         <span class="authors"> Andre Wibisono, Kaylee Yingxi Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.01512" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.ST, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We study the Inexact Langevin Algorithm (ILA) for sampling using estimated
score function when the target distribution satisfies log-Sobolev inequality
(LSI), motivated by Score-based Generative Modeling (SGM). We prove a long-term
convergence in Kullback-Leibler (KL) divergence under a sufficient assumption
that the error of the score estimator has a bounded Moment Generating Function
(MGF). Our assumption is weaker than $L^\infty$ (which is too strong to hold in
practice) and stronger than $L^2$ error assumption, which we show not
sufficient to guarantee convergence in general. Under the $L^\infty$ error
assumption, we additionally prove convergence in R\'enyi divergence, which is
stronger than KL divergence. We then study how to get a provably accurate score
estimator which satisfies bounded MGF assumption for LSI target distributions,
by using an estimator based on kernel density estimation. Together with the
convergence results, we yield the first end-to-end convergence guarantee for
ILA in the population level. Last, we generalize our convergence analysis to
SGM and derive a complexity guarantee in KL divergence for data satisfying LSI
under MGF-accurate score estimator.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## An optimal control perspective on diffusion-based generative modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2022 </span>    
         <span class="authors"> Julius Berner, Lorenz Richter, Karen Ullrich </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.01364" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.OC, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We establish a connection between stochastic optimal control and generative
models based on stochastic differential equations (SDEs) such as recently
developed diffusion probabilistic models. In particular, we derive a
Hamilton-Jacobi-Bellman equation that governs the evolution of the
log-densities of the underlying SDE marginals. This perspective allows to
transfer methods from optimal control theory to generative modeling. First, we
show that the evidence lower bound is a direct consequence of the well-known
verification theorem from control theory. Further, we develop a novel
diffusion-based method for sampling from unnormalized densities -- a problem
frequently occurring in statistics and computational sciences.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2022 </span>    
         <span class="authors"> Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.01324" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large-scale diffusion-based generative models have led to breakthroughs in
text-conditioned high-resolution image synthesis. Starting from random noise,
such text-to-image diffusion models gradually synthesize images in an iterative
fashion while conditioning on text prompts. We find that their synthesis
behavior qualitatively changes throughout this process: Early in sampling,
generation strongly relies on the text prompt to generate text-aligned content,
while later, the text conditioning is almost entirely ignored. This suggests
that sharing model parameters throughout the entire generation process may not
be ideal. Therefore, in contrast to existing works, we propose to train an
ensemble of text-to-image diffusion models specialized for different synthesis
stages. To maintain training efficiency, we initially train a single model,
which is then split into specialized models that are trained for the specific
stages of the iterative generation process. Our ensemble of diffusion models,
called eDiff-I, results in improved text alignment while maintaining the same
inference computation cost and preserving high visual quality, outperforming
previous large-scale text-to-image diffusion models on the standard benchmark.
In addition, we train our model to exploit a variety of embeddings for
conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We
show that these different embeddings lead to different behaviors. Notably, the
CLIP image embedding allows an intuitive way of transferring the style of a
reference image to the target text-to-image output. Lastly, we show a technique
that enables eDiff-I's "paint-with-words" capability. A user can select the
word in the input text and paint it in a canvas to control the output, which is
very handy for crafting the desired image in mind. The project page is
available at https://deepimagination.cc/eDiff-I/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generation of Anonymous Chest Radiographs Using Latent Diffusion Models for Training Thoracic Abnormality Classification Systems
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2022 </span>    
         <span class="authors"> Kai Packhäuser, Lukas Folle, Florian Thamm, Andreas Maier </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.01323" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The availability of large-scale chest X-ray datasets is a requirement for
developing well-performing deep learning-based algorithms in thoracic
abnormality detection and classification. However, biometric identifiers in
chest radiographs hinder the public sharing of such data for research purposes
due to the risk of patient re-identification. To counteract this issue,
synthetic data generation offers a solution for anonymizing medical images.
This work employs a latent diffusion model to synthesize an anonymous chest
X-ray dataset of high-quality class-conditional images. We propose a
privacy-enhancing sampling strategy to ensure the non-transference of biometric
information during the image generation process. The quality of the generated
images and the feasibility of serving as exclusive training data are evaluated
on a thoracic abnormality classification task. Compared to a real classifier,
we achieve competitive results with a performance gap of only 3.5% in the area
under the receiver operating characteristic curve.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Entropic Neural Optimal Transport via Diffusion Processes
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2022 </span>    
         <span class="authors"> Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, Evgeny Burnaev </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.01156" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a novel neural algorithm for the fundamental problem of computing
the entropic optimal transport (EOT) plan between probability distributions
which are accessible by samples. Our algorithm is based on the saddle point
reformulation of the dynamic version of EOT which is known as the Schr\"odinger
Bridge problem. In contrast to the prior methods for large-scale EOT, our
algorithm is end-to-end and consists of a single learning step, has fast
inference procedure, and allows handling small values of the entropy
regularization coefficient which is of particular importance in some applied
problems. Empirically, we show the performance of the method on several
large-scale EOT tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2022 </span>    
         <span class="authors"> Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.01095" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPMs) have achieved impressive success in
high-resolution image synthesis, especially in recent large-scale text-to-image
generation applications. An essential technique for improving the sample
quality of DPMs is guided sampling, which usually needs a large guidance scale
to obtain the best sample quality. The commonly-used fast sampler for guided
sampling is DDIM, a first-order diffusion ODE solver that generally needs 100
to 250 steps for high-quality samples. Although recent works propose dedicated
high-order solvers and achieve a further speedup for sampling without guidance,
their effectiveness for guided sampling has not been well-tested before. In
this work, we demonstrate that previous high-order fast samplers suffer from
instability issues, and they even become slower than DDIM when the guidance
scale grows large. To further speed up guided sampling, we propose
DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++
solves the diffusion ODE with the data prediction model and adopts thresholding
methods to keep the solution matches training data distribution. We further
propose a multistep variant of DPM-Solver++ to address the instability issue by
reducing the effective step size. Experiments show that DPM-Solver++ can
generate high-quality samples within only 15 to 20 steps for guided sampling by
pixel-space and latent-space DPMs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Spot the fake lungs: Generating Synthetic Medical Images using Neural Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2022 </span>    
         <span class="authors"> Hazrat Ali, Shafaq Murad, Zubair Shah </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.00902" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.AI, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative models are becoming popular for the synthesis of medical images.
Recently, neural diffusion models have demonstrated the potential to generate
photo-realistic images of objects. However, their potential to generate medical
images is not explored yet. In this work, we explore the possibilities of
synthesis of medical images using neural diffusion models. First, we use a
pre-trained DALLE2 model to generate lungs X-Ray and CT images from an input
text prompt. Second, we train a stable diffusion model with 3165 X-Ray images
and generate synthetic images. We evaluate the synthetic image data through a
qualitative analysis where two independent radiologists label randomly chosen
samples from the generated data as real, fake, or unsure. Results demonstrate
that images generated with the diffusion model can translate characteristics
that are otherwise very specific to certain medical conditions in chest X-Ray
or CT images. Careful tuning of the model can be very promising. To the best of
our knowledge, this is the first attempt to generate lungs X-Ray and CT images
using neural diffusion models. This work aims to introduce a new dimension in
artificial intelligence for medical imaging. Given that this is a new topic,
the paper will serve as an introduction and motivation for the research
community to explore the potential of diffusion models for medical image
synthesis. We have released the synthetic images on
https://www.kaggle.com/datasets/hazrat/awesomelungs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Concrete Score Matching: Generalized Score Matching for Discrete Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2022 </span>    
         <span class="authors"> Chenlin Meng, Kristy Choi, Jiaming Song, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.00802" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Representing probability distributions by the gradient of their density
functions has proven effective in modeling a wide range of continuous data
modalities. However, this representation is not applicable in discrete domains
where the gradient is undefined. To this end, we propose an analogous score
function called the "Concrete score", a generalization of the (Stein) score for
discrete settings. Given a predefined neighborhood structure, the Concrete
score of any input is defined by the rate of change of the probabilities with
respect to local directional changes of the input. This formulation allows us
to recover the (Stein) score in continuous domains when measuring such changes
by the Euclidean distance, while using the Manhattan distance leads to our
novel score function in discrete domains. Finally, we introduce a new framework
to learn such scores from samples called Concrete Score Matching (CSM), and
propose an efficient training objective to scale our approach to high
dimensions. Empirically, we demonstrate the efficacy of CSM on density
estimation tasks on a mixture of synthetic, tabular, and high-dimensional image
datasets, and demonstrate that it performs favorably relative to existing
baselines for modeling discrete data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On the detection of synthetic images generated by diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 01, 2022 </span>    
         <span class="authors"> Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, Luisa Verdoliva </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.00680" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Over the past decade, there has been tremendous progress in creating
synthetic media, mainly thanks to the development of powerful methods based on
generative adversarial networks (GAN). Very recently, methods based on
diffusion models (DM) have been gaining the spotlight. In addition to providing
an impressive level of photorealism, they enable the creation of text-based
visual content, opening up new and exciting opportunities in many different
application fields, from arts to video games. On the other hand, this property
is an additional asset in the hands of malicious users, who can generate and
distribute fake media perfectly adapted to their attacks, posing new challenges
to the media forensic community. With this work, we seek to understand how
difficult it is to distinguish synthetic images generated by diffusion models
from pristine ones and whether current state-of-the-art detectors are suitable
for the task. To this end, first we expose the forensics traces left by
diffusion models, then study how current detectors, developed for GAN-generated
images, perform on these new synthetic images, especially in challenging
social-networks scenarios involving image compression and resizing. Datasets
and code are available at github.com/grip-unina/DMimageDetection.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 01, 2022 </span>    
         <span class="authors"> Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yehui Yang, Haoyi Xiong, Huiying Liu, Yanwu Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.00611" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic model (DPM) recently becomes one of the hottest topic
in computer vision. Its image generation application such as Imagen, Latent
Diffusion Models and Stable Diffusion have shown impressive generation
capabilities, which aroused extensive discussion in the community. Many recent
studies also found it is useful in many other vision tasks, like image
deblurring, super-resolution and anomaly detection. Inspired by the success of
DPM, we propose the first DPM based model toward general medical image
segmentation tasks, which we named MedSegDiff. In order to enhance the
step-wise regional attention in DPM for the medical image segmentation, we
propose dynamic conditional encoding, which establishes the state-adaptive
conditions for each sampling step. We further propose Feature Frequency Parser
(FF-Parser), to eliminate the negative effect of high-frequency noise component
in this process. We verify MedSegDiff on three medical segmentation tasks with
different image modalities, which are optic cup segmentation over fundus
images, brain tumor segmentation over MRI images and thyroid nodule
segmentation over ultrasound images. The experimental results show that
MedSegDiff outperforms state-of-the-art (SOTA) methods with considerable
performance gap, indicating the generalization and effectiveness of the
proposed model. Our code is released at https://github.com/WuJunde/MedSegDiff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DOLPH: Diffusion Models for Phase Retrieval
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 01, 2022 </span>    
         <span class="authors"> Shirin Shoushtari, Jiaming Liu, Ulugbek S. Kamilov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.00529" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Phase retrieval refers to the problem of recovering an image from the
magnitudes of its complex-valued linear measurements. Since the problem is
ill-posed, the recovery requires prior knowledge on the unknown image. We
present DOLPH as a new deep model-based architecture for phase retrieval that
integrates an image prior specified using a diffusion model with a nonconvex
data-fidelity term for phase retrieval. Diffusion models are a recent class of
deep generative models that are relatively easy to train due to their
implementation as image denoisers. DOLPH reconstructs high-quality solutions by
alternating data-consistency updates with the sampling step of a diffusion
model. Our numerical results show the robustness of DOLPH to noise and its
ability to generate several candidate solutions given a set of measurements.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SDMuse: Stochastic Differential Music Editing and Generation via Hybrid Representation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 01, 2022 </span>    
         <span class="authors"> Chen Zhang, Yi Ren, Kejun Zhang, Shuicheng Yan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.00222" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.MM, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While deep generative models have empowered music generation, it remains a
challenging and under-explored problem to edit an existing musical piece at
fine granularity. In this paper, we propose SDMuse, a unified Stochastic
Differential Music editing and generation framework, which can not only compose
a whole musical piece from scratch, but also modify existing musical pieces in
many ways, such as combination, continuation, inpainting, and style
transferring. The proposed SDMuse follows a two-stage pipeline to achieve music
generation and editing on top of a hybrid representation including pianoroll
and MIDI-event. In particular, SDMuse first generates/edits pianoroll by
iteratively denoising through a stochastic differential equation (SDE) based on
a diffusion model generative prior, and then refines the generated pianoroll
and predicts MIDI-event tokens auto-regressively. We evaluate the generated
music of our method on ailabs1k7 pop music dataset in terms of quality and
controllability on various music editing and generation tasks. Experimental
results demonstrate the effectiveness of our proposed stochastic differential
music editing and generation process, as well as the hybrid representations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Accelerated Motion Correction for MRI using Score-Based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 01, 2022 </span>    
         <span class="authors"> Brett Levac, Ajil Jalal, Jonathan I. Tamir </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2211.00199" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Magnetic Resonance Imaging (MRI) is a powerful medical imaging modality, but
unfortunately suffers from long scan times which, aside from increasing
operational costs, can lead to image artifacts due to patient motion. Motion
during the acquisition leads to inconsistencies in measured data that manifest
as blurring and ghosting if unaccounted for in the image reconstruction
process. Various deep learning based reconstruction techniques have been
proposed which decrease scan time by reducing the number of measurements needed
for a high fidelity reconstructed image. Additionally, deep learning has been
used to correct motion using end-to-end techniques. This, however, increases
susceptibility to distribution shifts at test time (sampling pattern, motion
level). In this work we propose a framework for jointly reconstructing highly
sub-sampled MRI data while estimating patient motion using score-based
generative models. Our method does not make specific assumptions on the
sampling trajectory or motion pattern at training time and thus can be flexibly
applied to various types of measurement models and patient motion. We
demonstrate our framework on retrospectively accelerated 2D brain MRI corrupted
by rigid motion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 31, 2022 </span>    
         <span class="authors"> Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.17432" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite the growing success of diffusion models in continuous-valued domains
(e.g., images), diffusion-based language models on discrete text have yet to
match autoregressive language models on text generation benchmarks. In this
work, we present SSD-LM -- a diffusion language model with two key design
choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of
text, allowing for flexible output length at decoding time while enabling local
bidirectional context updates. Second, it is simplex-based, performing
diffusion on the natural vocabulary space rather than a learned latent space,
allowing us to incorporate classifier guidance and modular control without any
adaptation of off-the-shelf classifiers. We evaluate SSD-LM on unconstrained as
well as controlled text generation benchmarks, and show that it matches or
outperforms strong autoregressive GPT-2 baselines across standard quality and
diversity metrics.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Guided Conditional Diffusion for Controllable Traffic Simulation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 31, 2022 </span>    
         <span class="authors"> Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, Marco Pavone </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.17366" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.RO, cs.AI, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Controllable and realistic traffic simulation is critical for developing and
verifying autonomous vehicles. Typical heuristic-based traffic models offer
flexible control to make vehicles follow specific trajectories and traffic
rules. On the other hand, data-driven approaches generate realistic and
human-like behaviors, improving transfer from simulated to real-world traffic.
However, to the best of our knowledge, no traffic model offers both
controllability and realism. In this work, we develop a conditional diffusion
model for controllable traffic generation (CTG) that allows users to control
desired properties of trajectories at test time (e.g., reach a goal or follow a
speed limit) while maintaining realism and physical feasibility through
enforced dynamics. The key technical idea is to leverage recent advances from
diffusion modeling and differentiable logic to guide generated trajectories to
meet rules defined using signal temporal logic (STL). We further extend
guidance to multi-agent settings and enable interaction-based rules like
collision avoidance. CTG is extensively evaluated on the nuScenes dataset for
diverse and composite rules, demonstrating improvement over strong baselines in
terms of the controllability-realism tradeoff.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion models for missing value imputation in tabular data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 31, 2022 </span>    
         <span class="authors"> Shuhan Zheng, Nontawat Charoenphakdee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.17128" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Missing value imputation in machine learning is the task of estimating the
missing values in the dataset accurately using available information. In this
task, several deep generative modeling methods have been proposed and
demonstrated their usefulness, e.g., generative adversarial imputation
networks. Recently, diffusion models have gained popularity because of their
effectiveness in the generative modeling task in images, texts, audio, etc. To
our knowledge, less attention has been paid to the investigation of the
effectiveness of diffusion models for missing value imputation in tabular data.
Based on recent development of diffusion models for time-series data
imputation, we propose a diffusion model approach called "Conditional
Score-based Diffusion Models for Tabular data" (TabCSDI). To effectively handle
categorical variables and numerical variables simultaneously, we investigate
three techniques: one-hot encoding, analog bits encoding, and feature
tokenization. Experimental results on benchmark datasets demonstrated the
effectiveness of TabCSDI compared with well-known existing methods, and also
emphasized the importance of the categorical embedding techniques.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusER: Discrete Diffusion via Edit-based Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 30, 2022 </span>    
         <span class="authors"> Machel Reid, Vincent J. Hellendoorn, Graham Neubig </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.16886" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In text generation, models that generate text from scratch one token at a
time are currently the dominant paradigm. Despite being performant, these
models lack the ability to revise existing text, which limits their usability
in many practical scenarios. We look to address this, with DiffusER (Diffusion
via Edit-based Reconstruction), a new edit-based generative model for text
based on denoising diffusion models -- a class of models that use a Markov
chain of denoising steps to incrementally generate data. DiffusER is not only a
strong generative model in general, rivalling autoregressive models on several
tasks spanning machine translation, summarization, and style transfer; it can
also perform other varieties of generation that standard autoregressive models
are not well-suited for. For instance, we demonstrate that DiffusER makes it
possible for a user to condition generation on a prototype, or an incomplete
sequence, and continue revising based on previous edit steps.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditioning and Sampling in Variational Diffusion Models for Speech Super-resolution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 27, 2022 </span>    
         <span class="authors"> Chin-Yun Yu, Sung-Lin Yeh, György Fazekas, Hao Tang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.15793" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.SD, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models (DMs) have been increasingly used in audio
processing tasks, including speech super-resolution (SR), which aims to restore
high-frequency content given low-resolution speech utterances. This is commonly
achieved by conditioning the network of noise predictor with low-resolution
audio. In this paper, we propose a novel sampling algorithm that communicates
the information of the low-resolution audio via the reverse sampling process of
DMs. The proposed method can be a drop-in replacement for the vanilla sampling
process and can significantly improve the performance of the existing works.
Moreover, by coupling the proposed sampling method with an unconditional DM,
i.e., a DM with no auxiliary inputs to its noise predictor, we can generalize
it to a wide range of SR setups. We also attain state-of-the-art results on the
VCTK Multi-Speaker benchmark with this novel formulation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LAD: Language Augmented Diffusion for Reinforcement Learning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 27, 2022 </span>    
         <span class="authors"> Edwin Zhang, Yujie Lu, William Wang, Amy Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.15629" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CL
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Training generalist agents is difficult across several axes, requiring us to
deal with high-dimensional inputs (space), long horizons (time), and multiple
and new tasks. Recent advances with architectures have allowed for improved
scaling along one or two of these dimensions, but are still prohibitive
computationally. In this paper, we propose to address all three axes by
leveraging Language to Control Diffusion models as a hierarchical planner
conditioned on language (LCD). We effectively and efficiently scale diffusion
models for planning in extended temporal, state, and task dimensions to tackle
long horizon control problems conditioned on natural language instructions. We
compare LCD with other state-of-the-art models on the CALVIN language robotics
benchmark and find that LCD outperforms other SOTA methods in multi task
success rates while dramatically improving computational efficiency with a
single task success rate (SR) of 88.7% against the previous best of 82.6%. We
show that LCD can successfully leverage the unique strength of diffusion models
to produce coherent long range plans while addressing their weakness at
generating low-level details and control. We release our code and models at
https://github.com/ezhang7423/language-control-diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Solving Audio Inverse Problems with a Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 27, 2022 </span>    
         <span class="authors"> Eloi Moliner, Jaakko Lehtinen, Vesa Välimäki </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.15228" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper presents CQT-Diff, a data-driven generative audio model that can,
once trained, be used for solving various different audio inverse problems in a
problem-agnostic setting. CQT-Diff is a neural diffusion model with an
architecture that is carefully constructed to exploit pitch-equivariant
symmetries in music. This is achieved by preconditioning the model with an
invertible Constant-Q Transform (CQT), whose logarithmically-spaced frequency
axis represents pitch equivariance as translation equivariance. The proposed
method is evaluated with objective and subjective metrics in three different
and varied tasks: audio bandwidth extension, inpainting, and declipping. The
results show that CQT-Diff outperforms the compared baselines and ablations in
audio bandwidth extension and, without retraining, delivers competitive
performance against modern baselines in audio inpainting and declipping. This
work represents the first diffusion-based general framework for solving inverse
problems in audio processing.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling for Medical Image Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 27, 2022 </span>    
         <span class="authors"> Xutao Guo, Yanwu Yang, Chenfei Ye, Shang Lu, Yang Xiang, Ting Ma </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.17408" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Based on the Denoising Diffusion Probabilistic Model (DDPM), medical image
segmentation can be described as a conditional image generation task, which
allows to compute pixel-wise uncertainty maps of the segmentation and allows an
implicit ensemble of segmentations to boost the segmentation performance.
However, DDPM requires many iterative denoising steps to generate segmentations
from Gaussian noise, resulting in extremely inefficient inference. To mitigate
the issue, we propose a principled acceleration strategy, called
pre-segmentation diffusion sampling DDPM (PD-DDPM), which is specially used for
medical image segmentation. The key idea is to obtain pre-segmentation results
based on a separately trained segmentation network, and construct noise
predictions (non-Gaussian distribution) according to the forward diffusion
rule. We can then start with noisy predictions and use fewer reverse steps to
generate segmentation results. Experiments show that PD-DDPM yields better
segmentation results over representative baseline methods even if the number of
reverse steps is significantly reduced. Moreover, PD-DDPM is orthogonal to
existing advanced segmentation models, which can be combined to further improve
the segmentation performance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 26, 2022 </span>    
         <span class="authors"> Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, Duen Horng Chau </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.14896" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.HC, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 With recent advancements in diffusion models, users can generate high-quality
images by writing text prompts in natural language. However, generating images
with desired details requires proper prompts, and it is often unclear how a
model reacts to different prompts or what the best prompts are. To help
researchers tackle these critical challenges, we introduce DiffusionDB, the
first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14
million images generated by Stable Diffusion, 1.8 million unique prompts, and
hyperparameters specified by real users. We analyze the syntactic and semantic
characteristics of prompts. We pinpoint specific hyperparameter values and
prompt styles that can lead to model errors and present evidence of potentially
harmful model usage, such as the generation of misinformation. The
unprecedented scale and diversity of this human-actuated dataset provide
exciting research opportunities in understanding the interplay between prompts
and generative models, detecting deepfakes, and designing human-AI interaction
tools to help users more easily use these models. DiffusionDB is publicly
available at: https://poloclub.github.io/diffusiondb.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Categorical SDEs with Simplex Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 26, 2022 </span>    
         <span class="authors"> Pierre H. Richemond, Sander Dieleman, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.14784" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models typically operate in the standard framework of generative
modelling by producing continuously-valued datapoints. To this end, they rely
on a progressive Gaussian smoothing of the original data distribution, which
admits an SDE interpretation involving increments of a standard Brownian
motion. However, some applications such as text generation or reinforcement
learning might naturally be better served by diffusing categorical-valued data,
i.e., lifting the diffusion to a space of probability distributions. To this
end, this short theoretical note proposes Simplex Diffusion, a means to
directly diffuse datapoints located on an n-dimensional probability simplex. We
show how this relates to the Dirichlet distribution on the simplex and how the
analogous SDE is realized thanks to a multi-dimensional Cox-Ingersoll-Ross
process (abbreviated as CIR), previously used in economics and mathematical
finance. Finally, we make remarks as to the numerical implementation of
trajectories of the CIR process, and discuss some limitations of our approach.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Full-band General Audio Synthesis with Score-based Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 26, 2022 </span>    
         <span class="authors"> Santiago Pascual, Gautam Bhattacharya, Chunghsin Yeh, Jordi Pons, Joan Serrà </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.14661" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent works have shown the capability of deep generative models to tackle
general audio synthesis from a single label, producing a variety of impulsive,
tonal, and environmental sounds. Such models operate on band-limited signals
and, as a result of an autoregressive approach, they are typically conformed by
pre-trained latent encoders and/or several cascaded modules. In this work, we
propose a diffusion-based generative model for general audio synthesis, named
DAG, which deals with full-band signals end-to-end in the waveform domain.
Results show the superiority of DAG over existing label-conditioned generators
in terms of both quality and diversity. More specifically, when compared to the
state of the art, the band-limited and full-band versions of DAG achieve
relative improvements that go up to 40 and 65%, respectively. We believe DAG is
flexible enough to accommodate different conditioning schemas while providing
good quality synthesis.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Towards the Detection of Diffusion Model Deepfakes
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 26, 2022 </span>    
         <span class="authors"> Jonas Ricker, Simon Damm, Thorsten Holz, Asja Fischer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.14571" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models (DMs) have recently emerged as a promising method in image
synthesis. However, to date, only little attention has been paid to the
detection of DM-generated images, which is critical to prevent adverse impacts
on our society. In this work, we address this pressing challenge from two
different angles: First, we evaluate the performance of state-of-the-art
detectors, which are very effective against images generated by generative
adversarial networks (GANs), on a variety of DMs. Second, we analyze
DM-generated images in the frequency domain and study different factors that
influence the spectral properties of these images. Most importantly, we
demonstrate that GANs and DMs produce images with different characteristics,
which requires adaptation of existing classifiers to ensure reliable detection.
We are convinced that this work provides the foundation and starting point for
further research on effective detection of DM-generated images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On the failure of variational score matching for VAE models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 24, 2022 </span>    
         <span class="authors"> Li Kevin Wenliang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.13390" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score matching (SM) is a convenient method for training flexible
probabilistic models, which is often preferred over the traditional
maximum-likelihood (ML) approach. However, these models are less interpretable
than normalized models; as such, training robustness is in general difficult to
assess. We present a critical study of existing variational SM objectives,
showing catastrophic failure on a wide range of datasets and network
architectures. Our theoretical insights on the objectives emerge directly from
their equivalent autoencoding losses when optimizing variational autoencoder
(VAE) models. First, we show that in the Fisher autoencoder, SM produces far
worse models than maximum-likelihood, and approximate inference by Fisher
divergence can lead to low-density local optima. However, with important
modifications, this objective reduces to a regularized autoencoding loss that
resembles the evidence lower bound (ELBO). This analysis predicts that the
modified SM algorithm should behave very similarly to ELBO on Gaussian VAEs. We
then review two other FD-based objectives from the literature and show that
they reduce to uninterpretable autoencoding losses, likely leading to poor
performance. The experiments verify our theoretical predictions and suggest
that only ELBO and the baseline objective robustly produce expected results,
while previously proposed SM methods do not.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Structure-based Drug Design with Equivariant Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 24, 2022 </span>    
         <span class="authors"> Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lió, Carla Gomes, Max Welling, Michael Bronstein, Bruno Correia </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.13695" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Structure-based drug design (SBDD) aims to design small-molecule ligands that
bind with high affinity and specificity to pre-determined protein targets.
Traditional SBDD pipelines start with large-scale docking of compound libraries
from public databases, thus limiting the exploration of chemical space to
existent previously studied regions. Recent machine learning methods approached
this problem using an atom-by-atom generation approach, which is
computationally expensive. In this paper, we formulate SBDD as a 3D-conditional
generation problem and present DiffSBDD, an E(3)-equivariant 3D-conditional
diffusion model that generates novel ligands conditioned on protein pockets.
Furthermore, we curate a new dataset of experimentally determined binding
complex data from Binding MOAD to provide a realistic binding scenario that
complements the synthetic CrossDocked dataset. Comprehensive in silico
experiments demonstrate the efficiency of DiffSBDD in generating novel and
diverse drug-like ligands that engage protein pockets with high binding
energies as predicted by in silico docking.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MARS: Meta-Learning as Score Matching in the Function Space
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 24, 2022 </span>    
         <span class="authors"> Krunoslav Lehman Pavasovic, Jonas Rothfuss, Andreas Krause </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.13319" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Meta-learning aims to extract useful inductive biases from a set of related
datasets. In Bayesian meta-learning, this is typically achieved by constructing
a prior distribution over neural network parameters. However, specifying
families of computationally viable prior distributions over the
high-dimensional neural network parameters is difficult. As a result, existing
approaches resort to meta-learning restrictive diagonal Gaussian priors,
severely limiting their expressiveness and performance. To circumvent these
issues, we approach meta-learning through the lens of functional Bayesian
neural network inference, which views the prior as a stochastic process and
performs inference in the function space. Specifically, we view the
meta-training tasks as samples from the data-generating process and formalize
meta-learning as empirically estimating the law of this stochastic process. Our
approach can seamlessly acquire and represent complex prior knowledge by
meta-learning the score function of the data-generating process marginals
instead of parameter space priors. In a comprehensive benchmark, we demonstrate
that our method achieves state-of-the-art performance in terms of predictive
accuracy and substantial improvements in the quality of uncertainty estimates.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## High-Resolution Image Editing via Multi-Stage Blended Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 24, 2022 </span>    
         <span class="authors"> Johannes Ackermann, Minjun Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.12965" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown great results in image generation and in image
editing. However, current approaches are limited to low resolutions due to the
computational cost of training diffusion models for high-resolution generation.
We propose an approach that uses a pre-trained low-resolution diffusion model
to edit images in the megapixel range. We first use Blended Diffusion to edit
the image at a low resolution, and then upscale it in multiple stages, using a
super-resolution model and Blended Diffusion. Using our approach, we achieve
higher visual fidelity than by only applying off the shelf super-resolution
methods to the output of the diffusion model. We also obtain better global
consistency than directly using the diffusion model at a higher resolution.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Deep Equilibrium Approaches to Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 23, 2022 </span>    
         <span class="authors"> Ashwini Pokle, Zhengyang Geng, Zico Kolter </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.12867" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based generative models are extremely effective in generating
high-quality images, with generated samples often surpassing the quality of
those produced by other models under several metrics. One distinguishing
feature of these models, however, is that they typically require long sampling
chains to produce high-fidelity images. This presents a challenge not only from
the lenses of sampling time, but also from the inherent difficulty in
backpropagating through these chains in order to accomplish tasks such as model
inversion, i.e. approximately finding latent states that generate known images.
In this paper, we look at diffusion models through a different perspective,
that of a (deep) equilibrium (DEQ) fixed point model. Specifically, we extend
the recent denoising diffusion implicit model (DDIM; Song et al. 2020), and
model the entire sampling chain as a joint, multivariate fixed point system.
This setup provides an elegant unification of diffusion and equilibrium models,
and shows benefits in 1) single image sampling, as it replaces the fully-serial
typical sampling process with a parallel one; and 2) model inversion, where we
can leverage fast gradients in the DEQ setting to much more quickly find the
noise that generates a given image. The approach is also orthogonal and thus
complementary to other methods used to reduce the sampling time, or improve
model inversion. We demonstrate our method's strong performance across several
datasets, including CIFAR10, CelebA, and LSUN Bedrooms and Churches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 22, 2022 </span>    
         <span class="authors"> Zhiyuan Ren, Zhihong Pan, Xin Zhou, Le Kang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.12315" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a simple and novel method for generating 3D human motion from
complex natural language sentences, which describe different velocity,
direction and composition of all kinds of actions. Different from existing
methods that use classical generative architecture, we apply the Denoising
Diffusion Probabilistic Model to this task, synthesizing diverse motion results
under the guidance of texts. The diffusion model converts white noise into
structured 3D motion by a Markov process with a series of denoising steps and
is efficiently trained by optimizing a variational lower bound. To achieve the
goal of text-conditioned image synthesis, we use the classifier-free guidance
strategy to fuse text embedding into the model during training. Our experiments
demonstrate that our model achieves competitive results on HumanML3D test set
quantitatively and can generate more visually natural and diverse examples. We
also show with experiments that our model is capable of zero-shot generation of
motions for unseen text guidance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 21, 2022 </span>    
         <span class="authors"> Vikram Voleti, Christopher Pal, Adam Oberman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.12254" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative models based on denoising diffusion techniques have led to an
unprecedented increase in the quality and diversity of imagery that is now
possible to create with neural generative models. However, most contemporary
state-of-the-art methods are derived from a standard isotropic Gaussian
formulation. In this work we examine the situation where non-isotropic Gaussian
distributions are used. We present the key mathematical derivations for
creating denoising diffusion models using an underlying non-isotropic Gaussian
noise model. We also provide initial experiments with the CIFAR-10 dataset to
help verify empirically that this more general modeling approach can also yield
high-quality samples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Diffusion with Less Explicit Guidance via Model Predictive Control
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 21, 2022 </span>    
         <span class="authors"> Max W. Shen, Ehsan Hajiramezanali, Gabriele Scalia, Alex Tseng, Nathaniel Diamant, Tommaso Biancalani, Andreas Loukas </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.12192" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 How much explicit guidance is necessary for conditional diffusion? We
consider the problem of conditional sampling using an unconditional diffusion
model and limited explicit guidance (e.g., a noised classifier, or a
conditional diffusion model) that is restricted to a small number of time
steps. We explore a model predictive control (MPC)-like approach to approximate
guidance by simulating unconditional diffusion forward, and backpropagating
explicit guidance feedback. MPC-approximated guides have high cosine similarity
to real guides, even over large simulation distances. Adding MPC steps improves
generative quality when explicit guidance is limited to five time steps.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 21, 2022 </span>    
         <span class="authors"> Pouria Rouzrokh, Bardia Khosravi, Shahriar Faghani, Mana Moassefi, Sanaz Vahdati, Bradley J. Erickson </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.12113" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite the ever-increasing interest in applying deep learning (DL) models to
medical imaging, the typical scarcity and imbalance of medical datasets can
severely impact the performance of DL models. The generation of synthetic data
that might be freely shared without compromising patient privacy is a
well-known technique for addressing these difficulties. Inpainting algorithms
are a subset of DL generative models that can alter one or more regions of an
input image while matching its surrounding context and, in certain cases,
non-imaging input conditions. Although the majority of inpainting techniques
for medical imaging data use generative adversarial networks (GANs), the
performance of these algorithms is frequently suboptimal due to their limited
output variety, a problem that is already well-known for GANs. Denoising
diffusion probabilistic models (DDPMs) are a recently introduced family of
generative networks that can generate results of comparable quality to GANs,
but with diverse outputs. In this paper, we describe a DDPM to execute multiple
inpainting tasks on 2D axial slices of brain MRI with various sequences, and
present proof-of-concept examples of its performance in a variety of evaluation
scenarios. Our model and a public online interface to try our tool are
available at: https://github.com/Mayo-Radiology-Informatics-Lab/MBTI
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Boomerang: Local sampling on image manifolds using diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 21, 2022 </span>    
         <span class="authors"> Lorenzo Luzi, Ali Siahkoohi, Paul M Mayer, Josue Casco-Rodriguez, Richard Baraniuk </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.12100" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models can be viewed as mapping points in a high-dimensional latent
space onto a low-dimensional learned manifold, typically an image manifold. The
intermediate values between the latent space and image manifold can be
interpreted as noisy images which are determined by the noise scheduling scheme
employed during pre-training. We exploit this interpretation to introduce
Boomerang, a local image manifold sampling approach using the dynamics of
diffusion models. We call it Boomerang because we first add noise to an input
image, moving it closer to the latent space, then bring it back to the image
space through diffusion dynamics. We use this method to generate images which
are similar, but nonidentical, to the original input images on the image
manifold. We are able to set how close the generated image is to the original
based on how much noise we add. Additionally, the generated images have a
degree of stochasticity, allowing us to locally sample as many times as we want
without repetition. We show three applications for which Boomerang can be used.
First, we provide a framework for constructing privacy-preserving datasets
having controllable degrees of anonymity. Second, we show how to use Boomerang
for data augmentation while staying on the image manifold. Third, we introduce
a framework for image super-resolution with 8x upsampling. Boomerang does not
require any modification to the training of diffusion models and can be used
with pretrained models on a single, inexpensive GPU.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Visual Counterfactual Explanations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 21, 2022 </span>    
         <span class="authors"> Maximilian Augustin, Valentyn Boreiko, Francesco Croce, Matthias Hein </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.11841" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Visual Counterfactual Explanations (VCEs) are an important tool to understand
the decisions of an image classifier. They are 'small' but 'realistic' semantic
changes of the image changing the classifier decision. Current approaches for
the generation of VCEs are restricted to adversarially robust models and often
contain non-realistic artefacts, or are limited to image classification
problems with few classes. In this paper, we overcome this by generating
Diffusion Visual Counterfactual Explanations (DVCEs) for arbitrary ImageNet
classifiers via a diffusion process. Two modifications to the diffusion process
are key for our DVCEs: first, an adaptive parameterization, whose
hyperparameters generalize across images and models, together with distance
regularization and late start of the diffusion process, allow us to generate
images with minimal semantic changes to the original ones but different
classification. Second, our cone regularization via an adversarially robust
model ensures that the diffusion process does not converge to trivial
non-semantic changes, but instead produces realistic images of the target class
which achieve high confidence by the classifier.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Graphically Structured Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 20, 2022 </span>    
         <span class="authors"> Christian Weilbach, William Harvey, Frank Wood </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.11633" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.NE, cs.PL, G.3
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce a framework for automatically defining and learning deep
generative models with problem-specific structure. We tackle problem domains
that are more traditionally solved by algorithms such as sorting, constraint
satisfaction for Sudoku, and matrix factorization. Concretely, we train
diffusion models with an architecture tailored to the problem specification.
This problem specification should contain a graphical model describing
relationships between variables, and often benefits from explicit
representation of subcomputations. Permutation invariances can also be
exploited. Across a diverse set of experiments we improve the scaling
relationship between problem dimension and our model's performance, in terms of
both training time and final accuracy.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Representation Learning with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 20, 2022 </span>    
         <span class="authors"> Jeremias Traub </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.11058" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models (DMs) have achieved state-of-the-art results for image
synthesis tasks as well as density estimation. Applied in the latent space of a
powerful pretrained autoencoder (LDM), their immense computational requirements
can be significantly reduced without sacrificing sampling quality. However, DMs
and LDMs lack a semantically meaningful representation space as the diffusion
process gradually destroys information in the latent variables. We introduce a
framework for learning such representations with diffusion models (LRDM). To
that end, a LDM is conditioned on the representation extracted from the clean
image by a separate encoder. In particular, the DM and the representation
encoder are trained jointly in order to learn rich representations specific to
the generative denoising process. By introducing a tractable representation
prior, we can efficiently sample from the representation distribution for
unconditional image synthesis without training of any additional model. We
demonstrate that i) competitive image generation results can be achieved with
image-parameterized LDMs, ii) LRDMs are capable of learning semantically
meaningful representations, allowing for faithful image reconstructions and
semantic interpolations. Our implementation is available at
https://github.com/jeremiastraub/diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Differentially Private Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 18, 2022 </span>    
         <span class="authors"> Tim Dockhorn, Tianshi Cao, Arash Vahdat, Karsten Kreis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.09929" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While modern machine learning models rely on increasingly large training
datasets, data is often limited in privacy-sensitive domains. Generative models
trained with differential privacy (DP) on sensitive data can sidestep this
challenge, providing access to synthetic data instead. However, training DP
generative models is highly challenging due to the noise injected into training
to enforce DP. We propose to leverage diffusion models (DMs), an emerging class
of deep generative models, and introduce Differentially Private Diffusion
Models (DPDMs), which enforce privacy using differentially private stochastic
gradient descent (DP-SGD). We motivate why DP-SGD is well suited for training
DPDMs, and thoroughly investigate the DM parameterization and the sampling
algorithm, which turn out to be crucial ingredients in DPDMs. Furthermore, we
propose noise multiplicity, a simple yet powerful modification of the DM
training objective tailored to the DP setting to boost performance. We validate
our novel DPDMs on widely-used image generation benchmarks and achieve
state-of-the-art (SOTA) performance by large margins. For example, on MNIST we
improve the SOTA FID from 48.4 to 5.01 and downstream classification accuracy
from 83.2% to 98.1% for the privacy setting DP-$(\varepsilon{=}10,
\delta{=}10^{-5})$. Moreover, on standard benchmarks, classifiers trained on
DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained
classifiers, which has not been demonstrated before for DP generative models.
Project page and code: https://nv-tlabs.github.io/DPDM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improving Adversarial Robustness by Contrastive Guided Diffusion Process
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 18, 2022 </span>    
         <span class="authors"> Yidong Ouyang, Liyan Xie, Guang Cheng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.09643" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Synthetic data generation has become an emerging tool to help improve the
adversarial robustness in classification tasks since robust learning requires a
significantly larger amount of training samples compared with standard
classification tasks. Among various deep generative models, the diffusion model
has been shown to produce high-quality synthetic images and has achieved good
performance in improving the adversarial robustness. However, diffusion-type
methods are typically slow in data generation as compared with other generative
models. Although different acceleration techniques have been proposed recently,
it is also of great importance to study how to improve the sample efficiency of
generated data for the downstream task. In this paper, we first analyze the
optimality condition of synthetic distribution for achieving non-trivial robust
accuracy. We show that enhancing the distinguishability among the generated
data is critical for improving adversarial robustness. Thus, we propose the
Contrastive-Guided Diffusion Process (Contrastive-DP), which adopts the
contrastive loss to guide the diffusion model in data generation. We verify our
theoretical results using simulations and demonstrate the good performance of
Contrastive-DP on image datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 18, 2022 </span>    
         <span class="authors"> Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang, Quan Bai </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.09549" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, 94A08, I.4.0
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models have been proven to perform remarkably well in
text-to-image synthesis tasks in a number of studies, immediately presenting
new study opportunities for image generation. Google's Imagen follows this
research trend and outperforms DALLE2 as the best model for text-to-image
generation. However, Imagen merely uses a T5 language model for text
processing, which cannot ensure learning the semantic information of the text.
Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in
image processing. To address these issues, we propose the Swinv2-Imagen, a
novel text-to-image diffusion model based on a Hierarchical Visual Transformer
and a Scene Graph incorporating a semantic layout. In the proposed model, the
feature vectors of entities and relationships are extracted and involved in the
diffusion model, effectively improving the quality of generated images. On top
of that, we also introduce a Swin-Transformer-based UNet architecture, called
Swinv2-Unet, which can address the problems stemming from the CNN convolution
operations. Extensive experiments are conducted to evaluate the performance of
the proposed model by using three real-world datasets, i.e., MSCOCO, CUB and
MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen
model outperforms several popular state-of-the-art methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 17, 2022 </span>    
         <span class="authors"> Dani Valevski, Matan Kalman, Yossi Matias, Yaniv Leviathan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.09477" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Imagic: Text-Based Real Image Editing with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 17, 2022 </span>    
         <span class="authors"> Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.09276" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-conditioned image editing has recently attracted considerable interest.
However, most methods are currently either limited to specific editing types
(e.g., object overlay, style transfer), or apply to synthetically generated
images, or require multiple input images of a common object. In this paper we
demonstrate, for the very first time, the ability to apply complex (e.g.,
non-rigid) text-guided semantic edits to a single real image. For example, we
can change the posture and composition of one or multiple objects inside an
image, while preserving its original characteristics. Our method can make a
standing dog sit down or jump, cause a bird to spread its wings, etc. -- each
within its single high-resolution natural image provided by the user. Contrary
to previous work, our proposed method requires only a single input image and a
target text (the desired edit). It operates on real images, and does not
require any additional inputs (such as image masks or additional views of the
object). Our method, which we call "Imagic", leverages a pre-trained
text-to-image diffusion model for this task. It produces a text embedding that
aligns with both the input image and the target text, while fine-tuning the
diffusion model to capture the image-specific appearance. We demonstrate the
quality and versatility of our method on numerous inputs from various domains,
showcasing a plethora of high quality complex semantic image edits, all within
a single unified framework.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 17, 2022 </span>    
         <span class="authors"> Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.08933" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models have emerged as a new paradigm for generative
models. Despite the success in domains using continuous signals such as vision
and audio, adapting diffusion models to natural language is under-explored due
to the discrete nature of texts, especially for conditional generation. We
tackle this challenge by proposing DiffuSeq: a diffusion model designed for
sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation
over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or
even better performance than six established baselines, including a
state-of-the-art model that is based on pre-trained language models. Apart from
quality, an intriguing property of DiffuSeq is its high diversity during
generation, which is desired in many Seq2Seq tasks. We further include a
theoretical analysis revealing the connection between DiffuSeq and
autoregressive/non-autoregressive models. Bringing together theoretical
analysis and empirical evidence, we demonstrate the great potential of
diffusion models in complex conditional language generation tasks. Code is
available at \url{https://github.com/Shark-NLP/DiffuSeq}
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 16, 2022 </span>    
         <span class="authors"> Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.08573" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent generative models show impressive results in photo-realistic image
generation. However, artifacts often inevitably appear in the generated
results, leading to downgraded user experience and reduced performance in
downstream tasks. This work aims to develop a plugin post-processing module for
diverse generative models, which can faithfully restore images from diverse
generative artifacts. This is challenging because: (1) Unlike traditional
degradation patterns, generative artifacts are non-linear and the
transformation function is highly complex. (2) There are no readily available
artifact-image pairs. (3) Different from model-specific anti-artifact methods,
a model-agnostic framework views the generator as a black-box machine and has
no access to the architecture details. In this work, we first design a group of
mechanisms to simulate generative artifacts of popular generators (i.e., GANs,
autoregressive models, and diffusion models), given real images. Second, we
implement the model-agnostic anti-artifact framework as an image-to-image
diffusion model, due to its advantage in generation quality and capacity.
Finally, we design a conditioning scheme for the diffusion model to enable both
blind and non-blind image restoration. A guidance parameter is also introduced
to allow for a trade-off between restoration accuracy and image quality.
Extensive experiments show that our method significantly outperforms previous
approaches on the proposed datasets and real-world artifact images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## TransFusion: Transcribing Speech with Multinomial Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 14, 2022 </span>    
         <span class="authors"> Matthew Baas, Kevin Eloff, Herman Kamper </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.07677" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.AI, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown exceptional scaling properties in the image
synthesis domain, and initial attempts have shown similar benefits for applying
diffusion to unconditional text synthesis. Denoising diffusion models attempt
to iteratively refine a sampled noise signal until it resembles a coherent
signal (such as an image or written sentence). In this work we aim to see
whether the benefits of diffusion models can also be realized for speech
recognition. To this end, we propose a new way to perform speech recognition
using a diffusion model conditioned on pretrained speech features.
Specifically, we propose TransFusion: a transcribing diffusion model which
iteratively denoises a random character sequence into coherent text
corresponding to the transcript of a conditioning utterance. We demonstrate
comparable performance to existing high-performing contrastive models on the
LibriSpeech speech recognition benchmark. To the best of our knowledge, we are
the first to apply denoising diffusion to speech recognition. We also propose
new techniques for effectively sampling and decoding multinomial diffusion
models. These are required because traditional methods of sampling from
acoustic models are not possible with our new discrete diffusion approach. Code
and trained models are available: https://github.com/RF5/transfusion-asr
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Hierarchical Diffusion Models for Singing Voice Neural Vocoder
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 14, 2022 </span>    
         <span class="authors"> Naoya Takahashi, Mayank Kumar, Singh, Yuki Mitsufuji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.07508" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent progress in deep generative models has improved the quality of neural
vocoders in speech domain. However, generating a high-quality singing voice
remains challenging due to a wider variety of musical expressions in pitch,
loudness, and pronunciations. In this work, we propose a hierarchical diffusion
model for singing voice neural vocoders. The proposed method consists of
multiple diffusion models operating in different sampling rates; the model at
the lowest sampling rate focuses on generating accurate low-frequency
components such as pitch, and other models progressively generate the waveform
at higher sampling rates on the basis of the data at the lower sampling rate
and acoustic features. Experimental results show that the proposed method
produces high-quality singing voices for multiple singers, outperforming
state-of-the-art neural vocoders with a similar range of computational costs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 13, 2022 </span>    
         <span class="authors"> Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.06998" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CR, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Text-to-image generation models that generate images based on prompt
descriptions have attracted an increasing amount of attention during the past
few months. Despite their encouraging performance, these models raise concerns
about the misuse of their generated fake images. To tackle this problem, we
pioneer a systematic study on the detection and attribution of fake images
generated by text-to-image generation models. Concretely, we first build a
machine learning classifier to detect the fake images generated by various
text-to-image generation models. We then attribute these fake images to their
source models, such that model owners can be held responsible for their models'
misuse. We further investigate how prompts that generate fake images affect
detection and attribution. We conduct extensive experiments on four popular
text-to-image generation models, including DALL$\cdot$E 2, Stable Diffusion,
GLIDE, and Latent Diffusion, and two benchmark prompt-image datasets. Empirical
results show that (1) fake images generated by various models can be
distinguished from real ones, as there exists a common artifact shared by fake
images from different models; (2) fake images can be effectively attributed to
their source models, as different models leave unique fingerprints in their
generated images; (3) prompts with the ``person'' topic or a length between 25
and 75 enable models to generate fake images with higher authenticity. All
findings contribute to the community's insight into the threats caused by
text-to-image generation models. We appeal to the community's consideration of
the counterpart solutions, like ours, against the rapidly-evolving fake image
generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Action Matching: A Variational Method for Learning Stochastic Dynamics from Samples
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 13, 2022 </span>    
         <span class="authors"> Kirill Neklyudov, Rob Brekelmans, Daniel Severo, Alireza Makhzani </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.06662" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Learning the continuous dynamics of a system from snapshots of its time
evolution is a problem which appears throughout natural sciences and machine
learning, including in quantum systems, single-cell biological data, and
generative modeling. In these settings, we assume that only uncorrelated
samples rather than full trajectory data are available. In order to better
understand the systems under observation, we would like to learn a model of the
underlying process that allows us to propagate samples in time and thereby
simulate entire individual trajectories. In this work, we propose Action
Matching, a method for learning a rich family of dynamics using only
independent samples from its time evolution. We derive a tractable training
objective, which does not rely on explicit assumptions about the underlying
dynamics and does not require back-propagation through differential equation or
optimal transport solvers. Inspired by connections with optimal transport, we
derive extensions of Action Matching to learn stochastic differential equations
and dynamics involving creation or destruction of probability mass. Finally, we
showcase applications of Action Matching by achieving competitive performance
in a diverse set of experiments from biology, physics, and generative modeling.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Self-Guided Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 12, 2022 </span>    
         <span class="authors"> Vincent Tao Hu, David W Zhang, Yuki M. Asano, Gertjan J. Burghouts, Cees G. M. Snoek </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.06462" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have demonstrated remarkable progress in image generation
quality, especially when guidance is used to control the generative process.
However, guidance requires a large amount of image-annotation pairs for
training and is thus dependent on their availability, correctness and
unbiasedness. In this paper, we eliminate the need for such annotation by
instead leveraging the flexibility of self-supervision signals to design a
framework for self-guided diffusion models. By leveraging a feature extraction
function and a self-annotation function, our method provides guidance signals
at various image granularities: from the level of holistic images to object
boxes and even segmentation masks. Our experiments on single-label and
multi-label image datasets demonstrate that self-labeled guidance always
outperforms diffusion models without guidance and may even surpass guidance
based on ground-truth labels, especially on unbalanced data. When equipped with
self-supervised box or mask proposals, our method further generates visually
diverse yet semantically consistent images, without the need for any class,
box, or segment label annotation. Self-guided diffusion is simple, flexible and
expected to profit from deployment at scale.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for Causal Discovery via Topological Ordering
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 12, 2022 </span>    
         <span class="authors"> Pedro Sanchez, Xiao Liu, Alison Q O'Neil, Sotirios A. Tsaftaris </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.06201" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Discovering causal relations from observational data becomes possible with
additional assumptions such as considering the functional relations to be
constrained as nonlinear with additive noise. In this case, the Hessian of the
data log-likelihood can be used for finding leaf nodes in a causal graph.
Topological ordering approaches for causal discovery exploit this by performing
graph discovery in two steps, first sequentially identifying nodes in reverse
order of depth (topological ordering), and secondly pruning the potential
relations. This is more efficient since the search is performed over a
permutation rather than a graph space. However, existing computational methods
for obtaining the Hessian still do not scale as the number of variables and the
number of samples are increased. Therefore, inspired by recent innovations in
diffusion probabilistic models (DPMs), we propose DiffAN, a topological
ordering algorithm that leverages DPMs. Further, we introduce theory for
updating the learned Hessian without re-training the neural network, and we
show that computing with a subset of samples gives an accurate approximation of
the ordering, which allows scaling to datasets with more samples and variables.
We show empirically that our method scales exceptionally well to datasets with
up to $500$ nodes and up to $10^5$ samples while still performing on par over
small datasets with state-of-the-art causal discovery methods. Implementation
is available at https://github.com/vios-s/DiffAN .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LION: Latent Point Diffusion Models for 3D Shape Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 12, 2022 </span>    
         <span class="authors"> Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.06978" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models (DDMs) have shown promising results in 3D point
cloud synthesis. To advance 3D DDMs and make them useful for digital artists,
we require (i) high generation quality, (ii) flexibility for manipulation and
applications such as conditional synthesis and shape interpolation, and (iii)
the ability to output smooth surfaces or meshes. To this end, we introduce the
hierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION
is set up as a variational autoencoder (VAE) with a hierarchical latent space
that combines a global shape latent representation with a point-structured
latent space. For generation, we train two hierarchical DDMs in these latent
spaces. The hierarchical VAE approach boosts performance compared to DDMs that
operate on point clouds directly, while the point-structured latents are still
ideally suited for DDM-based modeling. Experimentally, LION achieves
state-of-the-art generation performance on multiple ShapeNet benchmarks.
Furthermore, our VAE framework allows us to easily use LION for different
relevant tasks: LION excels at multimodal shape denoising and voxel-conditioned
synthesis, and it can be adapted for text- and image-driven 3D generation. We
also demonstrate shape autoencoding and latent shape interpolation, and we
augment LION with modern surface reconstruction techniques to generate smooth
3D meshes. We hope that LION provides a powerful tool for artists working with
3D shapes due to its high-quality generation, flexibility, and surface
reconstruction. Project page and code: https://nv-tlabs.github.io/LION.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 11, 2022 </span>    
         <span class="authors"> Chen Henry Wu, Fernando De la Torre </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.05559" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have achieved unprecedented performance in generative
modeling. The commonly-adopted formulation of the latent code of diffusion
models is a sequence of gradually denoised samples, as opposed to the simpler
(e.g., Gaussian) latent space of GANs, VAEs, and normalizing flows. This paper
provides an alternative, Gaussian formulation of the latent space of various
diffusion models, as well as an invertible DPM-Encoder that maps images into
the latent space. While our formulation is purely based on the definition of
diffusion models, we demonstrate several intriguing consequences. (1)
Empirically, we observe that a common latent space emerges from two diffusion
models trained independently on related domains. In light of this finding, we
propose CycleDiffusion, which uses DPM-Encoder for unpaired image-to-image
translation. Furthermore, applying CycleDiffusion to text-to-image diffusion
models, we show that large-scale text-to-image diffusion models can be used as
zero-shot image-to-image editors. (2) One can guide pre-trained diffusion
models and GANs by controlling the latent codes in a unified, plug-and-play
formulation based on energy-based models. Using the CLIP model and a face
recognition model as guidance, we demonstrate that diffusion models have better
coverage of low-density sub-populations and individuals than GANs. The code is
publicly available at https://github.com/ChenWu98/cycle-diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GENIE: Higher-Order Denoising Diffusion Solvers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 11, 2022 </span>    
         <span class="authors"> Tim Dockhorn, Arash Vahdat, Karsten Kreis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.05475" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models (DDMs) have emerged as a powerful class of
generative models. A forward diffusion process slowly perturbs the data, while
a deep model learns to gradually denoise. Synthesis amounts to solving a
differential equation (DE) defined by the learnt model. Solving the DE requires
slow iterative solvers for high-quality generation. In this work, we propose
Higher-Order Denoising Diffusion Solvers (GENIE): Based on truncated Taylor
methods, we derive a novel higher-order solver that significantly accelerates
synthesis. Our solver relies on higher-order gradients of the perturbed data
distribution, that is, higher-order score functions. In practice, only
Jacobian-vector products (JVPs) are required and we propose to extract them
from the first-order score network via automatic differentiation. We then
distill the JVPs into a separate neural network that allows us to efficiently
compute the necessary higher-order terms for our novel sampler during
synthesis. We only need to train a small additional head on top of the
first-order score network. We validate GENIE on multiple image generation
benchmarks and demonstrate that GENIE outperforms all previous solvers. Unlike
recent methods that fundamentally alter the generation process in DDMs, our
GENIE solves the true generative DE and still enables applications such as
encoding and guided sampling. Project page and code:
https://nv-tlabs.github.io/GENIE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 11, 2022 </span>    
         <span class="authors"> Ilia Igashov, Hannes Stärk, Clément Vignac, Victor Garcia Satorras, Pascal Frossard, Max Welling, Michael Bronstein, Bruno Correia </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.05274" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, q-bio.BM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Fragment-based drug discovery has been an effective paradigm in early-stage
drug development. An open challenge in this area is designing linkers between
disconnected molecular fragments of interest to obtain chemically-relevant
candidate drug molecules. In this work, we propose DiffLinker, an
E(3)-equivariant 3D-conditional diffusion model for molecular linker design.
Given a set of disconnected fragments, our model places missing atoms in
between and designs a molecule incorporating all the initial fragments. Unlike
previous approaches that are only able to connect pairs of molecular fragments,
our method can link an arbitrary number of fragments. Additionally, the model
automatically determines the number of atoms in the linker and its attachment
points to the input fragments. We demonstrate that DiffLinker outperforms other
methods on the standard datasets generating more diverse and
synthetically-accessible molecules. Besides, we experimentally test our method
in real-world applications, showing that it can successfully generate valid
linkers conditioned on target protein pockets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 11, 2022 </span>    
         <span class="authors"> Kin Wai Cheuk, Ryosuke Sawata, Toshimitsu Uesaka, Naoki Murata, Naoya Takahashi, Shusuke Takahashi, Dorien Herremans, Yuki Mitsufuji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.05148" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.AI, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper we propose a novel generative approach, DiffRoll, to tackle
automatic music transcription (AMT). Instead of treating AMT as a
discriminative task in which the model is trained to convert spectrograms into
piano rolls, we think of it as a conditional generative task where we train our
model to generate realistic looking piano rolls from pure Gaussian noise
conditioned on spectrograms. This new AMT formulation enables DiffRoll to
transcribe, generate and even inpaint music. Due to the classifier-free nature,
DiffRoll is also able to be trained on unpaired datasets where only piano rolls
are available. Our experiments show that DiffRoll outperforms its
discriminative counterpart by 19 percentage points (ppt.) and our ablation
studies also indicate that it outperforms similar existing methods by 4.8 ppt.
  Source code and demonstration are available https://sony.github.io/DiffRoll/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Markup-to-Image Diffusion Models with Scheduled Sampling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 11, 2022 </span>    
         <span class="authors"> Yuntian Deng, Noriyuki Kojima, Alexander M. Rush </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.05147" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CL, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Building on recent advances in image generation, we present a fully
data-driven approach to rendering markup into images. The approach is based on
diffusion models, which parameterize the distribution of data using a sequence
of denoising operations on top of a Gaussian noise distribution. We view the
diffusion denoising process as a sequential decision making process, and show
that it exhibits compounding errors similar to exposure bias issues in
imitation learning problems. To mitigate these issues, we adapt the scheduled
sampling algorithm to diffusion training. We conduct experiments on four markup
datasets: mathematical formulas (LaTeX), table layouts (HTML), sheet music
(LilyPond), and molecular images (SMILES). These experiments each verify the
effectiveness of the diffusion process and the use of scheduled sampling to fix
generation issues. These results also show that the markup-to-image task
presents a useful controlled compositional setting for diagnosing and analyzing
generative image models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 10, 2022 </span>    
         <span class="authors"> Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, Josh Susskind </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.04955" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models (DMs) have recently emerged as SoTA tools for generative
modeling in various domains. Standard DMs can be viewed as an instantiation of
hierarchical variational autoencoders (VAEs) where the latent variables are
inferred from input-centered Gaussian distributions with fixed scales and
variances. Unlike VAEs, this formulation limits DMs from changing the latent
spaces and learning abstract representations. In this work, we propose f-DM, a
generalized family of DMs which allows progressive signal transformation. More
precisely, we extend DMs to incorporate a set of (hand-designed or learned)
transformations, where the transformed input is the mean of each diffusion
step. We propose a generalized formulation and derive the corresponding
de-noising objective with a modified sampling algorithm. As a demonstration, we
apply f-DM in image generation tasks with a range of functions, including
down-sampling, blurring, and learned transformations based on the encoder of
pretrained VAEs. In addition, we identify the importance of adjusting the noise
levels whenever the signal is sub-sampled and propose a simple rescaling
recipe. f-DM can produce high-quality samples on standard image generation
benchmarks like FFHQ, AFHQ, LSUN, and ImageNet with better efficiency and
semantic interpretation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## What the DAAM: Interpreting Stable Diffusion Using Cross Attention
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 10, 2022 </span>    
         <span class="authors"> Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, Ferhan Ture </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.04885" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CL
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large-scale diffusion neural networks represent a substantial milestone in
text-to-image generation, but they remain poorly understood, lacking
interpretability analyses. In this paper, we perform a text-image attribution
analysis on Stable Diffusion, a recently open-sourced model. To produce
pixel-level attribution maps, we upscale and aggregate cross-attention
word-pixel scores in the denoising subnetwork, naming our method DAAM. We
evaluate its correctness by testing its semantic segmentation ability on nouns,
as well as its generalized attribution quality on all parts of speech, rated by
humans. We then apply DAAM to study the role of syntax in the pixel space,
characterizing head--dependent heat map interaction patterns for ten common
dependency relations. Finally, we study several semantic phenomena using DAAM,
with a focus on feature entanglement, where we find that cohyponyms worsen
generation quality and descriptive adjectives attend too broadly. To our
knowledge, we are the first to interpret large diffusion models from a
visuolinguistic perspective, which enables future lines of research. Our code
is at https://github.com/castorini/daam.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 10, 2022 </span>    
         <span class="authors"> Louis Sharrock, Jack Simons, Song Liu, Mark Beaumont </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.04872" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce Sequential Neural Posterior Score Estimation (SNPSE) and
Sequential Neural Likelihood Score Estimation (SNLSE), two new score-based
methods for Bayesian inference in simulator-based models. Our methods, inspired
by the success of score-based methods in generative modelling, leverage
conditional score-based diffusion models to generate samples from the posterior
distribution of interest. These models can be trained using one of two possible
objective functions, one of which approximates the score of the intractable
likelihood, while the other directly estimates the score of the posterior. We
embed these models into a sequential training procedure, which guides
simulations using the current approximation of the posterior at the observation
of interest, thereby reducing the simulation cost. We validate our methods, as
well as their amortised, non-sequential variants, on several numerical
examples, demonstrating comparable or superior performance to existing
state-of-the-art methods such as Sequential Neural Posterior Estimation (SNPE)
and Sequential Neural Likelihood Estimation (SNLE).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 10, 2022 </span>    
         <span class="authors"> Shitong Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.04559" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image captioning task has been extensively researched by previous work.
However, limited experiments focus on generating captions based on
non-autoregressive text decoder. Inspired by the recent success of the
denoising diffusion model on image synthesis tasks, we apply denoising
diffusion probabilistic models to text generation in image captioning tasks. We
show that our CLIP-Diffusion-LM is capable of generating image captions using
significantly fewer inference steps than autoregressive models. On the Flickr8k
dataset, the model achieves 0.1876 BLEU-4 score. By training on the combined
Flickr8k and Flickr30k dataset, our model achieves 0.2470 BLEU-4 score. Our
code is available at https://github.com/xu-shitong/diffusion-image-captioning.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Regularizing Score-based Models with Score Fokker-Planck Equations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 09, 2022 </span>    
         <span class="authors"> Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.04296" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models learn a family of noise-conditional score
functions corresponding to the data density perturbed with increasingly large
amounts of noise. These perturbed data densities are tied together by the
Fokker-Planck equation (FPE), a partial differential equation (PDE) governing
the spatial-temporal evolution of a density undergoing a diffusion process. In
this work, we derive a corresponding equation, called the score FPE that
characterizes the noise-conditional scores of the perturbed data densities
(i.e., their gradients). Surprisingly, despite impressive empirical
performance, we observe that scores learned via denoising score matching (DSM)
do not satisfy the underlying score FPE. We prove that satisfying the FPE is
desirable as it improves the likelihood and the degree of conservativity.
Hence, we propose to regularize the DSM objective to enforce satisfaction of
the score FPE, and we show the effectiveness of this approach across various
datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## STaSy: Score-based Tabular data Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 08, 2022 </span>    
         <span class="authors"> Jayoung Kim, Chaejeong Lee, Noseong Park </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.04018" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.DB
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Tabular data synthesis is a long-standing research topic in machine learning.
Many different methods have been proposed over the past decades, ranging from
statistical methods to deep generative methods. However, it has not always been
successful due to the complicated nature of real-world tabular data. In this
paper, we present a new model named Score-based Tabular data Synthesis (STaSy)
and its training strategy based on the paradigm of score-based generative
modeling. Despite the fact that score-based generative models have resolved
many issues in generative models, there still exists room for improvement in
tabular data synthesis. Our proposed training strategy includes a self-paced
learning technique and a fine-tuning strategy, which further increases the
sampling quality and diversity by stabilizing the denoising score matching
training. Furthermore, we also conduct rigorous experimental studies in terms
of the generative task trilemma: sampling quality, diversity, and time. In our
experiments with 15 benchmark tabular datasets and 7 baselines, our method
outperforms existing methods in terms of task-dependant evaluations and
diversity.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Efficient Diffusion Models for Vision: A Survey
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 07, 2022 </span>    
         <span class="authors"> Anwaar Ulhaq, Naveed Akhtar, Ganna Pogrebna </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.09292" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Models (DMs) have demonstrated state-of-the-art performance in
content generation without requiring adversarial training. These models are
trained using a two-step process. First, a forward - diffusion - process
gradually adds noise to a datum (usually an image). Then, a backward - reverse
diffusion - process gradually removes the noise to turn it into a sample of the
target distribution being modelled. DMs are inspired by non-equilibrium
thermodynamics and have inherent high computational complexity. Due to the
frequent function evaluations and gradient calculations in high-dimensional
spaces, these models incur considerable computational overhead during both
training and inference stages. This can not only preclude the democratization
of diffusion-based modelling, but also hinder the adaption of diffusion models
in real-life applications. Not to mention, the efficiency of computational
models is fast becoming a significant concern due to excessive energy
consumption and environmental scares. These factors have led to multiple
contributions in the literature that focus on devising computationally
efficient DMs. In this review, we present the most recent advances in diffusion
models for vision, specifically focusing on the important design aspects that
affect the computational efficiency of DMs. In particular, we emphasize the
recently proposed design choices that have led to more efficient DMs. Unlike
the other recent reviews, which discuss diffusion models from a broad
perspective, this survey is aimed at pushing this research direction forward by
highlighting the design strategies in the literature that are resulting in
practicable models for the broader research community. We also provide a future
outlook of diffusion models in vision from their computational efficiency
viewpoint.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On Distillation of Guided Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 06, 2022 </span>    
         <span class="authors"> Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, Tim Salimans </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.03142" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Classifier-free guided diffusion models have recently been shown to be highly
effective at high-resolution image generation, and they have been widely used
in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and
Imagen. However, a downside of classifier-free guided diffusion models is that
they are computationally expensive at inference time since they require
evaluating two diffusion models, a class-conditional model and an unconditional
model, tens to hundreds of times. To deal with this limitation, we propose an
approach to distilling classifier-free guided diffusion models into models that
are fast to sample from: Given a pre-trained classifier-free guided model, we
first learn a single model to match the output of the combined conditional and
unconditional models, and then we progressively distill that model to a
diffusion model that requires much fewer sampling steps. For standard diffusion
models trained on the pixel-space, our approach is able to generate images
visually comparable to that of the original model using as few as 4 sampling
steps on ImageNet 64x64 and CIFAR-10, achieving FID/IS scores comparable to
that of the original model while being up to 256 times faster to sample from.
For diffusion models trained on the latent-space (e.g., Stable Diffusion), our
approach is able to generate high-fidelity images using as few as 1 to 4
denoising steps, accelerating inference by at least 10-fold compared to
existing methods on ImageNet 256x256 and LAION datasets. We further demonstrate
the effectiveness of our approach on text-guided image editing and inpainting,
where our distilled model is able to generate high-quality results using as few
as 2-4 denoising steps.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Novel View Synthesis with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 06, 2022 </span>    
         <span class="authors"> Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.04628" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present 3DiM, a diffusion model for 3D novel view synthesis, which is able
to translate a single input view into consistent and sharp completions across
many views. The core component of 3DiM is a pose-conditional image-to-image
diffusion model, which takes a source view and its pose as inputs, and
generates a novel view for a target pose as output. 3DiM can generate multiple
views that are 3D consistent using a novel technique called stochastic
conditioning. The output views are generated autoregressively, and during the
generation of each novel view, one selects a random conditioning view from the
set of available views at each denoising step. We demonstrate that stochastic
conditioning significantly improves the 3D consistency of a naive sampler for
an image-to-image diffusion model, which involves conditioning on a single
fixed view. We compare 3DiM to prior work on the SRN ShapeNet dataset,
demonstrating that 3DiM's generated completions from a single view achieve much
higher fidelity, while being approximately 3D consistent. We also introduce a
new evaluation methodology, 3D consistency scoring, to measure the 3D
consistency of a generated object by training a neural field on the model's
output views. 3DiM is geometry free, does not rely on hyper-networks or
test-time optimization for novel view synthesis, and allows a single model to
easily scale to a large number of scenes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Flow Matching for Generative Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 06, 2022 </span>    
         <span class="authors"> Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.02747" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce a new paradigm for generative modeling built on Continuous
Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale.
Specifically, we present the notion of Flow Matching (FM), a simulation-free
approach for training CNFs based on regressing vector fields of fixed
conditional probability paths. Flow Matching is compatible with a general
family of Gaussian probability paths for transforming between noise and data
samples -- which subsumes existing diffusion paths as specific instances.
Interestingly, we find that employing FM with diffusion paths results in a more
robust and stable alternative for training diffusion models. Furthermore, Flow
Matching opens the door to training CNFs with other, non-diffusion probability
paths. An instance of particular interest is using Optimal Transport (OT)
displacement interpolation to define the conditional probability paths. These
paths are more efficient than diffusion paths, provide faster training and
sampling, and result in better generalization. Training CNFs using Flow
Matching on ImageNet leads to consistently better performance than alternative
diffusion-based methods in terms of both likelihood and sample quality, and
allows fast and reliable sample generation using off-the-shelf numerical ODE
solvers.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 05, 2022 </span>    
         <span class="authors"> Ivan Kapelyukh, Vitalis Vosylius, Edward Johns </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.02438" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.RO, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce the first work to explore web-scale diffusion models for
robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first
inferring a text description of those objects, then generating an image
representing a natural, human-like arrangement of those objects, and finally
physically arranging the objects according to that goal image. We show that
this is possible zero-shot using DALL-E, without needing any further example
arrangements, data collection, or training. DALL-E-Bot is fully autonomous and
is not restricted to a pre-defined set of objects or scenes, thanks to DALL-E's
web-scale pre-training. Encouraging real-world results, with both human studies
and objective metrics, show that integrating web-scale diffusion models into
robotics pipelines is a promising direction for scalable, unsupervised robot
learning.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## clip2latent: Text driven sampling of a pre-trained StyleGAN using denoising diffusion and CLIP
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 05, 2022 </span>    
         <span class="authors"> Justin N. M. Pinkney, Chuan Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.02347" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce a new method to efficiently create text-to-image models from a
pre-trained CLIP and StyleGAN. It enables text driven sampling with an existing
generative model without any external data or fine-tuning. This is achieved by
training a diffusion model conditioned on CLIP embeddings to sample latent
vectors of a pre-trained StyleGAN, which we call clip2latent. We leverage the
alignment between CLIP's image and text embeddings to avoid the need for any
text labelled data for training the conditional diffusion model. We demonstrate
that clip2latent allows us to generate high-resolution (1024x1024 pixels)
images based on text prompts with fast sampling, high image quality, and low
training compute and data requirements. We also show that the use of the well
studied StyleGAN architecture, without further fine-tuning, allows us to
directly apply existing methods to control and modify the generated images
adding a further layer of control to our text-to-image pipeline.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Imagen Video: High Definition Video Generation with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 05, 2022 </span>    
         <span class="authors"> Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.02303" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Imagen Video, a text-conditional video generation system based on
a cascade of video diffusion models. Given a text prompt, Imagen Video
generates high definition videos using a base video generation model and a
sequence of interleaved spatial and temporal video super-resolution models. We
describe how we scale up the system as a high definition text-to-video model
including design decisions such as the choice of fully-convolutional temporal
and spatial super-resolution models at certain resolutions, and the choice of
the v-parameterization of diffusion models. In addition, we confirm and
transfer findings from previous work on diffusion-based image generation to the
video generation setting. Finally, we apply progressive distillation to our
video models with classifier-free guidance for fast, high quality sampling. We
find Imagen Video not only capable of generating videos of high fidelity, but
also having a high degree of controllability and world knowledge, including the
ability to generate diverse videos and text animations in various artistic
styles and with 3D object understanding. See
https://imagen.research.google/video/ for samples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Progressive Denoising Model for Fine-Grained Text-to-Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 05, 2022 </span>    
         <span class="authors"> Zhengcong Fei, Mingyuan Fan, Li Zhu, Junshi Huang, Xiaoming Wei, Xiaolin Wei </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.02291" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, Vector Quantized AutoRegressive (VQ-AR) models have shown
remarkable results in text-to-image synthesis by equally predicting discrete
image tokens from the top left to bottom right in the latent space. Although
the simple generative process surprisingly works well, is this the best way to
generate the image? For instance, human creation is more inclined to the
outline-to-fine of an image, while VQ-AR models themselves do not consider any
relative importance of image patches. In this paper, we present a progressive
model for high-fidelity text-to-image generation. The proposed method takes
effect by creating new image tokens from coarse to fine based on the existing
context in a parallel manner, and this procedure is recursively applied with
the proposed error revision mechanism until an image sequence is completed. The
resulting coarse-to-fine hierarchy makes the image generation process intuitive
and interpretable. Extensive experiments in MS COCO benchmark demonstrate that
the progressive model produces significantly better results compared with the
previous VQ-AR method in FID score across a wide variety of categories and
aspects. Moreover, the design of parallel generation in each step allows more
than $\times 13$ inference acceleration with slight performance loss.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## LDEdit: Towards Generalized Text Guided Image Manipulation via Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 05, 2022 </span>    
         <span class="authors"> Paramanand Chandramouli, Kanchana Vaishnavi Gandikota </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.02249" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Research in vision-language models has seen rapid developments off-late,
enabling natural language-based interfaces for image generation and
manipulation. Many existing text guided manipulation techniques are restricted
to specific classes of images, and often require fine-tuning to transfer to a
different style or domain. Nevertheless, generic image manipulation using a
single model with flexible text inputs is highly desirable. Recent work
addresses this task by guiding generative models trained on the generic image
datasets using pretrained vision-language encoders. While promising, this
approach requires expensive optimization for each input. In this work, we
propose an optimization-free method for the task of generic image manipulation
from text prompts. Our approach exploits recent Latent Diffusion Models (LDM)
for text to image generation to achieve zero-shot text guided manipulation. We
employ a deterministic forward diffusion in a lower dimensional latent space,
and the desired manipulation is achieved by simply providing the target text to
condition the reverse diffusion process. We refer to our approach as LDEdit. We
demonstrate the applicability of our method on semantic image manipulation and
artistic style transfer. Our method can accomplish image manipulation on
diverse domains and enables editing multiple attributes in a straightforward
fashion. Extensive experiments demonstrate the benefit of our approach over
competing baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 04, 2022 </span>    
         <span class="authors"> Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.01776" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.LG, physics.bio-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Predicting the binding structure of a small molecule ligand to a protein -- a
task known as molecular docking -- is critical to drug design. Recent deep
learning methods that treat docking as a regression problem have decreased
runtime compared to traditional search-based methods but have yet to offer
substantial improvements in accuracy. We instead frame molecular docking as a
generative modeling problem and develop DiffDock, a diffusion generative model
over the non-Euclidean manifold of ligand poses. To do so, we map this manifold
to the product space of the degrees of freedom (translational, rotational, and
torsional) involved in docking and develop an efficient diffusion process on
this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD<2A) on
PDBBind, significantly outperforming the previous state-of-the-art of
traditional docking (23%) and deep learning (20%) methods. Moreover, while
previous methods are not able to dock on computationally folded structures
(maximum accuracy 10.4%), DiffDock maintains significantly higher precision
(21.7%). Finally, DiffDock has fast inference times and provides confidence
estimates with high selective accuracy.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Red-Teaming the Stable Diffusion Safety Filter
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 03, 2022 </span>    
         <span class="authors"> Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, Florian Tramèr </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.04610" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.AI, cs.CR, cs.CV, cs.CY, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Stable Diffusion is a recent open-source image generation model comparable to
proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with
a safety filter that aims to prevent generating explicit images. Unfortunately,
the filter is obfuscated and poorly documented. This makes it hard for users to
prevent misuse in their applications, and to understand the filter's
limitations and improve it. We first show that it is easy to generate
disturbing content that bypasses the safety filter. We then reverse-engineer
the filter and find that while it aims to prevent sexual content, it ignores
violence, gore, and other similarly disturbing content. Based on our analysis,
we argue safety measures in future model releases should strive to be fully
open and properly documented to stimulate security contributions from the
community.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improving Sample Quality of Diffusion Models Using Self-Attention Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 03, 2022 </span>    
         <span class="authors"> Susung Hong, Gyuseong Lee, Wooseok Jang, Seungryong Kim </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.00939" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models (DDMs) have attracted attention for their
exceptional generation quality and diversity. This success is largely
attributed to the use of class- or text-conditional diffusion guidance methods,
such as classifier and classifier-free guidance. In this paper, we present a
more comprehensive perspective that goes beyond the traditional guidance
methods. From this generalized perspective, we introduce novel condition- and
training-free strategies to enhance the quality of generated images. As a
simple solution, blur guidance improves the suitability of intermediate samples
for their fine-scale information and structures, enabling diffusion models to
generate higher quality samples with a moderate guidance scale. Improving upon
this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps
of diffusion models to enhance their stability and efficacy. Specifically, SAG
adversarially blurs only the regions that diffusion models attend to at each
iteration and guides them accordingly. Our experimental results show that our
SAG improves the performance of various diffusion models, including ADM, IDDPM,
Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidance
methods leads to further improvement.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Statistical Efficiency of Score Matching: The View from Isoperimetry
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 03, 2022 </span>    
         <span class="authors"> Frederic Koehler, Alexander Heckett, Andrej Risteski </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.00726" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.ST, stat.ML, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep generative models parametrized up to a normalizing constant (e.g.
energy-based models) are difficult to train by maximizing the likelihood of the
data because the likelihood and/or gradients thereof cannot be explicitly or
efficiently written down. Score matching is a training method, whereby instead
of fitting the likelihood $\log p(x)$ for the training data, we instead fit the
score function $\nabla_x \log p(x)$ -- obviating the need to evaluate the
partition function. Though this estimator is known to be consistent, its
unclear whether (and when) its statistical efficiency is comparable to that of
maximum likelihood -- which is known to be (asymptotically) optimal. We
initiate this line of inquiry in this paper, and show a tight connection
between statistical efficiency of score matching and the isoperimetric
properties of the distribution being estimated -- i.e. the Poincar\'e,
log-Sobolev and isoperimetric constant -- quantities which govern the mixing
time of Markov processes like Langevin dynamics. Roughly, we show that the
score matching estimator is statistically comparable to the maximum likelihood
when the distribution has a small isoperimetric constant. Conversely, if the
distribution has a large isoperimetric constant -- even for simple families of
distributions like exponential families with rich enough sufficient statistics
-- score matching will be substantially less efficient than maximum likelihood.
We suitably formalize these results both in the finite sample regime, and in
the asymptotic regime. Finally, we identify a direct parallel in the discrete
setting, where we connect the statistical properties of pseudolikelihood
estimation with approximate tensorization of entropy and the Glauber dynamics.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 02, 2022 </span>    
         <span class="authors"> Ali Borji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.00586" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The field of image synthesis has made great strides in the last couple of
years. Recent models are capable of generating images with astonishing quality.
Fine-grained evaluation of these models on some interesting categories such as
faces is still missing. Here, we conduct a quantitative comparison of three
popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their
ability to generate photorealistic faces in the wild. We find that Stable
Diffusion generates better faces than the other systems, according to the FID
score. We also introduce a dataset of generated faces in the wild dubbed GFW,
including a total of 15,076 faces. Furthermore, we hope that our study spurs
follow-up research in assessing the generative models and improving them. Data
and code are available at data and code, respectively.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## OCD: Learning to Overfit with Conditional Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 02, 2022 </span>    
         <span class="authors"> Shahar Lutati, Lior Wolf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2210.00471" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a dynamic model in which the weights are conditioned on an input
sample x and are learned to match those that would be obtained by finetuning a
base model on x and its label y. This mapping between an input sample and
network weights is approximated by a denoising diffusion model. The diffusion
model we employ focuses on modifying a single layer of the base model and is
conditioned on the input, activations, and output of this layer. Since the
diffusion model is stochastic in nature, multiple initializations generate
different networks, forming an ensemble, which leads to further improvements.
Our experiments demonstrate the wide applicability of the method for image
classification, 3D reconstruction, tabular data, speech separation, and natural
language processing. Our code is available at
https://github.com/ShaharLutatiPersonal/OCD
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Protein structure generation via folding diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 30, 2022 </span>    
         <span class="authors"> Kevin E. Wu, Kevin K. Yang, Rianne van den Berg, James Y. Zou, Alex X. Lu, Ava P. Amini </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.15611" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.AI, I.2.0; J.3
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The ability to computationally generate novel yet physically foldable protein
structures could lead to new biological discoveries and new treatments
targeting yet incurable diseases. Despite recent advances in protein structure
prediction, directly generating diverse, novel protein structures from neural
networks remains difficult. In this work, we present a new diffusion-based
generative model that designs protein backbone structures via a procedure that
mirrors the native folding process. We describe protein backbone structure as a
series of consecutive angles capturing the relative orientation of the
constituent amino acid residues, and generate new structures by denoising from
a random, unfolded state towards a stable folded structure. Not only does this
mirror how proteins biologically twist into energetically favorable
conformations, the inherent shift and rotational invariance of this
representation crucially alleviates the need for complex equivariant networks.
We train a denoising diffusion probabilistic model with a simple transformer
backbone and demonstrate that our resulting model unconditionally generates
highly realistic protein structures with complexity and structural patterns
akin to those of naturally-occurring proteins. As a useful resource, we release
the first open-source codebase and trained models for protein structure
diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Building Normalizing Flows with Stochastic Interpolants
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 30, 2022 </span>    
         <span class="authors"> Michael S. Albergo, Eric Vanden-Eijnden </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.15571" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 A generative model based on a continuous-time normalizing flow between any
pair of base and target probability densities is proposed. The velocity field
of this flow is inferred from the probability current of a time-dependent
density that interpolates between the base and the target in finite time.
Unlike conventional normalizing flow inference methods based the maximum
likelihood principle, which require costly backpropagation through ODE solvers,
our interpolant approach leads to a simple quadratic loss for the velocity
itself which is expressed in terms of expectations that are readily amenable to
empirical estimation. The flow can be used to generate samples from either the
base or target, and to estimate the likelihood at any time along the
interpolant. In addition, the flow can be optimized to minimize the path length
of the interpolant density, thereby paving the way for building optimal
transport maps. In situations where the base is a Gaussian density, we also
show that the velocity of our normalizing flow can also be used to construct a
diffusion model to sample the target as well as estimate its score. However,
our approach shows that we can bypass this diffusion completely and work at the
level of the probability flow with greater simplicity, opening an avenue for
methods based solely on ordinary differential equations as an alternative to
those based on stochastic differential equations. Benchmarking on density
estimation tasks illustrates that the learned flow can match and surpass
conventional continuous flows at a fraction of the cost, and compares well with
diffusions on image generation on CIFAR-10 and ImageNet $32\times32$. The
method scales ab-initio ODE flows to previously unreachable image resolutions,
demonstrated up to $128\times128$.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## TabDDPM: Modelling Tabular Data with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 30, 2022 </span>    
         <span class="authors"> Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, Artem Babenko </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.15421" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models are currently becoming the leading
paradigm of generative modeling for many important data modalities. Being the
most prevalent in the computer vision community, diffusion models have also
recently gained some attention in other domains, including speech, NLP, and
graph-like data. In this work, we investigate if the framework of diffusion
models can be advantageous for general tabular problems, where datapoints are
typically represented by vectors of heterogeneous features. The inherent
heterogeneity of tabular data makes it quite challenging for accurate modeling,
since the individual features can be of completely different nature, i.e., some
of them can be continuous and some of them can be discrete. To address such
data types, we introduce TabDDPM -- a diffusion model that can be universally
applied to any tabular dataset and handles any type of feature. We extensively
evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority
over existing GAN/VAE alternatives, which is consistent with the advantage of
diffusion models in other fields. Additionally, we show that TabDDPM is
eligible for privacy-oriented setups, where the original datapoints cannot be
publicly shared.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-based Image Translation using Disentangled Style and Content Representation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 30, 2022 </span>    
         <span class="authors"> Gihyun Kwon, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.15264" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based image translation guided by semantic texts or a single target
image has enabled flexible style transfer which is not limited to the specific
domains. Unfortunately, due to the stochastic nature of diffusion models, it is
often difficult to maintain the original content of the image during the
reverse diffusion. To address this, here we present a novel diffusion-based
unsupervised image translation method using disentangled style and content
representation.
  Specifically, inspired by the splicing Vision Transformer, we extract
intermediate keys of multihead self attention layer from ViT model and used
them as the content preservation loss. Then, an image guided style transfer is
performed by matching the [CLS] classification token from the denoised samples
and target image, whereas additional CLIP loss is used for the text-driven
style transfer. To further accelerate the semantic change during the reverse
diffusion, we also propose a novel semantic divergence loss and resampling
strategy. Our experimental results show that the proposed method outperforms
state-of-the-art baseline models in both text-guided and image-guided
translation tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DreamFusion: Text-to-3D using 2D Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14988" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent breakthroughs in text-to-image synthesis have been driven by diffusion
models trained on billions of image-text pairs. Adapting this approach to 3D
synthesis would require large-scale datasets of labeled 3D data and efficient
architectures for denoising 3D data, neither of which currently exist. In this
work, we circumvent these limitations by using a pretrained 2D text-to-image
diffusion model to perform text-to-3D synthesis. We introduce a loss based on
probability density distillation that enables the use of a 2D diffusion model
as a prior for optimization of a parametric image generator. Using this loss in
a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a
Neural Radiance Field, or NeRF) via gradient descent such that its 2D
renderings from random angles achieve a low loss. The resulting 3D model of the
given text can be viewed from any angle, relit by arbitrary illumination, or
composited into any 3D environment. Our approach requires no 3D training data
and no modifications to the image diffusion model, demonstrating the
effectiveness of pretrained image diffusion models as priors.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Human Motion Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, Amit H. Bermano </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14916" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Natural and expressive human motion generation is the holy grail of computer
animation. It is a challenging task, due to the diversity of possible motion,
human perceptual sensitivity to it, and the difficulty of accurately describing
it. Therefore, current generative solutions are either low-quality or limited
in expressiveness. Diffusion models, which have already shown remarkable
generative capabilities in other domains, are promising candidates for human
motion due to their many-to-many nature, but they tend to be resource hungry
and hard to control. In this paper, we introduce Motion Diffusion Model (MDM),
a carefully adapted classifier-free diffusion-based generative model for the
human motion domain. MDM is transformer-based, combining insights from motion
generation literature. A notable design-choice is the prediction of the sample,
rather than the noise, in each diffusion step. This facilitates the use of
established geometric losses on the locations and velocities of the motion,
such as the foot contact loss. As we demonstrate, MDM is a generic approach,
enabling different modes of conditioning, and different generation tasks. We
show that our model is trained with lightweight resources and yet achieves
state-of-the-art results on leading benchmarks for text-to-motion and
action-to-motion. https://guytevet.github.io/mdm-page/ .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Probabilistic Models for Styled Walking Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Edmund J. C. Findlay, Haozheng Zhang, Ziyi Chang, Hubert P. H. Shum </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14828" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generating realistic motions for digital humans is time-consuming for many
graphics applications. Data-driven motion synthesis approaches have seen solid
progress in recent years through deep generative models. These results offer
high-quality motions but typically suffer in motion style diversity. For the
first time, we propose a framework using the denoising diffusion probabilistic
model (DDPM) to synthesize styled human motions, integrating two tasks into one
pipeline with increased style diversity compared with traditional motion
synthesis methods. Experimental results show that our system can generate
high-quality and diverse walking motions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Analyzing Diffusion as Serial Reproduction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Raja Marjieh, Ilia Sucholutsky, Thomas A. Langlois, Nori Jacoby, Thomas L. Griffiths </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14821" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are a class of generative models that learn to synthesize
samples by inverting a diffusion process that gradually maps data into noise.
While these models have enjoyed great success recently, a full theoretical
understanding of their observed properties is still lacking, in particular,
their weak sensitivity to the choice of noise family and the role of adequate
scheduling of noise levels for good synthesis. By identifying a correspondence
between diffusion models and a well-known paradigm in cognitive science known
as serial reproduction, whereby human agents iteratively observe and reproduce
stimuli from memory, we show how the aforementioned properties of diffusion
models can be explained as a natural consequence of this correspondence. We
then complement our theoretical analysis with simulations that exhibit these
key features. Our work highlights how classic paradigms in cognitive science
can shed light on state-of-the-art machine learning problems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Make-A-Video: Text-to-Video Generation without Text-Video Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14792" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose Make-A-Video -- an approach for directly translating the
tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video
(T2V). Our intuition is simple: learn what the world looks like and how it is
described from paired text-image data, and learn how the world moves from
unsupervised video footage. Make-A-Video has three advantages: (1) it
accelerates training of the T2V model (it does not need to learn visual and
multimodal representations from scratch), (2) it does not require paired
text-video data, and (3) the generated videos inherit the vastness (diversity
in aesthetic, fantastical depictions, etc.) of today's image generation models.
We design a simple yet effective way to build on T2I models with novel and
effective spatial-temporal modules. First, we decompose the full temporal U-Net
and attention tensors and approximate them in space and time. Second, we design
a spatial temporal pipeline to generate high resolution and frame rate videos
with a video decoder, interpolation model and two super resolution models that
can enable various applications besides T2V. In all aspects, spatial and
temporal resolution, faithfulness to text, and quality, Make-A-Video sets the
new state-of-the-art in text-to-video generation, as determined by both
qualitative and quantitative measures.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiGress: Discrete Denoising diffusion for graph generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, Pascal Frossard </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14734" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This work introduces DiGress, a discrete denoising diffusion model for
generating graphs with categorical node and edge attributes. Our model utilizes
a discrete diffusion process that progressively edits graphs with noise,
through the process of adding or removing edges and changing the categories. A
graph transformer network is trained to revert this process, simplifying the
problem of distribution learning over graphs into a sequence of node and edge
classification tasks. We further improve sample quality by introducing a
Markovian noise model that preserves the marginal distribution of node and edge
types during diffusion, and by incorporating auxiliary graph-theoretic
features. A procedure for conditioning the generation on graph-level features
is also proposed. DiGress achieves state-of-the-art performance on molecular
and non-molecular datasets, with up to 3x validity improvement on a planar
graph dataset. It is also the first model to scale to the large GuacaMol
dataset containing 1.3M drug-like molecules without the use of
molecule-specific representations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Creative Painting with Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Xianchao Wu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14697" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.CL, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Artistic painting has achieved significant progress during recent years.
Using an autoencoder to connect the original images with compressed latent
spaces and a cross attention enhanced U-Net as the backbone of diffusion,
latent diffusion models (LDMs) have achieved stable and high fertility image
generation. In this paper, we focus on enhancing the creative painting ability
of current LDMs in two directions, textual condition extension and model
retraining with Wikiart dataset. Through textual condition extension, users'
input prompts are expanded with rich contextual knowledge for deeper
understanding and explaining the prompts. Wikiart dataset contains 80K famous
artworks drawn during recent 400 years by more than 1,000 famous artists in
rich styles and genres. Through the retraining, we are able to ask these
artists to draw novel and creative painting on modern topics. Direct
comparisons with the original model show that the creativity and artistry are
enriched.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Posterior Sampling for General Noisy Inverse Problems
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14687" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.AI, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have been recently studied as powerful generative inverse
problem solvers, owing to their high quality reconstructions and the ease of
combining existing iterative solvers. However, most works focus on solving
simple linear inverse problems in noiseless settings, which significantly
under-represents the complexity of real-world problems. In this work, we extend
diffusion solvers to efficiently handle general noisy (non)linear inverse
problems via approximation of the posterior sampling. Interestingly, the
resulting posterior sampling scheme is a blended version of diffusion sampling
with the manifold constrained gradient without a strict measurement consistency
projection step, yielding a more desirable generative path in noisy settings
compared to the previous studies. Our method demonstrates that diffusion models
can incorporate various measurement noise statistics such as Gaussian and
Poisson, and also efficiently handle noisy nonlinear inverse problems such as
Fourier phase retrieval and non-uniform deblurring. Code available at
https://github.com/DPS2022/diffusion-posterior-sampling
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising MCMC for Accelerating Diffusion-Based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Beomsu Kim, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14593" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are powerful generative models that simulate the reverse of
diffusion processes using score functions to synthesize data from noise. The
sampling process of diffusion models can be interpreted as solving the reverse
stochastic differential equation (SDE) or the ordinary differential equation
(ODE) of the diffusion process, which often requires up to thousands of
discretization steps to generate a single image. This has sparked a great
interest in developing efficient integration techniques for reverse-S/ODEs.
Here, we propose an orthogonal approach to accelerating score-based sampling:
Denoising MCMC (DMCMC). DMCMC first uses MCMC to produce samples in the product
space of data and variance (or diffusion time). Then, a reverse-S/ODE
integrator is used to denoise the MCMC samples. Since MCMC traverses close to
the data manifold, the computation cost of producing a clean sample for DMCMC
is much less than that of producing a clean sample from noise. To verify the
proposed concept, we show that Denoising Langevin Gibbs (DLG), an instance of
DMCMC, successfully accelerates all six reverse-S/ODE integrators considered in
this work on the tasks of CIFAR10 and CelebA-HQ-256 image generation. Notably,
combined with integrators of Karras et al. (2022) and pre-trained score models
of Song et al. (2021b), DLG achieves SOTA results. In the limited number of
score function evaluation (NFE) settings on CIFAR10, we have $3.86$ FID with
$\approx 10$ NFE and $2.63$ FID with $\approx 20$ NFE. On CelebA-HQ-256, we
have $6.99$ FID with $\approx 160$ NFE, which beats the current best record of
Kim et al. (2022) among score-based models, $7.16$ FID with $4000$ NFE. Code:
https://github.com/1202kbs/DMCMC
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Boah Kim, Yujin Oh, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14566" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Vessel segmentation in medical images is one of the important tasks in the
diagnosis of vascular diseases and therapy planning. Although learning-based
segmentation approaches have been extensively studied, a large amount of
ground-truth labels are required in supervised methods and confusing background
structures make neural networks hard to segment vessels in an unsupervised
manner. To address this, here we introduce a novel diffusion adversarial
representation learning (DARL) model that leverages a denoising diffusion
probabilistic model with adversarial learning, and apply it to vessel
segmentation. In particular, for self-supervised vessel segmentation, DARL
learns the background signal using a diffusion module, which lets a generation
module effectively provide vessel representations. Also, by adversarial
learning based on the proposed switchable spatially-adaptive denormalization,
our model estimates synthetic fake vessel images as well as vessel segmentation
masks, which further makes the model capture vessel-relevant semantic
information. Once the proposed model is trained, the model generates
segmentation masks in a single step and can be applied to general vascular
structure segmentation of coronary angiography and retinal images. Experimental
results on various datasets show that our method significantly outperforms
existing unsupervised and self-supervised vessel segmentation methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Re-Imagen: Retrieval-Augmented Text-to-Image Generator
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 29, 2022 </span>    
         <span class="authors"> Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W. Cohen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14491" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Research on text-to-image generation has witnessed significant progress in
generating diverse and photo-realistic images, driven by diffusion and
auto-regressive models trained on large-scale image-text data. Though
state-of-the-art models can generate high-quality images of common entities,
they often have difficulty generating images of uncommon entities, such as
`Chortai (dog)' or `Picarones (food)'. To tackle this issue, we present the
Retrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model
that uses retrieved information to produce high-fidelity and faithful images,
even for rare or unseen entities. Given a text prompt, Re-Imagen accesses an
external multi-modal knowledge base to retrieve relevant (image, text) pairs
and uses them as references to generate the image. With this retrieval step,
Re-Imagen is augmented with the knowledge of high-level semantics and low-level
visual details of the mentioned entities, and thus improves its accuracy in
generating the entities' visual appearances. We train Re-Imagen on a
constructed dataset containing (image, text, retrieval) triples to teach the
model to ground on both text prompt and retrieval. Furthermore, we develop a
new sampling strategy to interleave the classifier-free guidance for text and
retrieval conditions to balance the text and retrieval alignment. Re-Imagen
achieves significant gain on FID score over COCO and WikiImage. To further
evaluate the capabilities of the model, we introduce EntityDrawBench, a new
benchmark that evaluates image generation for diverse entities, from frequent
to rare, across multiple object categories including dogs, foods, landmarks,
birds, and characters. Human evaluation on EntityDrawBench shows that Re-Imagen
can significantly improve the fidelity of generated images, especially on less
frequent entities.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score Modeling for Simulation-based Inference
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 28, 2022 </span>    
         <span class="authors"> Tomas Geffner, George Papamakarios, Andriy Mnih </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14249" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Neural Posterior Estimation methods for simulation-based inference can be
ill-suited for dealing with posterior distributions obtained by conditioning on
multiple observations, as they tend to require a large number of simulator
calls to learn accurate approximations. In contrast, Neural Likelihood
Estimation methods can handle multiple observations at inference time after
learning from individual observations, but they rely on standard inference
methods, such as MCMC or variational inference, which come with certain
performance drawbacks. We introduce a new method based on conditional score
modeling that enjoys the benefits of both approaches. We model the scores of
the (diffused) posterior distributions induced by individual observations, and
introduce a way of combining the learned scores to approximately sample from
the target posterior distribution. Our approach is sample-efficient, can
naturally aggregate multiple observations at inference time, and avoids the
drawbacks of standard inference methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Spectral Diffusion Processes
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 28, 2022 </span>    
         <span class="authors"> Angus Phillips, Thomas Seror, Michael Hutchinson, Valentin De Bortoli, Arnaud Doucet, Emile Mathieu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.14125" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative modelling (SGM) has proven to be a very effective
method for modelling densities on finite-dimensional spaces. In this work we
propose to extend this methodology to learn generative models over functional
spaces. To do so, we represent functional data in spectral space to dissociate
the stochastic part of the processes from their space-time part. Using
dimensionality reduction techniques we then sample from their stochastic
component using finite dimensional SGM. We demonstrate our method's
effectiveness for modelling various multimodal datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 27, 2022 </span>    
         <span class="authors"> Nisha Huang, Fan Tang, Weiming Dong, Changsheng Xu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.13360" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Digital art synthesis is receiving increasing attention in the multimedia
community because of engaging the public with art effectively. Current digital
art synthesis methods usually use single-modality inputs as guidance, thereby
limiting the expressiveness of the model and the diversity of generated
results. To solve this problem, we propose the multimodal guided artwork
diffusion (MGAD) model, which is a diffusion-based digital artwork generation
approach that utilizes multimodal prompts as guidance to control the
classifier-free diffusion model. Additionally, the contrastive language-image
pretraining (CLIP) model is used to unify text and image modalities. Extensive
experimental results on the quality and quantity of the generated digital art
paintings confirm the effectiveness of the combination of the diffusion model
and multimodal guidance. Code is available at
https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning to Learn with Generative Models of Neural Network Checkpoints
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 26, 2022 </span>    
         <span class="authors"> William Peebles, Ilija Radosavovic, Tim Brooks, Alexei A. Efros, Jitendra Malik </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.12892" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We explore a data-driven approach for learning to optimize neural networks.
We construct a dataset of neural network checkpoints and train a generative
model on the parameters. In particular, our model is a conditional diffusion
transformer that, given an initial input parameter vector and a prompted loss,
error, or return, predicts the distribution over parameter updates that achieve
the desired metric. At test time, it can optimize neural networks with unseen
parameters for downstream tasks in just one update. We find that our approach
successfully generates parameters for a wide range of loss prompts. Moreover,
it can sample multimodal parameter solutions and has favorable scaling
properties. We apply our method to different neural network architectures and
tasks in supervised and reinforcement learning.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Quasi-Conservative Score-based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 26, 2022 </span>    
         <span class="authors"> Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Chun-Yi Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.12753" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Existing Score-based Generative Models (SGMs) can be categorized into
constrained SGMs (CSGMs) or unconstrained SGMs (USGMs) according to their
parameterization approaches. CSGMs model probability density functions as
Boltzmann distributions, and assign their predictions as the negative gradients
of some scalar-valued energy functions. On the other hand, USGMs employ
flexible architectures capable of directly estimating scores without the need
to explicitly model energy functions. In this paper, we demonstrate that the
architectural constraints of CSGMs may limit their modeling ability. In
addition, we show that USGMs' inability to preserve the property of
conservativeness may lead to degraded sampling performance in practice. To
address the above issues, we propose Quasi-Conservative Score-based Generative
Models (QCSGMs) for keeping the advantages of both CSGMs and USGMs. Our
theoretical derivations demonstrate that the training objective of QCSGMs can
be efficiently integrated into the training processes by leveraging the
Hutchinson trace estimator. In addition, our experimental results on the
CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets validate the effectiveness of
QCSGMs. Finally, we justify the advantage of QCSGMs using an example of a
one-layered autoencoder.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Convergence of score-based generative modeling for general data distributions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 26, 2022 </span>    
         <span class="authors"> Holden Lee, Jianfeng Lu, Yixin Tan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.12381" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.PR, math.ST, stat.ML, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative modeling (SGM) has grown to be a hugely successful
method for learning to generate samples from complex data distributions such as
that of images and audio. It is based on evolving an SDE that transforms white
noise into a sample from the learned distribution, using estimates of the score
function, or gradient log-pdf. Previous convergence analyses for these methods
have suffered either from strong assumptions on the data distribution or
exponential dependencies, and hence fail to give efficient guarantees for the
multimodal and non-smooth distributions that arise in practice and for which
good empirical performance is observed. We consider a popular kind of SGM --
denoising diffusion models -- and give polynomial convergence guarantees for
general data distributions, with no assumptions related to functional
inequalities or smoothness. Assuming $L^2$-accurate score estimates, we obtain
Wasserstein distance guarantees for any distribution of bounded support or
sufficiently decaying tails, as well as TV guarantees for distributions with
further smoothness assumptions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 24, 2022 </span>    
         <span class="authors"> Qing Lyu, Ge Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.12104" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, physics.med-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 MRI and CT are most widely used medical imaging modalities. It is often
necessary to acquire multi-modality images for diagnosis and treatment such as
radiotherapy planning. However, multi-modality imaging is not only costly but
also introduces misalignment between MRI and CT images. To address this
challenge, computational conversion is a viable approach between MRI and CT
images, especially from MRI to CT images. In this paper, we propose to use an
emerging deep learning framework called diffusion and score-matching models in
this context. Specifically, we adapt denoising diffusion probabilistic and
score-matching models, use four different sampling strategies, and compare
their performance metrics with that using a convolutional neural network and a
generative adversarial network model. Our results show that the diffusion and
score-matching models generate better synthetic CT images than the CNN and GAN
models. Furthermore, we investigate the uncertainties associated with the
diffusion and score-matching networks using the Monte-Carlo method, and improve
the results by averaging their Monte-Carlo outputs. Our study suggests that
diffusion and score-matching models are powerful to generate high quality
images conditioned on an image obtained using a complementary imaging modality,
analytically rigorous with clear explainability, and highly competitive with
CNNs and GANs for image synthesis.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 22, 2022 </span>    
         <span class="authors"> Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, Anru R. Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.11215" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.ST, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We provide theoretical convergence guarantees for score-based generative
models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which
constitute the backbone of large-scale real-world generative models such as
DALL$\cdot$E 2. Our main result is that, assuming accurate score estimates,
such SGMs can efficiently sample from essentially any realistic data
distribution. In contrast to prior works, our results (1) hold for an
$L^2$-accurate score estimate (rather than $L^\infty$-accurate); (2) do not
require restrictive functional inequality conditions that preclude substantial
non-log-concavity; (3) scale polynomially in all relevant problem parameters;
and (4) match state-of-the-art complexity guarantees for discretization of the
Langevin diffusion, provided that the score error is sufficiently small. We
view this as strong theoretical justification for the empirical success of
SGMs. We also examine SGMs based on the critically damped Langevin diffusion
(CLD). Contrary to conventional wisdom, we provide evidence that the use of the
CLD does not reduce the complexity of SGMs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Poisson Flow Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 22, 2022 </span>    
         <span class="authors"> Yilun Xu, Ziming Liu, Max Tegmark, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.11178" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a new "Poisson flow" generative model (PFGM) that maps a uniform
distribution on a high-dimensional hemisphere into any data distribution. We
interpret the data points as electrical charges on the $z=0$ hyperplane in a
space augmented with an additional dimension $z$, generating a high-dimensional
electric field (the gradient of the solution to Poisson equation). We prove
that if these charges flow upward along electric field lines, their initial
distribution in the $z=0$ plane transforms into a distribution on the
hemisphere of radius $r$ that becomes uniform in the $r \to\infty$ limit. To
learn the bijective transformation, we estimate the normalized field in the
augmented space. For sampling, we devise a backward ODE that is anchored by the
physically meaningful additional dimension: the samples hit the unaugmented
data manifold when the $z$ reaches zero. Experimentally, PFGM achieves current
state-of-the-art performance among the normalizing flow models on CIFAR-10,
with an Inception score of $9.68$ and a FID score of $2.35$. It also performs
on par with the state-of-the-art SDE approaches while offering $10\times $ to
$20 \times$ acceleration on image generation tasks. Additionally, PFGM appears
more tolerant of estimation errors on a weaker network architecture and robust
to the step size in the Euler method. The code is available at
https://github.com/Newbeeer/poisson_flow .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 22, 2022 </span>    
         <span class="authors"> Junyoung Seo, Gyuseong Lee, Seokju Cho, Jiyoung Lee, Seungryong Kim </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.11047" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a novel method for exemplar-based image translation, called
matching interleaved diffusion models (MIDMs). Most existing methods for this
task were formulated as GAN-based matching-then-generation framework. However,
in this framework, matching errors induced by the difficulty of semantic
matching across cross-domain, e.g., sketch and photo, can be easily propagated
to the generation step, which in turn leads to degenerated results. Motivated
by the recent success of diffusion models overcoming the shortcomings of GANs,
we incorporate the diffusion models to overcome these limitations.
Specifically, we formulate a diffusion-based matching-and-generation framework
that interleaves cross-domain matching and diffusion steps in the latent space
by iteratively feeding the intermediate warp into the noising process and
denoising it to generate a translated image. In addition, to improve the
reliability of the diffusion process, we design a confidence-aware process
using cycle-consistency to consider only confident regions during translation.
Experimental results show that our MIDMs generate more plausible images than
state-of-the-art methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Implementing and Experimenting with Diffusion Models for Text-to-Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 22, 2022 </span>    
         <span class="authors"> Robin Zbinden </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.10948" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Taking advantage of the many recent advances in deep learning, text-to-image
generative models currently have the merit of attracting the general public
attention. Two of these models, DALL-E 2 and Imagen, have demonstrated that
highly photorealistic images could be generated from a simple textual
description of an image. Based on a novel approach for image generation called
diffusion models, text-to-image models enable the production of many different
types of high resolution images, where human imagination is the only limit.
  However, these models require exceptionally large amounts of computational
resources to train, as well as handling huge datasets collected from the
internet. In addition, neither the codebase nor the models have been released.
It consequently prevents the AI community from experimenting with these
cutting-edge models, making the reproduction of their results complicated, if
not impossible.
  In this thesis, we aim to contribute by firstly reviewing the different
approaches and techniques used by these models, and then by proposing our own
implementation of a text-to-image model. Highly based on DALL-E 2, we introduce
several slight modifications to tackle the high computational cost induced. We
thus have the opportunity to experiment in order to understand what these
models are capable of, especially in a low resource regime. In particular, we
provide additional and analyses deeper than the ones performed by the authors
of DALL-E 2, including ablation studies.
  Besides, diffusion models use so-called guidance methods to help the
generating process. We introduce a new guidance method which can be used in
conjunction with other guidance methods to improve image quality. Finally, the
images generated by our model are of reasonably good quality, without having to
sustain the significant training costs of state-of-the-art text-to-image
models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Mandarin Singing Voice Synthesis with Denoising Diffusion Probabilistic Wasserstein GAN
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 21, 2022 </span>    
         <span class="authors"> Yin-Ping Cho, Yu Tsao, Hsin-Min Wang, Yi-Wen Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.10446" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.SD, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Singing voice synthesis (SVS) is the computer production of a human-like
singing voice from given musical scores. To accomplish end-to-end SVS
effectively and efficiently, this work adopts the acoustic model-neural vocoder
architecture established for high-quality speech and singing voice synthesis.
Specifically, this work aims to pursue a higher level of expressiveness in
synthesized voices by combining the diffusion denoising probabilistic model
(DDPM) and \emph{Wasserstein} generative adversarial network (WGAN) to
construct the backbone of the acoustic model. On top of the proposed acoustic
model, a HiFi-GAN neural vocoder is adopted with integrated fine-tuning to
ensure optimal synthesis quality for the resulting end-to-end SVS system. This
end-to-end system was evaluated with the multi-singer Mpop600 Mandarin singing
voice dataset. In the experiments, the proposed system exhibits improvements
over previous landmark counterparts in terms of musical expressiveness and
high-frequency acoustic details. Moreover, the adversarial acoustic model
converged stably without the need to enforce reconstruction objectives,
indicating the convergence stability of the proposed DDPM and WGAN combined
architecture over alternative GAN-based SVS systems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Deep Generalized Schrödinger Bridge
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 20, 2022 </span>    
         <span class="authors"> Guan-Horng Liu, Tianrong Chen, Oswin So, Evangelos A. Theodorou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.09893" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.GT, cs.LG, math.OC
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Mean-Field Game (MFG) serves as a crucial mathematical framework in modeling
the collective behavior of individual agents interacting stochastically with a
large population. In this work, we aim at solving a challenging class of MFGs
in which the differentiability of these interacting preferences may not be
available to the solver, and the population is urged to converge exactly to
some desired distribution. These setups are, despite being well-motivated for
practical purposes, complicated enough to paralyze most (deep) numerical
solvers. Nevertheless, we show that Schr\"odinger Bridge - as an
entropy-regularized optimal transport model - can be generalized to accepting
mean-field structures, hence solving these MFGs. This is achieved via the
application of Forward-Backward Stochastic Differential Equations theory,
which, intriguingly, leads to a computational framework with a similar
structure to Temporal Difference learning. As such, it opens up novel
algorithmic connections to Deep Reinforcement Learning that we leverage to
facilitate practical training. We show that our proposed objective function
provides necessary and sufficient conditions to the mean-field problem. Our
method, named Deep Generalized Schr\"odinger Bridge (DeepGSB), not only
outperforms prior methods in solving classical population navigation MFGs, but
is also capable of solving 1000-dimensional opinion depolarization, setting a
new state-of-the-art numerical solver for high-dimensional MFGs. Our code will
be made available at https://github.com/ghliu/DeepGSB.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Metal Inpainting in CBCT Projections Using Score-based Generative Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 20, 2022 </span>    
         <span class="authors"> Siyuan Mei, Fuxin Fan, Andreas Maier </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.09733" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 During orthopaedic surgery, the inserting of metallic implants or screws are
often performed under mobile C-arm systems. Due to the high attenuation of
metals, severe metal artifacts occur in 3D reconstructions, which degrade the
image quality greatly. To reduce the artifacts, many metal artifact reduction
algorithms have been developed and metal inpainting in projection domain is an
essential step. In this work, a score-based generative model is trained on
simulated knee projections and the inpainted image is obtained by removing the
noise in conditional resampling process. The result implies that the inpainted
images by score-based generative model have more detailed information and
achieve the lowest mean absolute error and the highest
peak-signal-to-noise-ratio compared with interpolation and CNN based method.
Besides, the score-based model can also recover projections with big circlar
and rectangular masks, showing its generalization in inpainting task.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## T2V-DDPM: Thermal to Visible Face Translation using Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 19, 2022 </span>    
         <span class="authors"> Nithin Gopalakrishnan Nair, Vishal M. Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.08814" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Modern-day surveillance systems perform person recognition using deep
learning-based face verification networks. Most state-of-the-art facial
verification systems are trained using visible spectrum images. But, acquiring
images in the visible spectrum is impractical in scenarios of low-light and
nighttime conditions, and often images are captured in an alternate domain such
as the thermal infrared domain. Facial verification in thermal images is often
performed after retrieving the corresponding visible domain images. This is a
well-established problem often known as the Thermal-to-Visible (T2V) image
translation. In this paper, we propose a Denoising Diffusion Probabilistic
Model (DDPM) based solution for T2V translation specifically for facial images.
During training, the model learns the conditional distribution of visible
facial images given their corresponding thermal image through the diffusion
process. During inference, the visible domain image is obtained by starting
from Gaussian noise and performing denoising repeatedly. The existing inference
process for DDPMs is stochastic and time-consuming. Hence, we propose a novel
inference strategy for speeding up the inference time of DDPMs, specifically
for the problem of T2V image translation. We achieve the state-of-the-art
results on multiple datasets. The code and pretrained models are publically
available at http://github.com/Nithin-GK/T2V-DDPM
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Error Correction Codes
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 16, 2022 </span>    
         <span class="authors"> Yoni Choukroun, Lior Wolf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.13533" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.IT, cs.AI, cs.LG, math.IT
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Error correction code (ECC) is an integral part of the physical communication
layer, ensuring reliable data transfer over noisy channels. Recently, neural
decoders have demonstrated their advantage over classical decoding techniques.
However, recent state-of-the-art neural decoders suffer from high complexity
and lack the important iterative scheme characteristic of many legacy decoders.
In this work, we propose to employ denoising diffusion models for the soft
decoding of linear codes at arbitrary block lengths. Our framework models the
forward channel corruption as a series of diffusion steps that can be reversed
iteratively. Three contributions are made: (i) a diffusion process suitable for
the decoding setting is introduced, (ii) the neural diffusion decoder is
conditioned on the number of parity errors, which indicates the level of
corruption at a given step, (iii) a line search procedure based on the code's
syndrome obtains the optimal reverse diffusion step size. The proposed approach
demonstrates the power of diffusion models for ECC and is able to achieve state
of the art accuracy, outperforming the other neural decoders by sizable
margins, even for a single reverse diffusion step.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Brain Imaging Generation with Latent Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 15, 2022 </span>    
         <span class="authors"> Walter H. L. Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.07162" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, q-bio.QM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep neural networks have brought remarkable breakthroughs in medical image
analysis. However, due to their data-hungry nature, the modest dataset sizes in
medical imaging projects might be hindering their full potential. Generating
synthetic data provides a promising alternative, allowing to complement
training datasets and conducting medical image research at a larger scale.
Diffusion models recently have caught the attention of the computer vision
community by producing photorealistic synthetic images. In this study, we
explore using Latent Diffusion Models to generate synthetic images from
high-resolution 3D brain images. We used T1w MRI images from the UK Biobank
dataset (N=31,740) to train our models to learn about the probabilistic
distribution of brain images, conditioned on covariables, such as age, sex, and
brain structure volumes. We found that our models created realistic data, and
we could use the conditioning variables to control the data generation
effectively. Besides that, we created a synthetic dataset with 100,000 brain
images and made it openly available to the scientific community.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Lossy Image Compression with Conditional Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 14, 2022 </span>    
         <span class="authors"> Ruihan Yang, Stephan Mandt </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.06950" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper outlines an end-to-end optimized lossy image compression framework
using diffusion generative models. The approach relies on the transform coding
paradigm, where an image is mapped into a latent space for entropy coding and,
from there, mapped back to the data space for reconstruction. In contrast to
VAE-based neural compression, where the (mean) decoder is a deterministic
neural network, our decoder is a conditional diffusion model. Our approach thus
introduces an additional ``content'' latent variable on which the reverse
diffusion process is conditioned and uses this variable to store information
about the image. The remaining ``texture'' latent variables characterizing the
diffusion process are synthesized (stochastically or deterministically) at
decoding time. We show that the model's performance can be tuned toward
perceptual metrics of interest. Our extensive experiments involving five
datasets and sixteen image quality assessment metrics show that our approach
yields the strongest reported FID scores while also yielding competitive
performance with state-of-the-art models in several SIM-based reference
metrics.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PET image denoising based on denoising diffusion probabilistic models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 13, 2022 </span>    
         <span class="authors"> Kuang Gong, Keith A. Johnson, Georges El Fakhri, Quanzheng Li, Tinsu Pan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.06167" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, physics.med-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Due to various physical degradation factors and limited counts received, PET
image quality needs further improvements. The denoising diffusion probabilistic
models (DDPM) are distribution learning-based models, which try to transform a
normal distribution into a specific data distribution based on iterative
refinements. In this work, we proposed and evaluated different DDPM-based
methods for PET image denoising. Under the DDPM framework, one way to perform
PET image denoising is to provide the PET image and/or the prior image as the
network input. Another way is to supply the prior image as the input with the
PET image included in the refinement steps, which can fit for scenarios of
different noise levels. 120 18F-FDG datasets and 140 18F-MK-6240 datasets were
utilized to evaluate the proposed DDPM-based methods. Quantification show that
the DDPM-based frameworks with PET information included can generate better
results than the nonlocal mean and Unet-based denoising methods. Adding
additional MR prior in the model can help achieve better performance and
further reduce the uncertainty during image denoising. Solely relying on MR
prior while ignoring the PET information can result in large bias. Regional and
surface quantification shows that employing MR prior as the network input while
embedding PET image as a data-consistency constraint during inference can
achieve the best performance. In summary, DDPM-based PET image denoising is a
flexible framework, which can efficiently utilize prior information and achieve
better performance than the nonlocal mean and Unet-based denoising methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Blurring Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 12, 2022 </span>    
         <span class="authors"> Emiel Hoogeboom, Tim Salimans </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.05557" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, Rissanen et al., (2022) have presented a new type of diffusion
process for generative modeling based on heat dissipation, or blurring, as an
alternative to isotropic Gaussian diffusion. Here, we show that blurring can
equivalently be defined through a Gaussian diffusion process with non-isotropic
noise. In making this connection, we bridge the gap between inverse heat
dissipation and denoising diffusion, and we shed light on the inductive bias
that results from this modeling choice. Finally, we propose a generalized class
of diffusion models that offers the best of both standard Gaussian denoising
diffusion and inverse heat dissipation, which we call Blurring Diffusion
Models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Soft Diffusion: Score Matching for General Corruptions
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 12, 2022 </span>    
         <span class="authors"> Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G. Dimakis, Peyman Milanfar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.05442" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We define a broader family of corruption processes that generalizes
previously known diffusion models. To reverse these general diffusions, we
propose a new objective called Soft Score Matching that provably learns the
score function for any linear corruption process and yields state of the art
results for CelebA. Soft Score Matching incorporates the degradation process in
the network. Our new loss trains the model to predict a clean image,
\textit{that after corruption}, matches the diffused observation. We show that
our objective learns the gradient of the likelihood under suitable regularity
conditions for a family of corruption processes. We further develop a
principled way to select the corruption levels for general diffusion processes
and a novel sampling method that we call Momentum Sampler. We show
experimentally that our framework works for general linear corruption
processes, such as Gaussian blur and masking. We achieve state-of-the-art FID
score $1.85$ on CelebA-64, outperforming all previous linear diffusion models.
We also show significant computational benefits compared to vanilla denoising
diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models in Vision: A Survey
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 10, 2022 </span>    
         <span class="authors"> Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.04747" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models represent a recent emerging topic in computer
vision, demonstrating remarkable results in the area of generative modeling. A
diffusion model is a deep generative model that is based on two stages, a
forward diffusion stage and a reverse diffusion stage. In the forward diffusion
stage, the input data is gradually perturbed over several steps by adding
Gaussian noise. In the reverse stage, a model is tasked at recovering the
original input data by learning to gradually reverse the diffusion process,
step by step. Diffusion models are widely appreciated for the quality and
diversity of the generated samples, despite their known computational burdens,
i.e. low speeds due to the high number of steps involved during sampling. In
this survey, we provide a comprehensive review of articles on denoising
diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic
diffusion modeling frameworks, which are based on denoising diffusion
probabilistic models, noise conditioned score networks, and stochastic
differential equations. We further discuss the relations between diffusion
models and other deep generative models, including variational auto-encoders,
generative adversarial networks, energy-based models, autoregressive models and
normalizing flows. Then, we introduce a multi-perspective categorization of
diffusion models applied in computer vision. Finally, we illustrate the current
limitations of diffusion models and envision some interesting directions for
future research.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SE(3)-DiffusionFields: Learning cost functions for joint grasp and motion optimization through diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 08, 2022 </span>    
         <span class="authors"> Julen Urain, Niklas Funk, Jan Peters, Georgia Chalvatzaki </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.03855" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.RO, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Multi-objective optimization problems are ubiquitous in robotics, e.g., the
optimization of a robot manipulation task requires a joint consideration of
grasp pose configurations, collisions and joint limits. While some demands can
be easily hand-designed, e.g., the smoothness of a trajectory, several
task-specific objectives need to be learned from data. This work introduces a
method for learning data-driven SE(3) cost functions as diffusion models.
Diffusion models can represent highly-expressive multimodal distributions and
exhibit proper gradients over the entire space due to their score-matching
training objective. Learning costs as diffusion models allows their seamless
integration with other costs into a single differentiable objective function,
enabling joint gradient-based motion optimization. In this work, we focus on
learning SE(3) diffusion models for 6DoF grasping, giving rise to a novel
framework for joint grasp and motion optimization without needing to decouple
grasp selection from trajectory generation. We evaluate the representation
power of our SE(3) diffusion models w.r.t. classical generative models, and we
showcase the superior performance of our proposed optimization framework in a
series of simulated and real-world robotic manipulation tasks against
representative baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unifying Generative Models with GFlowNets
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 06, 2022 </span>    
         <span class="authors"> Dinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, Yoshua Bengio </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.02606" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 There are many frameworks for deep generative modeling, each often presented
with their own specific training algorithms and inference methods. Here, we
demonstrate the connections between existing deep generative models and the
recently introduced GFlowNet framework, a probabilistic inference machine which
treats sampling as a decision-making process. This analysis sheds light on
their overlapping traits and provides a unifying viewpoint through the lens of
learning with Markovian trajectories. Our framework provides a means for
unifying training and inference algorithms, and provides a route to shine a
unifying light over many generative models. Beyond this, we provide a practical
and experimentally verified recipe for improving generative modeling with
insights from the GFlowNet perspective.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Instrument Separation of Symbolic Music by Explicitly Guided Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 05, 2022 </span>    
         <span class="authors"> Sangjun Han, Hyeongrae Ihm, DaeHan Ahn, Woohyung Lim </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.02696" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.MM, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Similar to colorization in computer vision, instrument separation is to
assign instrument labels (e.g. piano, guitar...) to notes from unlabeled
mixtures which contain only performance information. To address the problem, we
adopt diffusion models and explicitly guide them to preserve consistency
between mixtures and music. The quantitative results show that our proposed
model can generate high-fidelity samples for multitrack symbolic music with
creativity.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## First Hitting Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 02, 2022 </span>    
         <span class="authors"> Mao Ye, Lemeng Wu, Qiang Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.01170" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a family of First Hitting Diffusion Models (FHDM), deep generative
models that generate data with a diffusion process that terminates at a random
first hitting time. This yields an extension of the standard fixed-time
diffusion models that terminate at a pre-specified deterministic time. Although
standard diffusion models are designed for continuous unconstrained data, FHDM
is naturally designed to learn distributions on continuous as well as a range
of discrete and structure domains. Moreover, FHDM enables instance-dependent
terminate time and accelerates the diffusion process to sample higher quality
data with fewer diffusion steps. Technically, we train FHDM by maximum
likelihood estimation on diffusion trajectories augmented from observed data
with conditional first hitting processes (i.e., bridge) derived based on Doob's
$h$-transform, deviating from the commonly used time-reversal mechanism. We
apply FHDM to generate data in various domains such as point cloud (general
continuous distribution), climate and geographical events on earth (continuous
distribution on the sphere), unweighted graphs (distribution of binary
matrices), and segmentation maps of 2D images (high-dimensional categorical
distribution). We observe considerable improvement compared with the
state-of-the-art approaches in both quality and speed.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-based Molecule Generation with Informative Prior Bridges
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 02, 2022 </span>    
         <span class="authors"> Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, Qiang Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.00865" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 AI-based molecule generation provides a promising approach to a large area of
biomedical sciences and engineering, such as antibody design, hydrolase
engineering, or vaccine development. Because the molecules are governed by
physical laws, a key challenge is to incorporate prior information into the
training procedure to generate high-quality and realistic molecules. We propose
a simple and novel approach to steer the training of diffusion-based generative
models with physical and statistics prior information. This is achieved by
constructing physically informed diffusion bridges, stochastic processes that
guarantee to yield a given observation at the fixed terminal time. We develop a
Lyapunov function based method to construct and determine bridges, and propose
a number of proposals of informative prior bridges for both high-quality
molecule generation and uniformity-promoted 3D point cloud generation. With
comprehensive experiments, we show that our method provides a powerful approach
to the 3D generation task, yielding molecule structures with better quality and
stability scores and more uniformly distributed point clouds of high qualities.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models: A Comprehensive Survey of Methods and Applications
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 02, 2022 </span>    
         <span class="authors"> Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, Ming-Hsuan Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2209.00796" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have emerged as a powerful new family of deep generative
models with record-breaking performance in many applications, including image
synthesis, video generation, and molecule design. In this survey, we provide an
overview of the rapidly expanding body of work on diffusion models,
categorizing the research into three key areas: efficient sampling, improved
likelihood estimation, and handling data with special structures. We also
discuss the potential for combining diffusion models with other generative
models for enhanced results. We further review the wide-ranging applications of
diffusion models in fields spanning from computer vision, natural language
generation, temporal data modeling, to interdisciplinary applications in other
scientific disciplines. This survey aims to provide a contextualized, in-depth
look at the state of diffusion models, identifying the key areas of focus and
pointing to potential areas for further exploration. Github:
https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 31, 2022 </span>    
         <span class="authors"> Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, Ziwei Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.15001" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Human motion modeling is important for many modern graphics applications,
which typically require professional skills. In order to remove the skill
barriers for laymen, recent motion generation methods can directly generate
human motions conditioned on natural languages. However, it remains challenging
to achieve diverse and fine-grained motion generation with various text inputs.
To address this problem, we propose MotionDiffuse, the first diffusion
model-based text-driven motion generation framework, which demonstrates several
desired properties over existing methods. 1) Probabilistic Mapping. Instead of
a deterministic language-motion mapping, MotionDiffuse generates motions
through a series of denoising steps in which variations are injected. 2)
Realistic Synthesis. MotionDiffuse excels at modeling complicated data
distribution and generating vivid motion sequences. 3) Multi-Level
Manipulation. MotionDiffuse responds to fine-grained instructions on body
parts, and arbitrary-length motion synthesis with time-varied text prompts. Our
experiments show MotionDiffuse outperforms existing SoTA methods by convincing
margins on text-driven motion generation and action-conditioned motion
generation. A qualitative analysis further demonstrates MotionDiffuse's
controllability for comprehensive motion generation. Homepage:
https://mingyuan-zhang.github.io/projects/MotionDiffuse.html
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Let us Build Bridges: Understanding and Extending Diffusion Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 31, 2022 </span>    
         <span class="authors"> Xingchao Liu, Lemeng Wu, Mao Ye, Qiang Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.14699" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based generative models have achieved promising results recently,
but raise an array of open questions in terms of conceptual understanding,
theoretical analysis, algorithm improvement and extensions to discrete,
structured, non-Euclidean domains. This work tries to re-exam the overall
framework, in order to gain better theoretical understandings and develop
algorithmic extensions for data from arbitrary domains. By viewing diffusion
models as latent variable models with unobserved diffusion trajectories and
applying maximum likelihood estimation (MLE) with latent trajectories imputed
from an auxiliary distribution, we show that both the model construction and
the imputation of latent trajectories amount to constructing diffusion bridge
processes that achieve deterministic values and constraints at end point, for
which we provide a systematic study and a suit of tools. Leveraging our
framework, we present 1) a first theoretical error analysis for learning
diffusion generation models, and 2) a simple and unified approach to learning
on data from different discrete and constrained domains. Experiments show that
our methods perform superbly on generating images, semantic segments and 3D
point clouds.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 30, 2022 </span>    
         <span class="authors"> Dominik J. E. Waibel, Ernst Röell, Bastian Rieck, Raja Giryes, Carsten Marr </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.14125" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, stat.ML, 68-06
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are a special type of generative model, capable of
synthesising new data from a learnt distribution. We introduce DISPR, a
diffusion-based model for solving the inverse problem of three-dimensional (3D)
cell shape prediction from two-dimensional (2D) single cell microscopy images.
Using the 2D microscopy image as a prior, DISPR is conditioned to predict
realistic 3D shape reconstructions. To showcase the applicability of DISPR as a
data augmentation tool in a feature-based single cell classification task, we
extract morphological features from the red blood cells grouped into six highly
imbalanced classes. Adding features from the DISPR predictions to the three
minority classes improved the macro F1 score from $F1_\text{macro} = 55.2 \pm
4.6\%$ to $F1_\text{macro} = 72.2 \pm 4.9\%$. We thus demonstrate that
diffusion models can be successfully applied to inverse biomedical problems,
and that they learn to reconstruct 3D shapes with realistic morphological
features from 2D microscopy images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 29, 2022 </span>    
         <span class="authors"> Wan-Cyuan Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, Yu-Chiang Frank Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.13753" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models (DMs) have shown great potential for high-quality image
synthesis. However, when it comes to producing images with complex scenes, how
to properly describe both image global structures and object details remains a
challenging task. In this paper, we present Frido, a Feature Pyramid Diffusion
model performing a multi-scale coarse-to-fine denoising process for image
synthesis. Our model decomposes an input image into scale-dependent vector
quantized features, followed by a coarse-to-fine gating for producing image
output. During the above multi-scale representation learning stage, additional
input conditions like text, scene graph, or image layout can be further
exploited. Thus, Frido can be also applied for conditional or cross-modality
image synthesis. We conduct extensive experiments over various unconditioned
and conditional image generation tasks, ranging from text-to-image synthesis,
layout-to-image, scene-graph-to-image, to label-to-image. More specifically, we
achieved state-of-the-art FID scores on five benchmarks, namely layout-to-image
on COCO and OpenImages, scene-graph-to-image on COCO and Visual Genome, and
label-to-image on COCO. Code is available at
https://github.com/davidhalladay/Frido.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 25, 2022 </span>    
         <span class="authors"> Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.12242" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large text-to-image models achieved a remarkable leap in the evolution of AI,
enabling high-quality and diverse synthesis of images from a given text prompt.
However, these models lack the ability to mimic the appearance of subjects in a
given reference set and synthesize novel renditions of them in different
contexts. In this work, we present a new approach for "personalization" of
text-to-image diffusion models. Given as input just a few images of a subject,
we fine-tune a pretrained text-to-image model such that it learns to bind a
unique identifier with that specific subject. Once the subject is embedded in
the output domain of the model, the unique identifier can be used to synthesize
novel photorealistic images of the subject contextualized in different scenes.
By leveraging the semantic prior embedded in the model with a new autogenous
class-specific prior preservation loss, our technique enables synthesizing the
subject in diverse scenes, poses, views and lighting conditions that do not
appear in the reference images. We apply our technique to several
previously-unassailable tasks, including subject recontextualization,
text-guided view synthesis, and artistic rendering, all while preserving the
subject's key features. We also provide a new dataset and evaluation protocol
for this new task of subject-driven generation. Project page:
https://dreambooth.github.io/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Understanding Diffusion Models: A Unified Perspective
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 25, 2022 </span>    
         <span class="authors"> Calvin Luo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.11970" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have shown incredible capabilities as generative models;
indeed, they power the current state-of-the-art models on text-conditioned
image generation such as Imagen and DALL-E 2. In this work we review,
demystify, and unify the understanding of diffusion models across both
variational and score-based perspectives. We first derive Variational Diffusion
Models (VDM) as a special case of a Markovian Hierarchical Variational
Autoencoder, where three key assumptions enable tractable computation and
scalable optimization of the ELBO. We then prove that optimizing a VDM boils
down to learning a neural network to predict one of three potential objectives:
the original source input from any arbitrary noisification of it, the original
source noise from any arbitrarily noisified input, or the score function of a
noisified input at any arbitrary noise level. We then dive deeper into what it
means to learn the score function, and connect the variational perspective of a
diffusion model explicitly with the Score-based Generative Modeling perspective
through Tweedie's Formula. Lastly, we cover how to learn a conditional
distribution using diffusion models via guidance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## AT-DDPM: Restoring Faces degraded by Atmospheric Turbulence using Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 24, 2022 </span>    
         <span class="authors"> Nithin Gopalakrishnan Nair, Kangfu Mei, Vishal M. Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.11284" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Although many long-range imaging systems are designed to support extended
vision applications, a natural obstacle to their operation is degradation due
to atmospheric turbulence. Atmospheric turbulence causes significant
degradation to image quality by introducing blur and geometric distortion. In
recent years, various deep learning-based single image atmospheric turbulence
mitigation methods, including CNN-based and GAN inversion-based, have been
proposed in the literature which attempt to remove the distortion in the image.
However, some of these methods are difficult to train and often fail to
reconstruct facial features and produce unrealistic results especially in the
case of high turbulence. Denoising Diffusion Probabilistic Models (DDPMs) have
recently gained some traction because of their stable training process and
their ability to generate high quality images. In this paper, we propose the
first DDPM-based solution for the problem of atmospheric turbulence mitigation.
We also propose a fast sampling technique for reducing the inference times for
conditional DDPMs. Extensive experiments are conducted on synthetic and
real-world data to show the significance of our model. To facilitate further
research, all codes and pretrained models are publically available at
http://github.com/Nithin-GK/AT-DDPM
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 21, 2022 </span>    
         <span class="authors"> Jiachen Sun, Weili Nie, Zhiding Yu, Z. Morley Mao, Chaowei Xiao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.09801" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.CR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 3D Point cloud is becoming a critical data representation in many real-world
applications like autonomous driving, robotics, and medical imaging. Although
the success of deep learning further accelerates the adoption of 3D point
clouds in the physical world, deep learning is notorious for its vulnerability
to adversarial attacks. In this work, we first identify that the
state-of-the-art empirical defense, adversarial training, has a major
limitation in applying to 3D point cloud models due to gradient obfuscation. We
further propose PointDP, a purification strategy that leverages diffusion
models to defend against 3D adversarial attacks. We extensively evaluate
PointDP on six representative 3D point cloud architectures, and leverage 10+
strong and adaptive attacks to demonstrate its lower-bound robustness. Our
evaluation shows that PointDP achieves significantly better robustness than
state-of-the-art purification methods under strong attacks. Results of
certified defenses on randomized smoothing combined with PointDP will be
included in the near future.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## TopoDiff: A Performance and Constraint-Guided Diffusion Model for Topology Optimization
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 20, 2022 </span>    
         <span class="authors"> François Mazé, Faez Ahmed </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.09591" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CE
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Structural topology optimization, which aims to find the optimal physical
structure that maximizes mechanical performance, is vital in engineering design
applications in aerospace, mechanical, and civil engineering. Generative
adversarial networks (GANs) have recently emerged as a popular alternative to
traditional iterative topology optimization methods. However, these models are
often difficult to train, have limited generalizability, and due to their goal
of mimicking optimal structures, neglect manufacturability and performance
objectives like mechanical compliance. We propose TopoDiff - a conditional
diffusion-model-based architecture to perform performance-aware and
manufacturability-aware topology optimization that overcomes these issues. Our
model introduces a surrogate model-based guidance strategy that actively favors
structures with low compliance and good manufacturability. Our method
significantly outperforms a state-of-art conditional GAN by reducing the
average error on physical performance by a factor of eight and by producing
eleven times fewer infeasible samples. By introducing diffusion models to
topology optimization, we show that conditional diffusion models have the
ability to outperform GANs in engineering design synthesis applications too.
Our work also suggests a general framework for engineering optimization
problems using diffusion models and external performance with constraint-aware
guidance. We publicly share the data, code, and trained models here:
https://decode.mit.edu/projects/topodiff/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 19, 2022 </span>    
         <span class="authors"> Juan Miguel Lopez Alcaraz, Nils Strodthoff </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.09399" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The imputation of missing values represents a significant obstacle for many
real-world data analysis pipelines. Here, we focus on time series data and put
forward SSSD, an imputation model that relies on two emerging technologies,
(conditional) diffusion models as state-of-the-art generative models and
structured state space models as internal model architecture, which are
particularly suited to capture long-term dependencies in time series data. We
demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic
imputation and forecasting performance on a broad range of data sets and
different missingness scenarios, including the challenging blackout-missing
scenarios, where prior approaches failed to provide meaningful results.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 19, 2022 </span>    
         <span class="authors"> Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.09392" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Standard diffusion models involve an image transform -- adding Gaussian noise
-- and an image restoration operator that inverts this degradation. We observe
that the generative behavior of diffusion models is not strongly dependent on
the choice of image degradation, and in fact an entire family of generative
models can be constructed by varying this choice. Even when using completely
deterministic degradations (e.g., blur, masking, and more), the training and
test-time update rules that underlie diffusion models can be easily generalized
to create generative models. The success of these fully deterministic models
calls into question the community's understanding of diffusion models, which
relies on noise in either gradient Langevin dynamics or variational inference,
and paves the way for generalized diffusion models that invert arbitrary
processes. Our code is available at
https://github.com/arpitbansal297/Cold-Diffusion-Models
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Vector Quantized Diffusion Model with CodeUnet for Text-to-Sign Pose Sequences Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 19, 2022 </span>    
         <span class="authors"> Pan Xie, Qipeng Zhang, Zexian Li, Hao Tang, Yao Du, Xiaohui Hu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.09141" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Sign Language Production (SLP) aims to translate spoken languages into sign
sequences automatically. The core process of SLP is to transform sign gloss
sequences into their corresponding sign pose sequences (G2P). Most existing G2P
models usually perform this conditional long-range generation in an
autoregressive manner, which inevitably leads to an accumulation of errors. To
address this issue, we propose a vector quantized diffusion method for
conditional pose sequences generation, called PoseVQ-Diffusion, which is an
iterative non-autoregressive method. Specifically, we first introduce a vector
quantized variational autoencoder (Pose-VQVAE) model to represent a pose
sequence as a sequence of latent codes. Then we model the latent discrete space
by an extension of the recently developed diffusion architecture. To better
leverage the spatial-temporal information, we introduce a novel architecture,
namely CodeUnet, to generate higher quality pose sequence in the discrete
space. Moreover, taking advantage of the learned codes, we develop a novel
sequential k-nearest-neighbours method to predict the variable lengths of pose
sequences for corresponding gloss sequences. Consequently, compared with the
autoregressive G2P models, our model has a faster sampling speed and produces
significantly better results. Compared with previous non-autoregressive G2P
methods, PoseVQ-Diffusion improves the predicted results with iterative
refinements, thus achieving state-of-the-art results on the SLP evaluation
benchmark.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 18, 2022 </span>    
         <span class="authors"> Bahjat Kawar, Roy Ganz, Michael Elad </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.08664" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) are a recent family of
generative models that achieve state-of-the-art results. In order to obtain
class-conditional generation, it was suggested to guide the diffusion process
by gradients from a time-dependent classifier. While the idea is theoretically
sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers
may achieve good accuracy scores, their gradients are possibly unreliable and
might hinder the improvement of the generation results. Recent work discovered
that adversarially robust classifiers exhibit gradients that are aligned with
human perception, and these could better guide a generative process towards
semantically meaningful images. We utilize this observation by defining and
training a time-dependent adversarially robust classifier and use it as
guidance for a generative diffusion model. In experiments on the highly
challenging and diverse ImageNet dataset, our scheme introduces significantly
more intelligible intermediate gradients, better alignment with theoretical
findings, as well as improved generation results under several evaluation
metrics. Furthermore, we conduct an opinion survey whose findings indicate that
human raters prefer our method's results.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Your ViT is Secretly a Hybrid Discriminative-Generative Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 16, 2022 </span>    
         <span class="authors"> Xiulong Yang, Sheng-Min Shih, Yinlin Fu, Xiaoting Zhao, Shihao Ji </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.07791" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Denoising Probability Models (DDPM) and Vision Transformer (ViT)
have demonstrated significant progress in generative tasks and discriminative
tasks, respectively, and thus far these models have largely been developed in
their own domains. In this paper, we establish a direct connection between DDPM
and ViT by integrating the ViT architecture into DDPM, and introduce a new
generative model called Generative ViT (GenViT). The modeling flexibility of
ViT enables us to further extend GenViT to hybrid discriminative-generative
modeling, and introduce a Hybrid ViT (HybViT). Our work is among the first to
explore a single ViT for image generation and classification jointly. We
conduct a series of experiments to analyze the performance of proposed models
and demonstrate their superiority over prior state-of-the-arts in both
generative and discriminative tasks. Our code and pre-trained models can be
found in https://github.com/sndnyang/Diffusion_ViT .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Langevin Diffusion Variational Inference
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 16, 2022 </span>    
         <span class="authors"> Tomas Geffner, Justin Domke </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.07743" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Many methods that build powerful variational distributions based on
unadjusted Langevin transitions exist. Most of these were developed using a
wide range of different approaches and techniques. Unfortunately, the lack of a
unified analysis and derivation makes developing new methods and reasoning
about existing ones a challenging task. We address this giving a single
analysis that unifies and generalizes these existing techniques. The main idea
is to augment the target and variational by numerically simulating the
underdamped Langevin diffusion process and its time reversal. The benefits of
this approach are twofold: it provides a unified formulation for many existing
methods, and it simplifies the development of new ones. In fact, using our
formulation we propose a new method that combines the strengths of previously
existing algorithms; it uses underdamped Langevin transitions and powerful
augmentations parameterized by a score network. Our empirical evaluation shows
that our proposed method consistently outperforms relevant baselines in a wide
range of tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Diffusion meets Annealed Importance Sampling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 16, 2022 </span>    
         <span class="authors"> Arnaud Doucet, Will Grathwohl, Alexander G. D. G. Matthews, Heiko Strathmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.07698" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 More than twenty years after its introduction, Annealed Importance Sampling
(AIS) remains one of the most effective methods for marginal likelihood
estimation. It relies on a sequence of distributions interpolating between a
tractable initial distribution and the target distribution of interest which we
simulate from approximately using a non-homogeneous Markov chain. To obtain an
importance sampling estimate of the marginal likelihood, AIS introduces an
extended target distribution to reweight the Markov chain proposal. While much
effort has been devoted to improving the proposal distribution used by AIS, an
underappreciated issue is that AIS uses a convenient but suboptimal extended
target distribution. We here leverage recent progress in score-based generative
modeling (SGM) to approximate the optimal extended target distribution
minimizing the variance of the marginal likelihood estimate for AIS proposals
corresponding to the discretization of Langevin and Hamiltonian dynamics. We
demonstrate these novel, differentiable, AIS procedures on a number of
synthetic benchmark distributions and variational auto-encoders.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Applying Regularized Schrödinger-Bridge-Based Stochastic Process in Generative Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 15, 2022 </span>    
         <span class="authors"> Ki-Ung Song </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.07131" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Compared to the existing function-based models in deep generative modeling,
the recently proposed diffusion models have achieved outstanding performance
with a stochastic-process-based approach. But a long sampling time is required
for this approach due to many timesteps for discretization. Schr\"odinger
bridge (SB)-based models attempt to tackle this problem by training
bidirectional stochastic processes between distributions. However, they still
have a slow sampling speed compared to generative models such as generative
adversarial networks. And due to the training of the bidirectional stochastic
processes, they require a relatively long training time. Therefore, this study
tried to reduce the number of timesteps and training time required and proposed
regularization terms to the existing SB models to make the bidirectional
stochastic processes consistent and stable with a reduced number of timesteps.
Each regularization term was integrated into a single term to enable more
efficient training in computation time and memory usage. Applying this
regularized stochastic process to various generation tasks, the desired
translations between different distributions were obtained, and accordingly,
the possibility of generative modeling based on a stochastic process with
faster sampling speed could be confirmed. The code is available at
https://github.com/KiUngSong/RSB.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 12, 2022 </span>    
         <span class="authors"> Zhendong Wang, Jonathan J Hunt, Mingyuan Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.06193" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Offline reinforcement learning (RL), which aims to learn an optimal policy
using a previously collected static dataset, is an important paradigm of RL.
Standard RL methods often perform poorly in this regime due to the function
approximation errors on out-of-distribution actions. While a variety of
regularization methods have been proposed to mitigate this issue, they are
often constrained by policy classes with limited expressiveness that can lead
to highly suboptimal solutions. In this paper, we propose representing the
policy as a diffusion model, a recent class of highly-expressive deep
generative models. We introduce Diffusion Q-learning (Diffusion-QL) that
utilizes a conditional diffusion model to represent the policy. In our
approach, we learn an action-value function and we add a term maximizing
action-values into the training loss of the conditional diffusion model, which
results in a loss that seeks optimal actions that are near the behavior policy.
We show the expressiveness of the diffusion model-based policy, and the
coupling of the behavior cloning and policy improvement under the diffusion
model both contribute to the outstanding performance of Diffusion-QL. We
illustrate the superiority of our method compared to prior works in a simple 2D
bandit example with a multimodal behavior policy. We then show that our method
can achieve state-of-the-art performance on the majority of the D4RL benchmark
tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Convergence of denoising diffusion models under the manifold hypothesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 10, 2022 </span>    
         <span class="authors"> Valentin De Bortoli </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.05314" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, math.PR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models are a recent class of generative models exhibiting
state-of-the-art performance in image and audio synthesis. Such models
approximate the time-reversal of a forward noising process from a target
distribution to a reference density, which is usually Gaussian. Despite their
strong empirical results, the theoretical analysis of such models remains
limited. In particular, all current approaches crucially assume that the target
density admits a density w.r.t. the Lebesgue measure. This does not cover
settings where the target distribution is supported on a lower-dimensional
manifold or is given by some empirical distribution. In this paper, we bridge
this gap by providing the first convergence results for diffusion models in
this more general setting. In particular, we provide quantitative bounds on the
Wasserstein distance of order one between the target data distribution and the
generative distribution of the diffusion model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Wavelet Score-Based Generative Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 09, 2022 </span>    
         <span class="authors"> Florentin Guth, Simon Coste, Valentin De Bortoli, Stephane Mallat </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.05003" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) synthesize new data samples from
Gaussian white noise by running a time-reversed Stochastic Differential
Equation (SDE) whose drift coefficient depends on some probabilistic score. The
discretization of such SDEs typically requires a large number of time steps and
hence a high computational cost. This is because of ill-conditioning properties
of the score that we analyze mathematically. We show that SGMs can be
considerably accelerated, by factorizing the data distribution into a product
of conditional probabilities of wavelet coefficients across scales. The
resulting Wavelet Score-based Generative Model (WSGM) synthesizes wavelet
coefficients with the same number of time steps at all scales, and its time
complexity therefore grows linearly with the image size. This is proved
mathematically over Gaussian distributions, and shown numerically over physical
processes at phase transition and natural image datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 08, 2022 </span>    
         <span class="authors"> Ting Chen, Ruixiang Zhang, Geoffrey Hinton </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.04202" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Bit Diffusion: a simple and generic approach for generating
discrete data with continuous state and continuous time diffusion models. The
main idea behind our approach is to first represent the discrete data as binary
bits, and then train a continuous diffusion model to model these bits as real
numbers which we call analog bits. To generate samples, the model first
generates the analog bits, which are then thresholded to obtain the bits that
represent the discrete variables. We further propose two simple techniques,
namely Self-Conditioning and Asymmetric Time Intervals, which lead to a
significant improvement in sample quality. Despite its simplicity, the proposed
approach can achieve strong performance in both discrete image generation and
image captioning tasks. For discrete image generation, we significantly improve
previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)
and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the
best autoregressive model in both sample quality (measured by FID) and
efficiency. For image captioning on MS-COCO dataset, our approach achieves
competitive results compared to autoregressive models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Pyramidal Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 03, 2022 </span>    
         <span class="authors"> Dohoon Ryu, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2208.01864" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion model have demonstrated impressive image generation
performances, and have been extensively studied in various computer vision
tasks. Unfortunately, training and evaluating diffusion models consume a lot of
time and computational resources. To address this problem, here we present a
novel pyramidal diffusion model that can generate high resolution images
starting from much coarser resolution images using a {\em single} score
function trained with a positional embedding. This enables a neural network to
be much lighter and also enables time-efficient image generation without
compromising its performances. Furthermore, we show that the proposed approach
can be also efficiently used for multi-scale super-resolution problem using a
single score function.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 26, 2022 </span>    
         <span class="authors"> Robin Rombach, Andreas Blattmann, Björn Ommer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.13038" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Novel architectures have recently improved generative image synthesis leading
to excellent visual quality in various tasks. Of particular note is the field
of ``AI-Art'', which has seen unprecedented growth with the emergence of
powerful multimodal models such as CLIP. By combining speech and image
synthesis models, so-called ``prompt-engineering'' has become established, in
which carefully selected and composed sentences are used to achieve a certain
visual style in the synthesized image. In this note, we present an alternative
approach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set
of nearest neighbors is retrieved from an external database during training for
each training instance, and the diffusion model is conditioned on these
informative samples. During inference (sampling), we replace the retrieval
database with a more specialized database that contains, for example, only
images of a particular visual style. This provides a novel way to prompt a
general trained model after training and thereby specify a particular visual
style. As shown by our experiments, this approach is superior to specifying the
visual style within the text prompt. We open-source code and model weights at
https://github.com/CompVis/latent-diffusion .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Classifier-Free Diffusion Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 26, 2022 </span>    
         <span class="authors"> Jonathan Ho, Tim Salimans </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.12598" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Classifier guidance is a recently introduced method to trade off mode
coverage and sample fidelity in conditional diffusion models post training, in
the same spirit as low temperature sampling or truncation in other types of
generative models. Classifier guidance combines the score estimate of a
diffusion model with the gradient of an image classifier and thereby requires
training an image classifier separate from the diffusion model. It also raises
the question of whether guidance can be performed without a classifier. We show
that guidance can be indeed performed by a pure generative model without such a
classifier: in what we call classifier-free guidance, we jointly train a
conditional and an unconditional diffusion model, and we combine the resulting
conditional and unconditional score estimates to attain a trade-off between
sample quality and diversity similar to that obtained using classifier
guidance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Non-Uniform Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 20, 2022 </span>    
         <span class="authors"> Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.09786" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have emerged as one of the most promising frameworks for
deep generative modeling. In this work, we explore the potential of non-uniform
diffusion models. We show that non-uniform diffusion leads to multi-scale
diffusion models which have similar structure to this of multi-scale
normalizing flows. We experimentally find that in the same or less training
time, the multi-scale diffusion model achieves better FID score than the
standard uniform diffusion model. More importantly, it generates samples $4.4$
times faster in $128\times 128$ resolution. The speed-up is expected to be
higher in higher resolutions where more scales are used. Moreover, we show that
non-uniform diffusion leads to a novel estimator for the conditional score
function which achieves on par performance with the state-of-the-art
conditional denoising estimator. Our theoretical and experimental findings are
accompanied by an open source library MSDiff which can facilitate further
research of non-uniform diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unsupervised Medical Image Translation with Adversarial Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 17, 2022 </span>    
         <span class="authors"> Muzaffer Özbey, Onat Dalmaz, Salman UH Dar, Hasan A Bedel, Şaban Özturk, Alper Güngör, Tolga Çukur </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.08208" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Imputation of missing images via source-to-target modality translation can
improve diversity in medical imaging protocols. A pervasive approach for
synthesizing target images involves one-shot mapping through generative
adversarial networks (GAN). Yet, GAN models that implicitly characterize the
image distribution can suffer from limited sample fidelity. Here, we propose a
novel method based on adversarial diffusion modeling, SynDiff, for improved
performance in medical image translation. To capture a direct correlate of the
image distribution, SynDiff leverages a conditional diffusion process that
progressively maps noise and source images onto the target image. For fast and
accurate image sampling during inference, large diffusion steps are taken with
adversarial projections in the reverse diffusion direction. To enable training
on unpaired datasets, a cycle-consistent architecture is devised with coupled
diffusive and non-diffusive modules that bilaterally translate between two
modalities. Extensive assessments are reported on the utility of SynDiff
against competing GAN and diffusion models in multi-contrast MRI and MRI-CT
translation. Our demonstrations indicate that SynDiff offers quantitatively and
qualitatively superior performance against competing baselines.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Threat Model-Agnostic Adversarial Defense using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 17, 2022 </span>    
         <span class="authors"> Tsachi Blau, Roy Ganz, Bahjat Kawar, Alex Bronstein, Michael Elad </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.08089" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep Neural Networks (DNNs) are highly sensitive to imperceptible malicious
perturbations, known as adversarial attacks. Following the discovery of this
vulnerability in real-world imaging and vision applications, the associated
safety concerns have attracted vast research attention, and many defense
techniques have been developed. Most of these defense methods rely on
adversarial training (AT) -- training the classification network on images
perturbed according to a specific threat model, which defines the magnitude of
the allowed modification. Although AT leads to promising results, training on a
specific threat model fails to generalize to other types of perturbations. A
different approach utilizes a preprocessing step to remove the adversarial
perturbation from the attacked image. In this work, we follow the latter path
and aim to develop a technique that leads to robust classifiers across various
realizations of threat models. To this end, we harness the recent advances in
stochastic generative modeling, and means to leverage these for sampling from
conditional distributions. Our defense relies on an addition of Gaussian i.i.d
noise to the attacked image, followed by a pretrained diffusion process -- an
architecture that performs a stochastic iterative process over a denoising
network, yielding a high perceptual quality denoised outcome. The obtained
robustness with this stochastic preprocessing step is validated through
extensive experiments on the CIFAR-10 dataset, showing that our method
outperforms the leading defense methods under various threat models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 16, 2022 </span>    
         <span class="authors"> Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang Sun, Yebin Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.08000" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose DiffuStereo, a novel system using only sparse cameras (8 in this
work) for high-quality 3D human reconstruction. At its core is a novel
diffusion-based stereo module, which introduces diffusion models, a type of
powerful generative models, into the iterative stereo matching network. To this
end, we design a new diffusion kernel and additional stereo constraints to
facilitate stereo matching and depth estimation in the network. We further
present a multi-level stereo network architecture to handle high-resolution (up
to 4k) inputs without requiring unaffordable memory footprint. Given a set of
sparse-view color images of a human, the proposed multi-level diffusion-based
stereo network can produce highly accurate depth maps, which are then converted
into a high-quality 3D human model through an efficient multi-view fusion
strategy. Overall, our method enables automatic reconstruction of human models
with quality on par to high-end dense-view camera rigs, and this is achieved
using a much more light-weight hardware setup. Experiments show that our method
outperforms state-of-the-art methods by a large margin both qualitatively and
quantitatively.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 16, 2022 </span>    
         <span class="authors"> Sangyun Lee, Hyungjin Chung, Jaehyeon Kim, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.11192" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models have shown remarkable results in image synthesis
by gradually removing noise and amplifying signals. Although the simple
generative process surprisingly works well, is this the best way to generate
image data? For instance, despite the fact that human perception is more
sensitive to the low frequencies of an image, diffusion models themselves do
not consider any relative importance of each frequency component. Therefore, to
incorporate the inductive bias for image data, we propose a novel generative
process that synthesizes images in a coarse-to-fine manner. First, we
generalize the standard diffusion models by enabling diffusion in a rotated
coordinate system with different velocities for each component of the vector.
We further propose a blur diffusion as a special case, where each frequency
component of an image is diffused at different speeds. Specifically, the
proposed blur diffusion consists of a forward process that blurs an image and
adds noise gradually, after which a corresponding reverse process deblurs an
image and removes noise progressively. Experiments show that the proposed model
outperforms the previous method in FID on LSUN bedroom and church datasets.
Code is available at https://github.com/sangyun884/blur-diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 14, 2022 </span>    
         <span class="authors"> Min Zhao, Fan Bao, Chongxuan Li, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.06635" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based diffusion models (SBDMs) have achieved the SOTA FID results in
unpaired image-to-image translation (I2I). However, we notice that existing
methods totally ignore the training data in the source domain, leading to
sub-optimal solutions for unpaired I2I. To this end, we propose energy-guided
stochastic differential equations (EGSDE) that employs an energy function
pretrained on both the source and target domains to guide the inference process
of a pretrained SDE for realistic and faithful unpaired I2I. Building upon two
feature extractors, we carefully design the energy function such that it
encourages the transferred image to preserve the domain-independent features
and discard domain-specific ones. Further, we provide an alternative
explanation of the EGSDE as a product of experts, where each of the three
experts (corresponding to the SDE and two feature extractors) solely
contributes to faithfulness or realism. Empirically, we compare EGSDE to a
large family of baselines on three widely-adopted unpaired I2I tasks under four
metrics. EGSDE not only consistently outperforms existing SBDMs-based methods
in almost all settings but also achieves the SOTA realism results without
harming the faithful performance. Furthermore, EGSDE allows for flexible
trade-offs between realism and faithfulness and we improve the realism results
further (e.g., FID of 51.04 in Cat to Dog and FID of 50.43 in Wild to Dog on
AFHQ) by tuning hyper-parameters. The code is available at
https://github.com/ML-GSAI/EGSDE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Adaptive Diffusion Priors for Accelerated MRI Reconstruction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 12, 2022 </span>    
         <span class="authors"> Alper Güngör, Salman UH Dar, Şaban Öztürk, Yilmaz Korkmaz, Gokberk Elmas, Muzaffer Özbey, Tolga Çukur </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.05876" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep MRI reconstruction is commonly performed with conditional models that
de-alias undersampled acquisitions to recover images consistent with
fully-sampled data. Since conditional models are trained with knowledge of the
imaging operator, they can show poor generalization across variable operators.
Unconditional models instead learn generative image priors decoupled from the
imaging operator to improve reliability against domain shifts. Recent diffusion
models are particularly promising given their high sample fidelity.
Nevertheless, inference with a static image prior can perform suboptimally.
Here we propose the first adaptive diffusion prior for MRI reconstruction,
AdaDiff, to improve performance and reliability against domain shifts. AdaDiff
leverages an efficient diffusion prior trained via adversarial mapping over
large reverse diffusion steps. A two-phase reconstruction is executed following
training: a rapid-diffusion phase that produces an initial reconstruction with
the trained prior, and an adaptation phase that further refines the result by
updating the prior to minimize reconstruction loss on acquired data.
Demonstrations on multi-contrast brain MRI clearly indicate that AdaDiff
outperforms competing conditional and unconditional methods under domain
shifts, and achieves superior or on par within-domain performance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improving Diffusion Model Efficiency Through Patching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 09, 2022 </span>    
         <span class="authors"> Troy Luhman, Eric Luhman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.04316" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models are a powerful class of generative models that iteratively
denoise samples to produce data. While many works have focused on the number of
iterations in this sampling procedure, few have focused on the cost of each
iteration. We find that adding a simple ViT-style patching transformation can
considerably reduce a diffusion model's sampling time and memory usage. We
justify our approach both through an analysis of the diffusion model objective,
and through empirical experiments on LSUN Church, ImageNet 256, and FFHQ 1024.
We provide implementations in Tensorflow and Pytorch.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Back to the Source: Diffusion-Driven Test-Time Adaptation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 07, 2022 </span>    
         <span class="authors"> Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, Dequan Wang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.03442" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Test-time adaptation harnesses test inputs to improve the accuracy of a model
trained on source data when tested on shifted target data. Existing methods
update the source model by (re-)training on each target domain. While
effective, re-training is sensitive to the amount and order of the data and the
hyperparameters for optimization. We instead update the target data, by
projecting all test inputs toward the source domain with a generative diffusion
model. Our diffusion-driven adaptation method, DDA, shares its models for
classification and generation across all domains. Both models are trained on
the source domain, then fixed during testing. We augment diffusion with image
guidance and self-ensembling to automatically decide how much to adapt. Input
adaptation by DDA is more robust than prior model adaptation approaches across
a variety of corruptions, architectures, and data regimes on the ImageNet-C
benchmark. With its input-wise updates, DDA succeeds where model adaptation
degrades on too little data in small batches, dependent data in non-uniform
order, or mixed data with multiple corruptions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 07, 2022 </span>    
         <span class="authors"> Xiangxi Meng, Yuning Gu, Yongsheng Pan, Nizhuan Wang, Peng Xue, Mengkang Lu, Xuming He, Yiqiang Zhan, Dinggang Shen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.03430" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Multi-modal medical image completion has been extensively applied to
alleviate the missing modality issue in a wealth of multi-modal diagnostic
tasks. However, for most existing synthesis methods, their inferences of
missing modalities can collapse into a deterministic mapping from the available
ones, ignoring the uncertainties inherent in the cross-modal relationships.
Here, we propose the Unified Multi-Modal Conditional Score-based Generative
Model (UMM-CSGM) to take advantage of Score-based Generative Model (SGM) in
modeling and stochastically sampling a target probability distribution, and
further extend SGM to cross-modal conditional synthesis for various
missing-modality configurations in a unified framework. Specifically, UMM-CSGM
employs a novel multi-in multi-out Conditional Score Network (mm-CSN) to learn
a comprehensive set of cross-modal conditional distributions via conditional
diffusion and reverse generation in the complete modality space. In this way,
the generation process can be accurately conditioned by all available
information, and can fit all possible configurations of missing modalities in a
single network. Experiments on BraTS19 dataset show that the UMM-CSGM can more
reliably synthesize the heterogeneous enhancement and irregular area in
tumor-induced lesions for any missing modalities.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Riemannian Diffusion Schrödinger Bridge
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 07, 2022 </span>    
         <span class="authors"> James Thornton, Michael Hutchinson, Emile Mathieu, Valentin De Bortoli, Yee Whye Teh, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.03024" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models exhibit state of the art performance on density
estimation and generative modeling tasks. These models typically assume that
the data geometry is flat, yet recent extensions have been developed to
synthesize data living on Riemannian manifolds. Existing methods to accelerate
sampling of diffusion models are typically not applicable in the Riemannian
setting and Riemannian score-based methods have not yet been adapted to the
important task of interpolation of datasets. To overcome these issues, we
introduce \emph{Riemannian Diffusion Schr\"odinger Bridge}. Our proposed method
generalizes Diffusion Schr\"odinger Bridge introduced in
\cite{debortoli2021neurips} to the non-Euclidean setting and extends Riemannian
score-based models beyond the first time reversal. We validate our proposed
method on synthetic data and real Earth and climate data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Semantic Image Synthesis via Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 30, 2022 </span>    
         <span class="authors"> Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2207.00050" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable
success in various image generation tasks compared with Generative Adversarial
Nets (GANs). Recent work on semantic image synthesis mainly follows the
\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality
or diversity of generated images. In this paper, we propose a novel framework
based on DDPM for semantic image synthesis. Unlike previous conditional
diffusion model directly feeds the semantic layout and noisy image as input to
a U-Net structure, which may not fully leverage the information in the input
semantic mask, our framework processes semantic layout and noisy image
differently. It feeds noisy image to the encoder of the U-Net structure while
the semantic layout to the decoder by multi-layer spatially-adaptive
normalization operators. To further improve the generation quality and semantic
interpretability in semantic image synthesis, we introduce the classifier-free
guidance sampling strategy, which acknowledge the scores of an unconditional
model for sampling process. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our proposed method, achieving
state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SPI-GAN: Distilling Score-based Generative Models with Straight-Path Interpolations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 29, 2022 </span>    
         <span class="authors"> Jinsung Jeon, Noseong Park </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.14464" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) are a recently proposed paradigm for
deep generative tasks and now show the state-of-the-art sampling performance.
It is known that the original SGM design solves the two problems of the
generative trilemma: i) sampling quality, and ii) sampling diversity. However,
the last problem of the trilemma was not solved, i.e., their training/sampling
complexity is notoriously high. To this end, distilling SGMs into simpler
models, e.g., generative adversarial networks (GANs), is gathering much
attention currently. We present an enhanced distillation method, called
straight-path interpolation GAN (SPI-GAN), which can be compared to the
state-of-the-art shortcut-based distillation method, called denoising diffusion
GAN (DD-GAN). However, our method corresponds to an extreme method that does
not use any intermediate shortcut information of the reverse SDE path, in which
case DD-GAN fails to obtain good results. Nevertheless, our straight-path
interpolation method greatly stabilizes the overall training process. As a
result, SPI-GAN is one of the best models in terms of the sampling
quality/diversity/time for CIFAR-10, CelebA-HQ-256, and LSUN-Church-256.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Deformable Model for 4D Temporal Medical Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 27, 2022 </span>    
         <span class="authors"> Boah Kim, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.13295" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Temporal volume images with 3D+t (4D) information are often used in medical
imaging to statistically analyze temporal dynamics or capture disease
progression. Although deep-learning-based generative models for natural images
have been extensively studied, approaches for temporal medical image generation
such as 4D cardiac volume data are limited. In this work, we present a novel
deep learning model that generates intermediate temporal volumes between source
and target volumes. Specifically, we propose a diffusion deformable model (DDM)
by adapting the denoising diffusion probabilistic model that has recently been
widely investigated for realistic image generation. Our proposed DDM is
composed of the diffusion and the deformation modules so that DDM can learn
spatial deformation information between the source and target volumes and
provide a latent code for generating intermediate frames along a geodesic path.
Once our model is trained, the latent code estimated from the diffusion module
is simply interpolated and fed into the deformation module, which enables DDM
to generate temporal frames along the continuous trajectory while preserving
the topology of the source image. We demonstrate the proposed method with the
4D cardiac MR image generation between the diastolic and systolic phases for
each subject. Compared to the existing deformation methods, our DDM achieves
high performance on temporal volume generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DDPM-CD: Remote Sensing Change Detection using Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 23, 2022 </span>    
         <span class="authors"> Wele Gedara Chaminda Bandara, Nithin Gopalakrishnan Nair, Vishal M. Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.11892" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Human civilization has an increasingly powerful influence on the earth
system, and earth observations are an invaluable tool for assessing and
mitigating the negative impacts. To this end, observing precisely defined
changes on Earth's surface is essential, and we propose an effective way to
achieve this goal. Notably, our change detection (CD)/ segmentation method
proposes a novel way to incorporate the millions of off-the-shelf, unlabeled,
remote sensing images available through different earth observation programs
into the training process through denoising diffusion probabilistic models. We
first leverage the information from these off-the-shelf, uncurated, and
unlabeled remote sensing images by using a pre-trained denoising diffusion
probabilistic model and then employ the multi-scale feature representations
from the diffusion model decoder to train a lightweight CD classifier to detect
precise changes. The experiments performed on four publically available CD
datasets show that the proposed approach achieves remarkably better results
than the state-of-the-art methods in F1, IoU, and overall accuracy. Code and
pre-trained models are available at: https://github.com/wgcban/ddpm-cd
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Guided Diffusion Model for Adversarial Purification from Random Noise
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 22, 2022 </span>    
         <span class="authors"> Quanlin Wu, Hang Ye, Yuntian Gu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.10875" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we propose a novel guided diffusion purification approach to
provide a strong defense against adversarial attacks. Our model achieves 89.62%
robust accuracy under PGD-L_inf attack (eps = 8/255) on the CIFAR-10 dataset.
We first explore the essential correlations between unguided diffusion models
and randomized smoothing, enabling us to apply the models to certified
robustness. The empirical results show that our models outperform randomized
smoothing by 5% when the certified L2 radius r is larger than 0.5.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## (Certified!!) Adversarial Robustness for Free!
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 21, 2022 </span>    
         <span class="authors"> Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, J. Zico Kolter </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.10550" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper we show how to achieve state-of-the-art certified adversarial
robustness to 2-norm bounded perturbations by relying exclusively on
off-the-shelf pretrained models. To do so, we instantiate the denoised
smoothing approach of Salman et al. 2020 by combining a pretrained denoising
diffusion probabilistic model and a standard high-accuracy classifier. This
allows us to certify 71% accuracy on ImageNet under adversarial perturbations
constrained to be within an 2-norm of 0.5, an improvement of 14 percentage
points over the prior certified SoTA using any approach, or an improvement of
30 percentage points over denoised smoothing. We obtain these results using
only pretrained diffusion models and image classifiers, without requiring any
fine tuning or retraining of model parameters.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Faster Diffusion Cardiac MRI with Deep Learning-based breath hold reduction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 21, 2022 </span>    
         <span class="authors"> Michael Tanzer, Pedro Ferreira, Andrew Scott, Zohya Khalique, Maria Dwornik, Dudley Pennell, Guang Yang, Daniel Rueckert, Sonia Nielles-Vallespin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.10543" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) enables us to probe the
microstructural arrangement of cardiomyocytes within the myocardium in vivo and
non-invasively, which no other imaging modality allows. This innovative
technology could revolutionise the ability to perform cardiac clinical
diagnosis, risk stratification, prognosis and therapy follow-up. However,
DT-CMR is currently inefficient with over six minutes needed to acquire a
single 2D static image. Therefore, DT-CMR is currently confined to research but
not used clinically. We propose to reduce the number of repetitions needed to
produce DT-CMR datasets and subsequently de-noise them, decreasing the
acquisition time by a linear factor while maintaining acceptable image quality.
Our proposed approach, based on Generative Adversarial Networks, Vision
Transformers, and Ensemble Learning, performs significantly and considerably
better than previous proposed approaches, bringing single breath-hold DT-CMR
closer to reality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generative Modelling With Inverse Heat Dissipation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 21, 2022 </span>    
         <span class="authors"> Severi Rissanen, Markus Heinonen, Arno Solin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.13397" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While diffusion models have shown great success in image generation, their
noise-inverting generative process does not explicitly consider the structure
of images, such as their inherent multi-scale nature. Inspired by diffusion
models and the empirical success of coarse-to-fine modelling, we propose a new
diffusion-like model that generates images through stochastically reversing the
heat equation, a PDE that locally erases fine-scale information when run over
the 2D plane of the image. We interpret the solution of the forward heat
equation with constant additive noise as a variational approximation in the
diffusion latent variable model. Our new model shows emergent qualitative
properties not seen in standard diffusion models, such as disentanglement of
overall colour and shape in images. Spectral analysis on natural images
highlights connections to diffusion models and reveals an implicit
coarse-to-fine inductive bias in them.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 18, 2022 </span>    
         <span class="authors"> Giannis Daras, Yuval Dagan, Alexandros G. Dimakis, Constantinos Daskalakis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.09104" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We prove fast mixing and characterize the stationary distribution of the
Langevin Algorithm for inverting random weighted DNN generators. This result
extends the work of Hand and Voroninski from efficient inversion to efficient
posterior sampling. In practice, to allow for increased expressivity, we
propose to do posterior sampling in the latent space of a pre-trained
generative model. To achieve that, we train a score-based model in the latent
space of a StyleGAN-2 and we use it to solve inverse problems. Our framework,
Score-Guided Intermediate Layer Optimization (SGILO), extends prior work by
replacing the sparsity regularization with a generative prior in the
intermediate layer. Experimentally, we obtain significant improvements over the
previous state-of-the-art, especially in the low measurement regime.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion models as plug-and-play priors
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 17, 2022 </span>    
         <span class="authors"> Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, Dimitris Samaras </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.09012" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We consider the problem of inferring high-dimensional data $\mathbf{x}$ in a
model that consists of a prior $p(\mathbf{x})$ and an auxiliary differentiable
constraint $c(\mathbf{x},\mathbf{y})$ on $x$ given some additional information
$\mathbf{y}$. In this paper, the prior is an independently trained denoising
diffusion generative model. The auxiliary constraint is expected to have a
differentiable form, but can come from diverse sources. The possibility of such
inference turns diffusion models into plug-and-play modules, thereby allowing a
range of potential applications in adapting models to new domains and tasks,
such as conditional generation or image segmentation. The structure of
diffusion models allows us to perform approximate inference by iterating
differentiation through the fixed denoising network enriched with different
amounts of noise at each step. Considering many noised versions of $\mathbf{x}$
in evaluation of its fitness is a novel search mechanism that may lead to new
algorithms for solving combinatorial optimization problems.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Generative Models for Calorimeter Shower Simulation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 17, 2022 </span>    
         <span class="authors"> Vinicius Mikuni, Benjamin Nachman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.11898" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  hep-ph, cs.LG, hep-ex, physics.data-an, physics.ins-det
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models are a new class of generative algorithms that
have been shown to produce realistic images even in high dimensional spaces,
currently surpassing other state-of-the-art models for different benchmark
categories and applications. In this work we introduce CaloScore, a score-based
generative model for collider physics applied to calorimeter shower generation.
Three different diffusion models are investigated using the Fast Calorimeter
Simulation Challenge 2022 dataset. CaloScore is the first application of a
score-based generative model in collider physics and is able to produce
high-fidelity calorimeter images for all datasets, providing an alternative
paradigm for calorimeter shower simulation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Lossy Compression with Gaussian Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 17, 2022 </span>    
         <span class="authors"> Lucas Theis, Tim Salimans, Matthew D. Hoffman, Fabian Mentzer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.08889" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.IT, cs.LG, math.IT
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We consider a novel lossy compression approach based on unconditional
diffusion generative models, which we call DiffC. Unlike modern compression
schemes which rely on transform coding and quantization to restrict the
transmitted information, DiffC relies on the efficient communication of pixels
corrupted by Gaussian noise. We implement a proof of concept and find that it
works surprisingly well despite the lack of an encoder transform, outperforming
the state-of-the-art generative compression method HiFiC on ImageNet 64x64.
DiffC only uses a single model to encode and denoise corrupted pixels at
arbitrary bitrates. The approach further provides support for progressive
coding, that is, decoding from partial bit streams. We perform a
rate-distortion analysis to gain a deeper understanding of its performance,
providing analytical results for multivariate Gaussian data as well as
theoretic bounds for general distributions. Furthermore, we prove that a
flow-based reconstruction achieves a 3 dB gain over ancestral sampling at high
bitrates.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Flexible Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 17, 2022 </span>    
         <span class="authors"> Weitao Du, Tao Yang, He Zhang, Yuanqi Du </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.10365" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion (score-based) generative models have been widely used for modeling
various types of complex data, including images, audios, and point clouds.
Recently, the deep connection between forward-backward stochastic differential
equations (SDEs) and diffusion-based models has been revealed, and several new
variants of SDEs are proposed (e.g., sub-VP, critically-damped Langevin) along
this line. Despite the empirical success of the hand-crafted fixed forward
SDEs, a great quantity of proper forward SDEs remain unexplored. In this work,
we propose a general framework for parameterizing the diffusion model,
especially the spatial part of the forward SDE. An abstract formalism is
introduced with theoretical guarantees, and its connection with previous
diffusion models is leveraged. We demonstrate the theoretical advantage of our
method from an optimization perspective. Numerical experiments on synthetic
datasets, MINIST and CIFAR10 are also presented to validate the effectiveness
of our framework.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SOS: Score-based Oversampling for Tabular Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 17, 2022 </span>    
         <span class="authors"> Jayoung Kim, Chaejeong Lee, Yehjin Shin, Sewon Park, Minjung Kim, Noseong Park, Jihoon Cho </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.08555" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) are a recent breakthrough in generating
fake images. SGMs are known to surpass other generative models, e.g.,
generative adversarial networks (GANs) and variational autoencoders (VAEs).
Being inspired by their big success, in this work, we fully customize them for
generating fake tabular data. In particular, we are interested in oversampling
minor classes since imbalanced classes frequently lead to sub-optimal training
outcomes. To our knowledge, we are the first presenting a score-based tabular
data oversampling method. Firstly, we re-design our own score network since we
have to process tabular data. Secondly, we propose two options for our
generation method: the former is equivalent to a style transfer for tabular
data and the latter uses the standard generative policy of SGMs. Lastly, we
define a fine-tuning method, which further enhances the oversampling quality.
In our experiments with 6 datasets and 10 baselines, our method outperforms
other oversampling methods in all cases.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 16, 2022 </span>    
         <span class="authors"> Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.08265" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models have excellent performance in terms of
generation quality and likelihood. They model the data distribution by matching
a parameterized score network with first-order data score functions. The score
network can be used to define an ODE ("score-based diffusion ODE") for exact
likelihood evaluation. However, the relationship between the likelihood of the
ODE and the score matching objective is unclear. In this work, we prove that
matching the first-order score is not sufficient to maximize the likelihood of
the ODE, by showing a gap between the maximum likelihood and score matching
objectives. To fill up this gap, we show that the negative likelihood of the
ODE can be bounded by controlling the first, second, and third-order score
matching errors; and we further present a novel high-order denoising score
matching method to enable maximum likelihood training of score-based diffusion
ODEs. Our algorithm guarantees that the higher-order matching error is bounded
by the training error and the lower-order errors. We empirically observe that
by high-order score matching, score-based diffusion ODEs achieve better
likelihood on both synthetic data and CIFAR-10, while retaining the high
generation quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 15, 2022 </span>    
         <span class="authors"> Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, Yan Yan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.07771" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPMs) have become a popular approach to
conditional generation, due to their promising results and support for
cross-modal synthesis. A key desideratum in conditional synthesis is to achieve
high correspondence between the conditioning input and generated output. Most
existing methods learn such relationships implicitly, by incorporating the
prior into the variational lower bound. In this work, we take a different route
-- we explicitly enhance input-output connections by maximizing their mutual
information. To this end, we introduce a Conditional Discrete Contrastive
Diffusion (CDCD) loss and design two contrastive diffusion mechanisms to
effectively incorporate it into the denoising process, combining the diffusion
training and contrastive learning for the first time by connecting it with the
conventional variational objectives. We demonstrate the efficacy of our
approach in evaluations with diverse multimodal conditional synthesis tasks:
dance-to-music generation, text-to-image synthesis, as well as
class-conditioned image synthesis. On each, we enhance the input-output
correspondence and achieve higher or competitive general synthesis quality.
Furthermore, the proposed approach improves the convergence of diffusion
models, reducing the number of required diffusion steps by more than 35% on two
benchmarks, significantly increasing the inference speed.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for Video Prediction and Infilling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 15, 2022 </span>    
         <span class="authors"> Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, Andrea Dittadi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.07696" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Predicting and anticipating future outcomes or reasoning about missing
information in a sequence are critical skills for agents to be able to make
intelligent decisions. This requires strong, temporally coherent generative
capabilities. Diffusion models have shown remarkable success in several
generative tasks, but have not been extensively explored in the video domain.
We present Random-Mask Video Diffusion (RaMViD), which extends image diffusion
models to videos using 3D convolutions, and introduces a new conditioning
technique during training. By varying the mask we condition on, the model is
able to perform video prediction, infilling, and upsampling. Due to our simple
conditioning scheme, we can utilize the same architecture as used for
unconditional training, which allows us to train the model in a conditional and
unconditional fashion at the same time. We evaluate RaMViD on two benchmark
datasets for video prediction, on which we achieve state-of-the-art results,
and one for video generation. High-resolution videos are provided at
https://sites.google.com/view/video-diffusion-prediction.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 15, 2022 </span>    
         <span class="authors"> Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, Bo Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.07309" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPMs) are a class of powerful deep generative
models (DGMs). Despite their success, the iterative generation process over the
full timesteps is much less efficient than other DGMs such as GANs. Thus, the
generation performance on a subset of timesteps is crucial, which is greatly
influenced by the covariance design in DPMs. In this work, we consider diagonal
and full covariances to improve the expressive power of DPMs. We derive the
optimal result for such covariances, and then correct it when the mean of DPMs
is imperfect. Both the optimal and the corrected ones can be decomposed into
terms of conditional expectations over functions of noise. Building upon it, we
propose to estimate the optimal covariance and its correction given imperfect
mean by learning these conditional expectations. Our method can be applied to
DPMs with both discrete and continuous timesteps. We consider the diagonal
covariance in our implementation for computational efficiency. For an efficient
practical implementation, we adopt a parameter sharing scheme and a two-stage
training process. Empirically, our method outperforms a wide variety of
covariance design on likelihood results, and improves the sample quality
especially on a small number of timesteps.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CARD: Classification and Regression Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 15, 2022 </span>    
         <span class="authors"> Xizewen Han, Huangjie Zheng, Mingyuan Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.07275" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, stat.CO, stat.ME
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Learning the distribution of a continuous or categorical response variable
$\boldsymbol y$ given its covariates $\boldsymbol x$ is a fundamental problem
in statistics and machine learning. Deep neural network-based supervised
learning algorithms have made great progress in predicting the mean of
$\boldsymbol y$ given $\boldsymbol x$, but they are often criticized for their
ability to accurately capture the uncertainty of their predictions. In this
paper, we introduce classification and regression diffusion (CARD) models,
which combine a denoising diffusion-based conditional generative model and a
pre-trained conditional mean estimator, to accurately predict the distribution
of $\boldsymbol y$ given $\boldsymbol x$. We demonstrate the outstanding
ability of CARD in conditional distribution prediction with both toy examples
and real-world datasets, the experimental results on which show that CARD in
general outperforms state-of-the-art methods, including Bayesian neural
network-based ones that are designed for uncertainty estimation, especially
when the conditional distribution of $\boldsymbol y$ given $\boldsymbol x$ is
multi-modal. In addition, we utilize the stochastic nature of the generative
model outputs to obtain a finer granularity in model confidence assessment at
the instance level for classification tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Realistic Gramophone Noise Synthesis using a Diffusion Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 13, 2022 </span>    
         <span class="authors"> Eloi Moliner, Vesa Välimäki </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.06259" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper introduces a novel data-driven strategy for synthesizing
gramophone noise audio textures. A diffusion probabilistic model is applied to
generate highly realistic quasiperiodic noises. The proposed model is designed
to generate samples of length equal to one disk revolution, but a method to
generate plausible periodic variations between revolutions is also proposed. A
guided approach is also applied as a conditioning method, where an audio signal
generated with manually-tuned signal processing is refined via reverse
diffusion to improve realism. The method has been evaluated in a subjective
listening test, in which the participants were often unable to recognize the
synthesized signals from the real ones. The synthetic noises produced with the
best proposed unconditional method are statistically indistinguishable from
real noise recordings. This work shows the potential of diffusion models for
highly realistic audio synthesis tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Convergence for score-based generative modeling with polynomial complexity
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 13, 2022 </span>    
         <span class="authors"> Holden Lee, Jianfeng Lu, Yixin Tan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.06227" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.PR, math.ST, stat.ML, stat.TH
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative modeling (SGM) is a highly successful approach for
learning a probability distribution from data and generating further samples.
We prove the first polynomial convergence guarantees for the core mechanic
behind SGM: drawing samples from a probability density $p$ given a score
estimate (an estimate of $\nabla \ln p$) that is accurate in $L^2(p)$. Compared
to previous works, we do not incur error that grows exponentially in time or
that suffers from a curse of dimensionality. Our guarantee works for any smooth
distribution and depends polynomially on its log-Sobolev constant. Using our
guarantee, we give a theoretical analysis of score-based generative modeling,
which transforms white-noise input into samples from a learned data
distribution given score estimates at different noise scales. Our analysis
gives theoretical grounding to the observation that an annealed procedure is
required in practice to generate good samples, as our proof depends essentially
on using annealing to obtain a warm start at each step. Moreover, we show that
a predictor-corrector algorithm gives better convergence than using either
portion alone.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Latent Diffusion Energy-Based Model for Interpretable Text Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 13, 2022 </span>    
         <span class="authors"> Peiyu Yu, Sirui Xie, Xiaojian Ma, Baoxiong Jia, Bo Pang, Ruiqi Gao, Yixin Zhu, Song-Chun Zhu, Ying Nian Wu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.05895" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CL
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Latent space Energy-Based Models (EBMs), also known as energy-based priors,
have drawn growing interests in generative modeling. Fueled by its flexibility
in the formulation and strong modeling power of the latent space, recent works
built upon it have made interesting attempts aiming at the interpretability of
text modeling. However, latent space EBMs also inherit some flaws from EBMs in
data space; the degenerate MCMC sampling quality in practice can lead to poor
generation quality and instability in training, especially on data with complex
latent structures. Inspired by the recent efforts that leverage diffusion
recovery likelihood learning as a cure for the sampling issue, we introduce a
novel symbiosis between the diffusion models and latent space EBMs in a
variational learning framework, coined as the latent diffusion energy-based
model. We develop a geometric clustering-based regularization jointly with the
information bottleneck to further improve the quality of the learned latent
space. Experiments on several challenging tasks demonstrate the superior
performance of our model on interpretable text modeling over strong
counterparts.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## gDDIM: Generalized denoising diffusion implicit models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 11, 2022 </span>    
         <span class="authors"> Qinsheng Zhang, Molei Tao, Yongxin Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.05564" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Our goal is to extend the denoising diffusion implicit model (DDIM) to
general diffusion models~(DMs) besides isotropic diffusions. Instead of
constructing a non-Markov noising process as in the original DDIM, we examine
the mechanism of DDIM from a numerical perspective. We discover that the DDIM
can be obtained by using some specific approximations of the score when solving
the corresponding stochastic differential equation. We present an
interpretation of the accelerating effects of DDIM that also explains the
advantages of a deterministic sampling scheme over the stochastic one for fast
sampling. Building on this insight, we extend DDIM to general DMs, coined
generalized DDIM (gDDIM), with a small but delicate modification in
parameterizing the score network. We validate gDDIM in two non-isotropic DMs:
Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model
(CLD). We observe more than 20 times acceleration in BDM. In the CLD, a
diffusion model by augmenting the diffusion process with velocity, our
algorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of
score function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs.
Code is available at https://github.com/qsh-zh/gDDIM
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Multi-instrument Music Synthesis with Spectrogram Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 11, 2022 </span>    
         <span class="authors"> Curtis Hawthorne, Ian Simon, Adam Roberts, Neil Zeghidour, Josh Gardner, Ethan Manilow, Jesse Engel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.05408" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 An ideal music synthesizer should be both interactive and expressive,
generating high-fidelity audio in realtime for arbitrary combinations of
instruments and notes. Recent neural synthesizers have exhibited a tradeoff
between domain-specific models that offer detailed control of only specific
instruments, or raw waveform models that can train on any music but with
minimal control and slow generation. In this work, we focus on a middle ground
of neural synthesizers that can generate audio from MIDI sequences with
arbitrary combinations of instruments in realtime. This enables training on a
wide range of transcription datasets with a single model, which in turn offers
note-level control of composition and instrumentation across a wide range of
instruments. We use a simple two-stage process: MIDI to spectrograms with an
encoder-decoder Transformer, then spectrograms to audio with a generative
adversarial network (GAN) spectrogram inverter. We compare training the decoder
as an autoregressive model and as a Denoising Diffusion Probabilistic Model
(DDPM) and find that the DDPM approach is superior both qualitatively and as
measured by audio reconstruction and Fr\'echet distance metrics. Given the
interactivity and generality of this approach, we find this to be a promising
first step towards interactive and expressive neural synthesis for arbitrary
combinations of instruments and notes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## How Much is Enough? A Study on Diffusion Times in Score-based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 10, 2022 </span>    
         <span class="authors"> Giulio Franzese, Simone Rossi, Lixuan Yang, Alessandro Finamore, Dario Rossi, Maurizio Filippone, Pietro Michiardi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.05173" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based diffusion models are a class of generative models whose dynamics
is described by stochastic differential equations that map noise into data.
While recent works have started to lay down a theoretical foundation for these
models, an analytical understanding of the role of the diffusion time T is
still lacking. Current best practice advocates for a large T to ensure that the
forward dynamics brings the diffusion sufficiently close to a known and simple
noise distribution; however, a smaller value of T should be preferred for a
better approximation of the score-matching objective and higher computational
efficiency. Starting from a variational interpretation of diffusion models, in
this work we quantify this trade-off, and suggest a new method to improve
quality and efficiency of both training and sampling, by adopting smaller
diffusion times. Indeed, we show how an auxiliary model can be used to bridge
the gap between the ideal and the simulated forward dynamics, followed by a
standard reverse diffusion process. Empirical results support our analysis; for
image data, our method is competitive w.r.t. the state-of-the-art, according to
standard sample quality metrics and log-likelihood.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Image Generation with Multimodal Priors using Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 10, 2022 </span>    
         <span class="authors"> Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M Patel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.05039" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image synthesis under multi-modal priors is a useful and challenging task
that has received increasing attention in recent years. A major challenge in
using generative models to accomplish this task is the lack of paired data
containing all modalities (i.e. priors) and corresponding outputs. In recent
work, a variational auto-encoder (VAE) model was trained in a weakly supervised
manner to address this challenge. Since the generative power of VAEs is usually
limited, it is difficult for this method to synthesize images belonging to
complex distributions. To this end, we propose a solution based on a denoising
diffusion probabilistic models to synthesise images under multi-model priors.
Based on the fact that the distribution over each time step in the diffusion
model is Gaussian, in this work we show that there exists a closed-form
expression to the generate the image corresponds to the given modalities. The
proposed solution does not require explicit retraining for all modalities and
can leverage the outputs of individual modalities to generate realistic images
according to different constraints. We conduct studies on two real-world
datasets to demonstrate the effectiveness of our approach
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Probability flow solution of the Fokker-Planck equation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 09, 2022 </span>    
         <span class="authors"> Nicholas M. Boffi, Eric Vanden-Eijnden </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.04642" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cond-mat.dis-nn, cond-mat.stat-mech, cs.NA, math.NA, math.PR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The method of choice for integrating the time-dependent Fokker-Planck
equation in high-dimension is to generate samples from the solution via
integration of the associated stochastic differential equation. Here, we study
an alternative scheme based on integrating an ordinary differential equation
that describes the flow of probability. Acting as a transport map, this
equation deterministically pushes samples from the initial density onto samples
from the solution at any later time. Unlike integration of the stochastic
dynamics, the method has the advantage of giving direct access to quantities
that are challenging to estimate from trajectories alone, such as the
probability current, the density itself, and its entropy. The probability flow
equation depends on the gradient of the logarithm of the solution (its
"score"), and so is a-priori unknown. To resolve this dependence, we model the
score with a deep neural network that is learned on-the-fly by propagating a
set of samples according to the instantaneous probability current. We show
theoretically that the proposed approach controls the KL divergence from the
learned solution to the target, while learning on external samples from the
stochastic differential equation does not control either direction of the KL
divergence. Empirically, we consider several high-dimensional Fokker-Planck
equations from the physics of interacting particle systems. We find that the
method accurately matches analytical solutions when they are available as well
as moments computed via Monte-Carlo when they are not. Moreover, the method
offers compelling predictions for the global entropy production rate that
out-perform those obtained from learning on stochastic trajectories, and can
effectively capture non-equilibrium steady-state probability currents over long
time intervals.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 08, 2022 </span>    
         <span class="authors"> Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.04119" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  q-bio.BM, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Construction of a scaffold structure that supports a desired motif,
conferring protein function, shows promise for the design of vaccines and
enzymes. But a general solution to this motif-scaffolding problem remains open.
Current machine-learning techniques for scaffold design are either limited to
unrealistically small scaffolds (up to length 20) or struggle to produce
multiple diverse scaffolds. We propose to learn a distribution over diverse and
longer protein backbone structures via an E(3)-equivariant graph neural
network. We develop SMCDiff to efficiently sample scaffolds from this
distribution conditioned on a given motif; our algorithm is the first to
theoretically guarantee conditional samples from a diffusion model in the
large-compute limit. We evaluate our designed backbones by how well they align
with AlphaFold2-predicted structures. We show that our method can (1) sample
scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for
a fixed motif.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Accelerating Score-based Generative Models for High-Resolution Image Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 08, 2022 </span>    
         <span class="authors"> Hengyuan Ma, Li Zhang, Xiatian Zhu, Jingfeng Zhang, Jianfeng Feng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.04029" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. The key idea is to produce high-quality images by
recurrently adding Gaussian noises and gradients to a Gaussian sample until
converging to the target distribution, a.k.a. the diffusion sampling. To ensure
stability of convergence in sampling and generation quality, however, this
sequential sampling process has to take a small step size and many sampling
iterations (e.g., 2000). Several acceleration methods have been proposed with
focus on low-resolution generation. In this work, we consider the acceleration
of high-resolution generation with SGMs, a more challenging yet more important
problem. We prove theoretically that this slow convergence drawback is
primarily due to the ignorance of the target distribution. Further, we
introduce a novel Target Distribution Aware Sampling (TDAS) method by
leveraging the structural priors in space and frequency domains. Extensive
experiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can
consistently accelerate state-of-the-art SGMs, particularly on more challenging
high resolution (1024x1024) image generation tasks by up to 18.4x, whilst
largely maintaining the synthesis quality. With fewer sampling iterations, TDAS
can still generate good quality images. In contrast, the existing methods
degrade drastically or even fails completely
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Neural Diffusion Processes
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 08, 2022 </span>    
         <span class="authors"> Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, Fergus Simpson </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.03992" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Gaussian processes provide an elegant framework for specifying prior and
posterior distributions over functions. They are, however, also computationally
expensive, and limited by the expressivity of their covariance function. We
propose Neural Diffusion Processes (NDPs), a novel approach based upon
diffusion models, that learn to sample from distributions over functions. Using
a novel attention block, we can incorporate properties of stochastic processes,
such as exchangeability, directly into the NDP's architecture. We empirically
show that NDPs are able to capture functional distributions that are close to
the true Bayesian posterior of a Gaussian process. This enables a variety of
downstream tasks, including hyperparameter marginalisation and Bayesian
optimisation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 07, 2022 </span>    
         <span class="authors"> Walter H. L. Pinaya, Mark S. Graham, Robert Gray, Pedro F Da Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H. Mah, Andrew D. MacKinnon, James T. Teo, Rolf Jager, David Werring, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.03461" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV, q-bio.QM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep generative models have emerged as promising tools for detecting
arbitrary anomalies in data, dispensing with the necessity for manual
labelling. Recently, autoregressive transformers have achieved state-of-the-art
performance for anomaly detection in medical imaging. Nonetheless, these models
still have some intrinsic weaknesses, such as requiring images to be modelled
as 1D sequences, the accumulation of errors during the sampling process, and
the significant inference times associated with transformers. Denoising
diffusion probabilistic models are a class of non-autoregressive generative
models recently shown to produce excellent samples in computer vision
(surpassing Generative Adversarial Networks), and to achieve log-likelihoods
that are competitive with transformers while having fast inference times.
Diffusion models can be applied to the latent representations learnt by
autoencoders, making them easily scalable and great candidates for application
to high dimensional data, such as medical images. Here, we propose a method
based on diffusion models to detect and segment anomalies in brain imaging. By
training the models on healthy data and then exploring its diffusion and
reverse steps across its Markov chain, we can identify anomalous areas in the
latent space and hence identify anomalies in the pixel space. Our diffusion
models achieve competitive performance compared with autoregressive approaches
across a series of experiments with 2D CT and MRI data involving synthetic and
real pathological lesions with much reduced inference times, making their usage
clinically viable.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Universal Speech Enhancement with Score-based Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 07, 2022 </span>    
         <span class="authors"> Joan Serrà, Santiago Pascual, Jordi Pons, R. Oguz Araz, Davide Scaini </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.03065" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Removing background noise from speech audio has been the subject of
considerable effort, especially in recent years due to the rise of virtual
communication and amateur recordings. Yet background noise is not the only
unpleasant disturbance that can prevent intelligibility: reverb, clipping,
codec artifacts, problematic equalization, limited bandwidth, or inconsistent
loudness are equally disturbing and ubiquitous. In this work, we propose to
consider the task of speech enhancement as a holistic endeavor, and present a
universal speech enhancement system that tackles 55 different distortions at
the same time. Our approach consists of a generative model that employs
score-based diffusion, together with a multi-resolution conditioning network
that performs enhancement with mixture density networks. We show that this
approach significantly outperforms the state of the art in a subjective test
performed by expert listeners. We also show that it achieves competitive
objective scores with just 4-8 diffusion steps, despite not considering any
particular strategy for fast sampling. We hope that both our methodology and
technical contributions encourage researchers and practitioners to adopt a
universal approach to speech enhancement, possibly framing it as a generative
task.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Blended Latent Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 06, 2022 </span>    
         <span class="authors"> Omri Avrahami, Ohad Fried, Dani Lischinski </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.02779" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The tremendous progress in neural image generation, coupled with the
emergence of seemingly omnipotent vision-language models has finally enabled
text-based interfaces for creating and editing images. Handling generic images
requires a diverse underlying generative model, hence the latest works utilize
diffusion models, which were shown to surpass GANs in terms of diversity. One
major drawback of diffusion models, however, is their relatively slow inference
time. In this paper, we present an accelerated solution to the task of local
text-driven editing of generic images, where the desired edits are confined to
a user-provided mask. Our solution leverages a recent text-to-image Latent
Diffusion Model (LDM), which speeds up diffusion by operating in a
lower-dimensional latent space. We first convert the LDM into a local image
editor by incorporating Blended Diffusion into it. Next we propose an
optimization-based solution for the inherent inability of this LDM to
accurately reconstruct images. Finally, we address the scenario of performing
local edits using thin masks. We evaluate our method against the available
baselines both qualitatively and quantitatively and demonstrate that in
addition to being faster, our method achieves better precision than the
baselines while mitigating some of their artifacts.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-GAN: Training GANs with Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 05, 2022 </span>    
         <span class="authors"> Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.02262" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative adversarial networks (GANs) are challenging to train stably, and a
promising remedy of injecting instance noise into the discriminator input has
not been very effective in practice. In this paper, we propose Diffusion-GAN, a
novel GAN framework that leverages a forward diffusion chain to generate
Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three
components, including an adaptive diffusion process, a diffusion
timestep-dependent discriminator, and a generator. Both the observed and
generated data are diffused by the same adaptive diffusion process. At each
diffusion timestep, there is a different noise-to-data ratio and the
timestep-dependent discriminator learns to distinguish the diffused real data
from the diffused generated data. The generator learns from the discriminator's
feedback by backpropagating through the forward diffusion chain, whose length
is adaptively adjusted to balance the noise and data levels. We theoretically
show that the discriminator's timestep-dependent strategy gives consistent and
helpful guidance to the generator, enabling it to match the true data
distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN
baselines on various datasets, showing that it can produce more realistic
images with higher stability and data efficiency than state-of-the-art GANs.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models

{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 05, 2022 </span>    
         <span class="authors"> Alon Levkovitch, Eliya Nachmani, Lior Wolf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.02246" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.AI, cs.LG, eess.AS, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a novel way of conditioning a pretrained denoising diffusion
speech model to produce speech in the voice of a novel person unseen during
training. The method requires a short (~3 seconds) sample from the target
person, and generation is steered at inference time, without any training
steps. At the heart of the method lies a sampling process that combines the
estimation of the denoising model with a low-pass version of the new speaker's
sample. The objective and subjective evaluations show that our sampling method
can generate a voice similar to that of the target speaker in terms of
frequency, with an accuracy comparable to state-of-the-art methods, and without
training.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Compositional Visual Generation with Composable Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 03, 2022 </span>    
         <span class="authors"> Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, Joshua B. Tenenbaum </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.01714" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Large text-guided diffusion models, such as DALLE-2, are able to generate
stunning photorealistic images given natural language descriptions. While such
models are highly flexible, they struggle to understand the composition of
certain concepts, such as confusing the attributes of different objects or
relations between objects. In this paper, we propose an alternative structured
approach for compositional generation using diffusion models. An image is
generated by composing a set of diffusion models, with each of them modeling a
certain component of the image. To do this, we interpret diffusion models as
energy-based models in which the data distributions defined by the energy
functions may be explicitly combined. The proposed method can generate scenes
at test time that are substantially more complex than those seen in training,
composing sentence descriptions, object relations, human facial attributes, and
even generalizing to new combinations that are rarely seen in the real world.
We further illustrate how our approach may be used to compose pre-trained
text-guided diffusion models and generate photorealistic images containing all
the details described in the input descriptions, including the binding of
certain object attributes that have been shown difficult for DALLE-2. These
results point to the effectiveness of the proposed method in promoting
structured generalization for visual generation. Project page:
https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Generative Models Detect Manifolds
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 02, 2022 </span>    
         <span class="authors"> Jakiw Pidstrigach </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.01018" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, cs.NA, math.NA, math.PR, 68T99, I.2.0
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) need to approximate the scores $\nabla
\log p_t$ of the intermediate distributions as well as the final distribution
$p_T$ of the forward process. The theoretical underpinnings of the effects of
these approximations are still lacking. We find precise conditions under which
SGMs are able to produce samples from an underlying (low-dimensional) data
manifold $\mathcal{M}$. This assures us that SGMs are able to generate the
"right kind of samples". For example, taking $\mathcal{M}$ to be the subset of
images of faces, we find conditions under which the SGM robustly produces an
image of a face, even though the relative frequencies of these images might not
accurately represent the true data generating distribution. Moreover, this
analysis is a first step towards understanding the generalization properties of
SGMs: Taking $\mathcal{M}$ to be the set of all training samples, our results
provide a precise description of when the SGM memorizes its training data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improving Diffusion Models for Inverse Problems using Manifold Constraints
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 02, 2022 </span>    
         <span class="authors"> Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.00941" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, diffusion models have been used to solve various inverse problems
in an unsupervised manner with appropriate modifications to the sampling
process. However, the current solvers, which recursively apply a reverse
diffusion step followed by a projection-based measurement consistency step,
often produce suboptimal results. By studying the generative sampling path,
here we show that current solvers throw the sample path off the data manifold,
and hence the error accumulates. To address this, we propose an additional
correction term inspired by the manifold constraint, which can be used
synergistically with the previous solvers to make the iterations close to the
manifold. The proposed manifold constraint is straightforward to implement
within a few lines of code, yet boosts the performance by a surprisingly large
margin. With extensive experiments, we show that our method is superior to the
previous methods both theoretically and empirically, producing promising
results in many applications such as image inpainting, colorization, and
sparse-view computed tomography. Code available
https://github.com/HJ-harry/MCG_diffusion
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 02, 2022 </span>    
         <span class="authors"> Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.00927" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPMs) are emerging powerful generative
models. Despite their high-quality generation performance, DPMs still suffer
from their slow sampling as they generally need hundreds or thousands of
sequential function evaluations (steps) of large neural networks to draw a
sample. Sampling from DPMs can be viewed alternatively as solving the
corresponding diffusion ordinary differential equations (ODEs). In this work,
we propose an exact formulation of the solution of diffusion ODEs. The
formulation analytically computes the linear part of the solution, rather than
leaving all terms to black-box ODE solvers as adopted in previous works. By
applying change-of-variable, the solution can be equivalently simplified to an
exponentially weighted integral of the neural network. Based on our
formulation, we propose DPM-Solver, a fast dedicated high-order solver for
diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for
both discrete-time and continuous-time DPMs without any further training.
Experimental results show that DPM-Solver can generate high-quality samples in
only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in
10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10
dataset, and a $4\sim 16\times$ speedup compared with previous state-of-the-art
training-free samplers on various datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Elucidating the Design Space of Diffusion-Based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 01, 2022 </span>    
         <span class="authors"> Tero Karras, Miika Aittala, Timo Aila, Samuli Laine </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.00364" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG, cs.NE, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We argue that the theory and practice of diffusion-based generative models
are currently unnecessarily convoluted and seek to remedy the situation by
presenting a design space that clearly separates the concrete design choices.
This lets us identify several changes to both the sampling and training
processes, as well as preconditioning of the score networks. Together, our
improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a
class-conditional setting and 1.97 in an unconditional setting, with much
faster sampling (35 network evaluations per image) than prior designs. To
further demonstrate their modular nature, we show that our design changes
dramatically improve both the efficiency and quality obtainable with
pre-trained score networks from previous work, including improving the FID of a
previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after
re-training with our proposed improvements to a new SOTA of 1.36.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Torsional Diffusion for Molecular Conformer Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 01, 2022 </span>    
         <span class="authors"> Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.01729" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  physics.chem-ph, cs.LG, q-bio.BM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Molecular conformer generation is a fundamental task in computational
chemistry. Several machine learning approaches have been developed, but none
have outperformed state-of-the-art cheminformatics methods. We propose
torsional diffusion, a novel diffusion framework that operates on the space of
torsion angles via a diffusion process on the hypertorus and an
extrinsic-to-intrinsic score model. On a standard benchmark of drug-like
molecules, torsional diffusion generates superior conformer ensembles compared
to machine learning and cheminformatics methods in terms of both RMSD and
chemical properties, and is orders of magnitude faster than previous
diffusion-based models. Moreover, our model provides exact likelihoods, which
we employ to build the first generalizable Boltzmann generator. Code is
available at https://github.com/gcorso/torsional-diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Discovering the Hidden Vocabulary of DALLE-2
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 01, 2022 </span>    
         <span class="authors"> Giannis Daras, Alexandros G. Dimakis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.00169" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CL, cs.CR, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We discover that DALLE-2 seems to have a hidden vocabulary that can be used
to generate images with absurd prompts. For example, it seems that
\texttt{Apoploe vesrreaitais} means birds and \texttt{Contarra ccetnxniams
luryca tanniounons} (sometimes) means bugs or pests. We find that these prompts
are often consistent in isolation but also sometimes in combinations. We
present our black-box method to discover words that seem random but have some
correspondence to visual concepts. This creates important security and
interpretability challenges.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 31, 2022 </span>    
         <span class="authors"> Kamil Deja, Anna Kuzina, Tomasz Trzciński, Jakub M. Tomczak </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2206.00070" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based Deep Generative Models (DDGMs) offer state-of-the-art
performance in generative modeling. Their main strength comes from their unique
setup in which a model (the backward diffusion process) is trained to reverse
the forward diffusion process, which gradually adds noise to the input signal.
Although DDGMs are well studied, it is still unclear how the small amount of
noise is transformed during the backward diffusion process. Here, we focus on
analyzing this problem to gain more insight into the behavior of DDGMs and
their denoising and generative capabilities. We observe a fluid transition
point that changes the functionality of the backward diffusion process from
generating a (corrupted) image from noise to denoising the corrupted image to
the final sample. Based on this observation, we postulate to divide a DDGM into
two parts: a denoiser and a generator. The denoiser could be parameterized by a
denoising auto-encoder, while the generator is a diffusion-based model with its
own set of parameters. We experimentally validate our proposition, showing its
pros and cons.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Few-Shot Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 30, 2022 </span>    
         <span class="authors"> Giorgio Giannone, Didrik Nielsen, Ole Winther </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.15463" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPM) are powerful hierarchical
latent variable models with remarkable sample generation quality and training
stability. These properties can be attributed to parameter sharing in the
generative hierarchy, as well as a parameter-free diffusion-based inference
procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a
framework for few-shot generation leveraging conditional DDPMs. FSDMs are
trained to adapt the generative process conditioned on a small set of images
from a given class by aggregating image patch information using a set-based
Vision Transformer (ViT). At test time, the model is able to generate samples
from previously unseen classes conditioned on as few as 5 samples from that
class. We empirically show that FSDM can perform few-shot generation and
transfer to new datasets. We benchmark variants of our method on complex vision
datasets for few-shot learning and compare to unconditional and conditional
DDPM baselines. Additionally, we show how conditioning the model on patch-based
input set information improves training convergence.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 30, 2022 </span>    
         <span class="authors"> Sungwon Kim, Heeseung Kim, Sungroh Yoon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.15370" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.AI, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose Guided-TTS 2, a diffusion-based generative model for high-quality
adaptive TTS using untranscribed data. Guided-TTS 2 combines a
speaker-conditional diffusion model with a speaker-dependent phoneme classifier
for adaptive text-to-speech. We train the speaker-conditional diffusion model
on large-scale untranscribed datasets for a classifier-free guidance method and
further fine-tune the diffusion model on the reference speech of the target
speaker for adaptation, which only takes 40 seconds. We demonstrate that
Guided-TTS 2 shows comparable performance to high-quality single-speaker TTS
baselines in terms of speech quality and speaker similarity with only a
ten-second untranscribed data. We further show that Guided-TTS 2 outperforms
adaptive TTS baselines on multi-speaker datasets even with a zero-shot
adaptation setting. Guided-TTS 2 can adapt to a wide range of voices only using
untranscribed speech, which enables adaptive TTS with the voice of non-human
characters such as Gollum in \textit{"The Lord of the Rings"}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Guided Diffusion Model for Adversarial Purification
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 30, 2022 </span>    
         <span class="authors"> Jinyi Wang, Zhaoyang Lyu, Dahua Lin, Bo Dai, Hongfei Fu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.14969" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 With wider application of deep neural networks (DNNs) in various algorithms
and frameworks, security threats have become one of the concerns. Adversarial
attacks disturb DNN-based image classifiers, in which attackers can
intentionally add imperceptible adversarial perturbations on input images to
fool the classifiers. In this paper, we propose a novel purification approach,
referred to as guided diffusion model for purification (GDMP), to help protect
classifiers from adversarial attacks. The core of our approach is to embed
purification into the diffusion denoising process of a Denoised Diffusion
Probabilistic Model (DDPM), so that its diffusion process could submerge the
adversarial perturbations with gradually added Gaussian noises, and both of
these noises can be simultaneously removed following a guided denoising
process. On our comprehensive experiments across various datasets, the proposed
GDMP is shown to reduce the perturbations raised by adversarial attacks to a
shallow range, thereby significantly improving the correctness of
classification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under
PGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on
the challenging ImageNet dataset.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-LM Improves Controllable Text Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 27, 2022 </span>    
         <span class="authors"> Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.14217" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Controlling the behavior of language models (LMs) without re-training is a
major open problem in natural language generation. While recent works have
demonstrated successes on controlling simple sentence attributes (e.g.,
sentiment), there has been little progress on complex, fine-grained controls
(e.g., syntactic structure). To address this challenge, we develop a new
non-autoregressive language model based on continuous diffusions that we call
Diffusion-LM. Building upon the recent successes of diffusion models in
continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian
vectors into word vectors, yielding a sequence of intermediate latent
variables. The continuous, hierarchical nature of these intermediate variables
enables a simple gradient-based algorithm to perform complex, controllable
generation tasks. We demonstrate successful control of Diffusion-LM for six
challenging fine-grained control tasks, significantly outperforming prior work.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Maximum Likelihood Training of Implicit Nonlinear Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 27, 2022 </span>    
         <span class="authors"> Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, Il-Chul Moon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.13699" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Whereas diverse variations of diffusion models exist, extending the linear
diffusion into a nonlinear diffusion process is investigated by very few works.
The nonlinearity effect has been hardly understood, but intuitively, there
would be promising diffusion patterns to efficiently train the generative
distribution towards the data distribution. This paper introduces a
data-adaptive nonlinear diffusion process for score-based diffusion models. The
proposed Implicit Nonlinear Diffusion Model (INDM) learns by combining a
normalizing flow and a diffusion process. Specifically, INDM implicitly
constructs a nonlinear diffusion on the \textit{data space} by leveraging a
linear diffusion on the \textit{latent space} through a flow network. This flow
network is key to forming a nonlinear diffusion, as the nonlinearity depends on
the flow network. This flexible nonlinearity improves the learning curve of
INDM to nearly Maximum Likelihood Estimation (MLE) against the non-MLE curve of
DDPM++, which turns out to be an inflexible version of INDM with the flow fixed
as an identity mapping. Also, the discretization of INDM shows the sampling
robustness. In experiments, INDM achieves the state-of-the-art FID of 1.75 on
CelebA. We release our code at https://github.com/byeonghu-na/INDM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Accelerating Diffusion Models via Early Stop of the Diffusion Process
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 25, 2022 </span>    
         <span class="authors"> Zhaoyang Lyu, Xudong XU, Ceyuan Yang, Dahua Lin, Bo Dai </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.12524" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Probabilistic Models (DDPMs) have achieved impressive
performance on various generation tasks. By modeling the reverse process of
gradually diffusing the data distribution into a Gaussian distribution,
generating a sample in DDPMs can be regarded as iteratively denoising a
randomly sampled Gaussian noise. However, in practice DDPMs often need hundreds
even thousands of denoising steps to obtain a high-quality sample from the
Gaussian noise, leading to extremely low inference efficiency. In this work, we
propose a principled acceleration strategy, referred to as Early-Stopped DDPM
(ES-DDPM), for DDPMs. The key idea is to stop the diffusion process early where
only the few initial diffusing steps are considered and the reverse denoising
process starts from a non-Gaussian distribution. By further adopting a powerful
pre-trained generative model, such as GAN and VAE, in ES-DDPM, sampling from
the target non-Gaussian distribution can be efficiently achieved by diffusing
samples obtained from the pre-trained generative model. In this way, the number
of required denoising steps is significantly reduced. In the meantime, the
sample quality of ES-DDPM also improves substantially, outperforming both the
vanilla DDPM and the adopted pre-trained generative model. On extensive
experiments across CIFAR-10, CelebA, ImageNet, LSUN-Bedroom and LSUN-Cat,
ES-DDPM obtains promising acceleration effect and performance improvement over
representative baseline methods. Moreover, ES-DDPM also demonstrates several
attractive properties, including being orthogonal to existing acceleration
methods, as well as simultaneously enabling both global semantic and local
pixel-level control in image generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Flexible Diffusion Modeling of Long Videos
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2022 </span>    
         <span class="authors"> William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.11495" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a framework for video modeling based on denoising diffusion
probabilistic models that produces long-duration video completions in a variety
of realistic environments. We introduce a generative model that can at
test-time sample any arbitrary subset of video frames conditioned on any other
subset and present an architecture adapted for this purpose. Doing so allows us
to efficiently compare and optimize a variety of schedules for the order in
which frames in a long video are sampled and use selective sparse and
long-range conditioning on previously sampled frames. We demonstrate improved
video modeling over prior work on a number of datasets and sample temporally
coherent videos over 25 minutes in length. We additionally release a new video
modeling dataset and semantically meaningful metrics based on videos generated
in the CARLA autonomous driving simulator.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 23, 2022 </span>    
         <span class="authors"> Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.11487" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present Imagen, a text-to-image diffusion model with an unprecedented
degree of photorealism and a deep level of language understanding. Imagen
builds on the power of large transformer language models in understanding text
and hinges on the strength of diffusion models in high-fidelity image
generation. Our key discovery is that generic large language models (e.g. T5),
pretrained on text-only corpora, are surprisingly effective at encoding text
for image synthesis: increasing the size of the language model in Imagen boosts
both sample fidelity and image-text alignment much more than increasing the
size of the image diffusion model. Imagen achieves a new state-of-the-art FID
score of 7.27 on the COCO dataset, without ever training on COCO, and human
raters find Imagen samples to be on par with the COCO data itself in image-text
alignment. To assess text-to-image models in greater depth, we introduce
DrawBench, a comprehensive and challenging benchmark for text-to-image models.
With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,
Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen
over other models in side-by-side comparisons, both in terms of sample quality
and image-text alignment. See https://imagen.research.google/ for an overview
of the results.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Planning with Diffusion for Flexible Behavior Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 20, 2022 </span>    
         <span class="authors"> Michael Janner, Yilun Du, Joshua B. Tenenbaum, Sergey Levine </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.09991" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Model-based reinforcement learning methods often use learning only for the
purpose of estimating an approximate dynamics model, offloading the rest of the
decision-making work to classical trajectory optimizers. While conceptually
simple, this combination has a number of empirical shortcomings, suggesting
that learned models may not be well-suited to standard trajectory optimization.
In this paper, we consider what it would look like to fold as much of the
trajectory optimization pipeline as possible into the modeling problem, such
that sampling from the model and planning with it become nearly identical. The
core of our technical approach lies in a diffusion probabilistic model that
plans by iteratively denoising trajectories. We show how classifier-guided
sampling and image inpainting can be reinterpreted as coherent planning
strategies, explore the unusual and useful properties of diffusion-based
planning methods, and demonstrate the effectiveness of our framework in control
settings that emphasize long-horizon decision-making and test-time flexibility.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 19, 2022 </span>    
         <span class="authors"> Vikram Voleti, Alexia Jolicoeur-Martineau, Christopher Pal </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.09853" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Video prediction is a challenging task. The quality of video frames from
current state-of-the-art (SOTA) generative models tends to be poor and
generalization beyond the training data is difficult. Furthermore, existing
prediction frameworks are typically not capable of simultaneously handling
other video-related tasks such as unconditional generation or interpolation. In
this work, we devise a general-purpose framework called Masked Conditional
Video Diffusion (MCVD) for all of these video synthesis tasks using a
probabilistic conditional score-based denoising diffusion model, conditioned on
past and/or future frames. We train the model in a manner where we randomly and
independently mask all the past frames or all the future frames. This novel but
straightforward setup allows us to train a single model that is capable of
executing a broad range of video tasks, specifically: future/past prediction --
when only future/past frames are masked; unconditional generation -- when both
past and future frames are masked; and interpolation -- when neither past nor
future frames are masked. Our experiments show that this approach can generate
high-quality frames for diverse types of videos. Our MCVD models are built from
simple non-recurrent 2D-convolutional architectures, conditioning on blocks of
frames and generating blocks of frames. We generate videos of arbitrary lengths
autoregressively in a block-wise manner. Our approach yields SOTA results
across standard video prediction and interpolation benchmarks, with computation
times for training models measured in 1-12 days using $\le$ 4 GPUs. Project
page: https://mask-cond-video-diffusion.github.io ; Code :
https://github.com/voletiv/mcvd-pytorch
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for Adversarial Purification
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 16, 2022 </span>    
         <span class="authors"> Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, Anima Anandkumar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.07460" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Adversarial purification refers to a class of defense methods that remove
adversarial perturbations using a generative model. These methods do not make
assumptions on the form of attack and the classification model, and thus can
defend pre-existing classifiers against unseen threats. However, their
performance currently falls behind adversarial training methods. In this work,
we propose DiffPure that uses diffusion models for adversarial purification:
Given an adversarial example, we first diffuse it with a small amount of noise
following a forward diffusion process, and then recover the clean image through
a reverse generative process. To evaluate our method against strong adaptive
attacks in an efficient and scalable way, we propose to use the adjoint method
to compute full gradients of the reverse generative process. Extensive
experiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQ
with three classifier architectures including ResNet, WideResNet and ViT
demonstrate that our method achieves the state-of-the-art results,
outperforming current adversarial training and adversarial purification
methods, often by a large margin. Project page: https://diffpure.github.io.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On Conditioning the Input Noise for Controlled Image Generation with Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 08, 2022 </span>    
         <span class="authors"> Vedant Singh, Surgan Jandial, Ayush Chopra, Siddharth Ramesh, Balaji Krishnamurthy, Vineeth N. Balasubramanian </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.03859" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Conditional image generation has paved the way for several breakthroughs in
image editing, generating stock photos and 3-D object generation. This
continues to be a significant area of interest with the rise of new
state-of-the-art methods that are based on diffusion models. However, diffusion
models provide very little control over the generated image, which led to
subsequent works exploring techniques like classifier guidance, that provides a
way to trade off diversity with fidelity. In this work, we explore techniques
to condition diffusion models with carefully crafted input noise artifacts.
This allows generation of images conditioned on semantic attributes. This is
different from existing approaches that input Gaussian noise and further
introduce conditioning at the diffusion model's inference step. Our experiments
over several examples and conditional settings show the potential of our
approach.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Subspace Diffusion Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 03, 2022 </span>    
         <span class="authors"> Bowen Jing, Gabriele Corso, Renato Berlinghieri, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2205.01490" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based models generate samples by mapping noise to data (and vice versa)
via a high-dimensional diffusion process. We question whether it is necessary
to run this entire process at high dimensionality and incur all the
inconveniences thereof. Instead, we restrict the diffusion via projections onto
subspaces as the data distribution evolves toward noise. When applied to
state-of-the-art models, our framework simultaneously improves sample quality
-- reaching an FID of 2.17 on unconditional CIFAR-10 -- and reduces the
computational cost of inference for the same number of denoising steps. Our
framework is fully compatible with continuous-time diffusion and retains its
flexible capabilities, including exact log-likelihoods and controllable
generation. Code is available at
https://github.com/bjing2016/subspace-diffusion.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Fast Sampling of Diffusion Models with Exponential Integrator
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 29, 2022 </span>    
         <span class="authors"> Qinsheng Zhang, Yongxin Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2204.13902" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The past few years have witnessed the great success of Diffusion models~(DMs)
in generating high-fidelity samples in generative modeling tasks. A major
limitation of the DM is its notoriously slow sampling procedure which normally
requires hundreds to thousands of time discretization steps of the learned
diffusion process to reach the desired accuracy. Our goal is to develop a fast
sampling method for DMs with a much less number of steps while retaining high
sample quality. To this end, we systematically analyze the sampling procedure
in DMs and identify key factors that affect the sample quality, among which the
method of discretization is most crucial. By carefully examining the learned
diffusion process, we propose Diffusion Exponential Integrator Sampler~(DEIS).
It is based on the Exponential Integrator designed for discretizing ordinary
differential equations (ODEs) and leverages a semilinear structure of the
learned diffusion process to reduce the discretization error. The proposed
method can be applied to any DMs and can generate high-fidelity samples in as
few as 10 steps. In our experiments, it takes about 3 minutes on one A6000 GPU
to generate $50k$ images from CIFAR10. Moreover, by directly using pre-trained
DMs, we achieve the state-of-art sampling performance when the number of score
function evaluation~(NFE) is limited, e.g., 4.17 FID with 10 NFEs, 3.37 FID,
and 9.74 IS with only 15 NFEs on CIFAR10. Code is available at
https://github.com/qsh-zh/deis
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Retrieval-Augmented Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 25, 2022 </span>    
         <span class="authors"> Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, Björn Ommer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2204.11824" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Novel architectures have recently improved generative image synthesis leading
to excellent visual quality in various tasks. Much of this success is due to
the scalability of these architectures and hence caused by a dramatic increase
in model complexity and in the computational resources invested in training
these models. Our work questions the underlying paradigm of compressing large
training data into ever growing parametric representations. We rather present
an orthogonal, semi-parametric approach. We complement comparably small
diffusion or autoregressive models with a separate image database and a
retrieval strategy. During training we retrieve a set of nearest neighbors from
this external database for each training instance and condition the generative
model on these informative samples. While the retrieval approach is providing
the (local) content, the model is focusing on learning the composition of
scenes based on this content. As demonstrated by our experiments, simply
swapping the database for one with different contents transfers a trained model
post-hoc to a novel domain. The evaluation shows competitive performance on
tasks which the generative model has not been trained on, such as
class-conditional synthesis, zero-shot stylization or text-to-image synthesis
without requiring paired text-image data. With negligible memory and
computational overhead for the external database and retrieval we can
significantly reduce the parameter count of the generative model and still
outperform the state-of-the-art.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 21, 2022 </span>    
         <span class="authors"> Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, Zhou Zhao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2204.09934" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) have recently achieved
leading performances in many generative tasks. However, the inherited iterative
sampling process costs hindered their applications to speech synthesis. This
paper proposes FastDiff, a fast conditional diffusion model for high-quality
speech synthesis. FastDiff employs a stack of time-aware location-variable
convolutions of diverse receptive field patterns to efficiently model long-term
time dependencies with adaptive conditions. A noise schedule predictor is also
adopted to reduce the sampling steps without sacrificing the generation
quality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer,
FastDiff-TTS, which generates high-fidelity speech waveforms without any
intermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff
demonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech
samples. Also, FastDiff enables a sampling speed of 58x faster than real-time
on a V100 GPU, making diffusion models practically applicable to speech
synthesis deployment for the first time. We further show that FastDiff
generalized well to the mel-spectrogram inversion of unseen speakers, and
FastDiff-TTS outperformed other competing methods in end-to-end text-to-speech
synthesis. Audio samples are available at \url{https://FastDiff.github.io/}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Score-based Geometric Model for Molecular Dynamics Simulations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 19, 2022 </span>    
         <span class="authors"> Fang Wu, Stan Z. Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2204.08672" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CE, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Molecular dynamics (MD) has long been the de facto choice for simulating
complex atomistic systems from first principles. Recently deep learning models
become a popular way to accelerate MD. Notwithstanding, existing models depend
on intermediate variables such as the potential energy or force fields to
update atomic positions, which requires additional computations to perform
back-propagation. To waive this requirement, we propose a novel model called
DiffMD by directly estimating the gradient of the log density of molecular
conformations. DiffMD relies on a score-based denoising diffusion generative
model that perturbs the molecular structure with a conditional noise depending
on atomic accelerations and treats conformations at previous timeframes as the
prior distribution for sampling. Another challenge of modeling such a
conformation generation process is that a molecule is kinetic instead of
static, which no prior works have strictly studied. To solve this challenge, we
propose an equivariant geometric Transformer as the score function in the
diffusion process to calculate corresponding gradients. It incorporates the
directions and velocities of atomic motions via 3D spherical Fourier-Bessel
representations. With multiple architectural improvements, we outperform
state-of-the-art baselines on MD17 and isomers of C7O2H10 datasets. This work
contributes to accelerating material and drug discovery.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Hierarchical Text-Conditional Image Generation with CLIP Latents
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 13, 2022 </span>    
         <span class="authors"> Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2204.06125" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Contrastive models like CLIP have been shown to learn robust representations
of images that capture both semantics and style. To leverage these
representations for image generation, we propose a two-stage model: a prior
that generates a CLIP image embedding given a text caption, and a decoder that
generates an image conditioned on the image embedding. We show that explicitly
generating image representations improves image diversity with minimal loss in
photorealism and caption similarity. Our decoders conditioned on image
representations can also produce variations of an image that preserve both its
semantics and style, while varying the non-essential details absent from the
image representation. Moreover, the joint embedding space of CLIP enables
language-guided image manipulations in a zero-shot fashion. We use diffusion
models for the decoder and experiment with both autoregressive and diffusion
models for the prior, finding that the latter are computationally more
efficient and produce higher-quality samples.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Video Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 07, 2022 </span>    
         <span class="authors"> Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J. Fleet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2204.03458" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generating temporally coherent high fidelity video is an important milestone
in generative modeling research. We make progress towards this milestone by
proposing a diffusion model for video generation that shows very promising
initial results. Our model is a natural extension of the standard image
diffusion architecture, and it enables jointly training from image and video
data, which we find to reduce the variance of minibatch gradients and speed up
optimization. To generate long and higher resolution videos we introduce a new
conditional sampling technique for spatial and temporal video extension that
performs better than previously proposed methods. We present the first results
on a large text-conditioned video generation task, as well as state-of-the-art
results on established benchmarks for video prediction and unconditional video
generation. Supplementary material is available at
https://video-diffusion.github.io/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## KNN-Diffusion: Image Generation via Large-Scale Retrieval
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 06, 2022 </span>    
         <span class="authors"> Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, Yaniv Taigman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2204.02849" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.CL, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent text-to-image models have achieved impressive results. However, since
they require large-scale datasets of text-image pairs, it is impractical to
train them on new domains where data is scarce or not labeled. In this work, we
propose using large-scale retrieval methods, in particular, efficient
k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a
substantially small and efficient text-to-image diffusion model without any
text, (2) generating out-of-distribution images by simply swapping the
retrieval database at inference time, and (3) performing text-driven local
semantic manipulations while preserving object identity. To demonstrate the
robustness of our method, we apply our kNN approach on two state-of-the-art
diffusion backbones, and show results on several different datasets. As
evaluated by human studies and automatic metrics, our method achieves
state-of-the-art results compared to existing approaches that train
text-to-image generation models using images only (without paired text data)
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Perception Prioritized Training of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 01, 2022 </span>    
         <span class="authors"> Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, Sungroh Yoon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2204.00227" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models learn to restore noisy data, which is corrupted with
different levels of noise, by optimizing the weighted sum of the corresponding
loss terms, i.e., denoising score matching loss. In this paper, we show that
restoring data corrupted with certain noise levels offers a proper pretext task
for the model to learn rich visual concepts. We propose to prioritize such
noise levels over other levels during training, by redesigning the weighting
scheme of the objective function. We show that our simple redesign of the
weighting scheme significantly improves the performance of diffusion models
regardless of the datasets, architectures, and sampling strategies.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generating High Fidelity Data from Low-density Regions using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 31, 2022 </span>    
         <span class="authors"> Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, Cristian Canton Ferrer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.17260" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Our work focuses on addressing sample deficiency from low-density regions of
data manifold in common image datasets. We leverage diffusion process based
generative models to synthesize novel images from low-density regions. We
observe that uniform sampling from diffusion models predominantly samples from
high-density regions of the data manifold. Therefore, we modify the sampling
process to guide it towards low-density regions while simultaneously
maintaining the fidelity of synthetic data. We rigorously demonstrate that our
process successfully generates novel high fidelity samples from low-density
regions. We further examine generated samples and show that the model does not
memorize low-density data and indeed learns to generate novel samples from
low-density regions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 31, 2022 </span>    
         <span class="authors"> Simon Welker, Julius Richter, Timo Gerkmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.17004" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) have recently shown impressive results
for difficult generative tasks such as the unconditional and conditional
generation of natural images and audio signals. In this work, we extend these
models to the complex short-time Fourier transform (STFT) domain, proposing a
novel training task for speech enhancement using a complex-valued deep neural
network. We derive this training task within the formalism of stochastic
differential equations (SDEs), thereby enabling the use of predictor-corrector
samplers. We provide alternative formulations inspired by previous publications
on using generative diffusion models for speech enhancement, avoiding the need
for any prior assumptions on the noise distribution and making the training
task purely generative which, as we show, results in improved enhancement
performance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Equivariant Diffusion for Molecule Generation in 3D
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 31, 2022 </span>    
         <span class="authors"> Emiel Hoogeboom, Victor Garcia Satorras, Clément Vignac, Max Welling </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.17003" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, q-bio.QM, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This work introduces a diffusion model for molecule generation in 3D that is
equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model
(EDM) learns to denoise a diffusion process with an equivariant network that
jointly operates on both continuous (atom coordinates) and categorical features
(atom types). In addition, we provide a probabilistic analysis which admits
likelihood computation of molecules using our model. Experimentally, the
proposed method significantly outperforms previous 3D molecular generative
methods regarding the quality of generated samples and efficiency at training
time.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 31, 2022 </span>    
         <span class="authors"> Yuma Koizumi, Heiga Zen, Kohei Yatabe, Nanxin Chen, Michiel Bacchiani </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.16749" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Neural vocoder using denoising diffusion probabilistic model (DDPM) has been
improved by adaptation of the diffusion noise distribution to given acoustic
features. In this study, we propose SpecGrad that adapts the diffusion noise so
that its time-varying spectral envelope becomes close to the conditioning
log-mel spectrogram. This adaptation by time-varying filtering improves the
sound quality especially in the high-frequency bands. It is processed in the
time-frequency domain to keep the computational cost almost the same as the
conventional DDPM-based neural vocoders. Experimental results showed that
SpecGrad generates higher-fidelity speech waveform than conventional DDPM-based
neural vocoders in both analysis-synthesis and speech enhancement scenarios.
Audio demos are available at wavegrad.github.io/specgrad/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for Counterfactual Explanations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 29, 2022 </span>    
         <span class="authors"> Guillaume Jeanneret, Loïc Simon, Frédéric Jurie </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.15636" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Counterfactual explanations have shown promising results as a post-hoc
framework to make image classifiers more explainable. In this paper, we propose
DiME, a method allowing the generation of counterfactual images using the
recent diffusion models. By leveraging the guided generative diffusion process,
our proposed methodology shows how to use the gradients of the target
classifier to generate counterfactual explanations of input instances. Further,
we analyze current approaches to evaluate spurious correlations and extend the
evaluation measurements by proposing a new metric: Correlation Difference. Our
experimental validations show that the proposed algorithm surpasses previous
State-of-the-Art results on 5 out of 6 metrics on CelebA.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Likelihood Score Matching for Conditional Score-based Data Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 27, 2022 </span>    
         <span class="authors"> Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, Chia-Ping Chen, Chun-Yi Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.14206" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Many existing conditional score-based data generation methods utilize Bayes'
theorem to decompose the gradients of a log posterior density into a mixture of
scores. These methods facilitate the training procedure of conditional score
models, as a mixture of scores can be separately estimated using a score model
and a classifier. However, our analysis indicates that the training objectives
for the classifier in these methods may lead to a serious score mismatch issue,
which corresponds to the situation that the estimated scores deviate from the
true ones. Such an issue causes the samples to be misled by the deviated scores
during the diffusion process, resulting in a degraded sampling quality. To
resolve it, we formulate a novel training objective, called Denoising
Likelihood Score Matching (DLSM) loss, for the classifier to match the
gradients of the true log likelihood density. Our experimental evidence shows
that the proposed method outperforms the previous methods on both Cifar-10 and
Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We
thus conclude that, by adopting DLSM, the conditional scores can be accurately
modeled, and the effect of the score mismatch issue is alleviated.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## BBDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 25, 2022 </span>    
         <span class="authors"> Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.13508" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.AI, cs.LG, cs.SD, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPMs) and their extensions have emerged as
competitive generative models yet confront challenges of efficient sampling. We
propose a new bilateral denoising diffusion model (BDDM) that parameterizes
both the forward and reverse processes with a schedule network and a score
network, which can train with a novel bilateral modeling objective. We show
that the new surrogate objective can achieve a lower bound of the log marginal
likelihood tighter than a conventional surrogate. We also find that BDDM allows
inheriting pre-trained score network parameters from any DPMs and consequently
enables speedy and stable learning of the schedule network and optimization of
a noise schedule for sampling. Our experiments demonstrate that BDDMs can
generate high-fidelity audio samples with as few as three sampling steps.
Moreover, compared to other state-of-the-art diffusion-based neural vocoders,
BDDMs produce comparable or higher quality samples indistinguishable from human
speech, notably with only seven sampling steps (143x faster than WaveGrad and
28.6x faster than DiffWave). We release our code at
https://github.com/tencent-ailab/bddm.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Accelerating Bayesian Optimization for Biological Sequence design with Denoising Autoencoders
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2022 </span>    
         <span class="authors"> Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, Andrew Gordon Wilson </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.12742" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.NE, q-bio.QM, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Bayesian optimization (BayesOpt) is a gold standard for query-efficient
continuous optimization. However, its adoption for drug design has been
hindered by the discrete, high-dimensional nature of the decision variables. We
develop a new approach (LaMBO) which jointly trains a denoising autoencoder
with a discriminative multi-task Gaussian process head, allowing gradient-based
optimization of multi-objective acquisition functions in the latent space of
the autoencoder. These acquisition functions allow LaMBO to balance the
explore-exploit tradeoff over multiple design rounds, and to balance objective
tradeoffs by optimizing sequences at many different points on the Pareto
frontier. We evaluate LaMBO on two small-molecule design tasks, and introduce
new tasks optimizing \emph{in silico} and \emph{in vitro} properties of
large-molecule fluorescent proteins. In our experiments LaMBO outperforms
genetic optimizers and does not require a large pretraining corpus,
demonstrating that BayesOpt is practical and effective for biological sequence
design.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 23, 2022 </span>    
         <span class="authors"> Hyungjin Chung, Eun Sun Lee, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.12621" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.AI, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Patient scans from MRI often suffer from noise, which hampers the diagnostic
capability of such images. As a method to mitigate such artifact, denoising is
largely studied both within the medical imaging community and beyond the
community as a general subject. However, recent deep neural network-based
approaches mostly rely on the minimum mean squared error (MMSE) estimates,
which tend to produce a blurred output. Moreover, such models suffer when
deployed in real-world sitautions: out-of-distribution data, and complex noise
distributions that deviate from the usual parametric noise models. In this
work, we propose a new denoising method based on score-based reverse diffusion
sampling, which overcomes all the aforementioned drawbacks. Our network,
trained only with coronal knee scans, excels even on out-of-distribution in
vivo liver MRI data, contaminated with complex mixture of noise. Even more, we
propose a method to enhance the resolution of the denoised image with the same
network. With extensive experiments, we show that our method establishes
state-of-the-art performance, while having desirable properties which prior
MMSE denoisers did not have: flexibly choosing the extent of denoising, and
quantifying uncertainty.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Dual Diffusion Implicit Bridges for Image-to-Image Translation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2022 </span>    
         <span class="authors"> Xuan Su, Jiaming Song, Chenlin Meng, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.08382" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Common image-to-image translation methods rely on joint training over data
from both source and target domains. The training process requires concurrent
access to both datasets, which hinders data separation and privacy protection;
and existing models cannot be easily adapted for translation of new domain
pairs. We present Dual Diffusion Implicit Bridges (DDIBs), an image translation
method based on diffusion models, that circumvents training on domain pairs.
Image translation with DDIBs relies on two diffusion models trained
independently on each domain, and is a two-step process: DDIBs first obtain
latent encodings for source images with the source diffusion model, and then
decode such encodings using the target model to construct target images. Both
steps are defined via ordinary differential equations (ODEs), thus the process
is cycle consistent only up to discretization errors of the ODE solvers.
Theoretically, we interpret DDIBs as concatenation of source to latent, and
latent to target Schrodinger Bridges, a form of entropy-regularized optimal
transport, to explain the efficacy of the method. Experimentally, we apply
DDIBs on synthetic and high-resolution image datasets, to demonstrate their
utility in a wide variety of translation tasks and their inherent optimal
transport properties.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Probabilistic Modeling for Video Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 16, 2022 </span>    
         <span class="authors"> Ruihan Yang, Prakhar Srivastava, Stephan Mandt </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.09481" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models are a promising new class of
generative models that mark a milestone in high-quality image generation. This
paper showcases their ability to sequentially generate video, surpassing prior
methods in perceptual and probabilistic forecasting metrics. We propose an
autoregressive, end-to-end optimized video diffusion model inspired by recent
advances in neural video compression. The model successively generates future
frames by correcting a deterministic next-frame prediction using a stochastic
residual generated by an inverse diffusion process. We compare this approach
against five baselines on four datasets involving natural and simulation-based
videos. We find significant improvements in terms of perceptual quality for all
datasets. Furthermore, by introducing a scalable version of the Continuous
Ranked Probability Score (CRPS) applicable to video, we show that our model
also outperforms existing approaches in their probabilistic frame forecasting
ability.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score matching enables causal discovery of nonlinear additive noise models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 08, 2022 </span>    
         <span class="authors"> Paul Rolland, Volkan Cevher, Matthäus Kleindessner, Chris Russel, Bernhard Schölkopf, Dominik Janzing, Francesco Locatello </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.04413" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper demonstrates how to recover causal graphs from the score of the
data distribution in non-linear additive (Gaussian) noise models. Using score
matching algorithms as a building block, we show how to design a new generation
of scalable causal discovery methods. To showcase our approach, we also propose
a new efficient method for approximating the score's Jacobian, enabling to
recover the causal graph. Empirically, we find that the new algorithm, called
SCORE, is competitive with state-of-the-art causal discovery methods while
being significantly faster.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Models for Medical Anomaly Detection
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 08, 2022 </span>    
         <span class="authors"> Julia Wolleb, Florentin Bieder, Robin Sandkühler, Philippe C. Cattin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.04306" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In medical applications, weakly supervised anomaly detection methods are of
great interest, as only image-level annotations are required for training.
Current anomaly detection methods mainly rely on generative adversarial
networks or autoencoder models. Those models are often complicated to train or
have difficulties to preserve fine details in the image. We present a novel
weakly supervised anomaly detection method based on denoising diffusion
implicit models. We combine the deterministic iterative noising and denoising
scheme with classifier guidance for image-to-image translation between diseased
and healthy subjects. Our method generates very detailed anomaly maps without
the need for a complex training procedure. We evaluate our method on the
BRATS2020 dataset for brain tumor detection and the CheXpert dataset for
detecting pleural effusions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Dynamic Dual-Output Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 08, 2022 </span>    
         <span class="authors"> Yaniv Benny, Lior Wolf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.04304" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Iterative denoising-based generation, also known as denoising diffusion
models, has recently been shown to be comparable in quality to other classes of
generative models, and even surpass them. Including, in particular, Generative
Adversarial Networks, which are currently the state of the art in many
sub-tasks of image generation. However, a major drawback of this method is that
it requires hundreds of iterations to produce a competitive result. Recent
works have proposed solutions that allow for faster generation with fewer
iterations, but the image quality gradually deteriorates with increasingly
fewer iterations being applied during generation. In this paper, we reveal some
of the causes that affect the generation quality of diffusion models,
especially when sampling with few iterations, and come up with a simple, yet
effective, solution to mitigate them. We consider two opposite equations for
the iterative denoising, the first predicts the applied noise, and the second
predicts the image directly. Our solution takes the two options and learns to
dynamically alternate between them through the denoising process. Our proposed
solution is general and can be applied to any existing diffusion model. As we
show, when applied to various SOTA architectures, our solution immediately
improves their generation quality, with negligible added complexity and
parameters. We experiment on multiple datasets and configurations and run an
extensive ablation study to support these findings.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Towards performant and reliable undersampled MR reconstruction via diffusion model sampling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 08, 2022 </span>    
         <span class="authors"> Cheng Peng, Pengfei Guo, S. Kevin Zhou, Vishal Patel, Rama Chellappa </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.04292" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Magnetic Resonance (MR) image reconstruction from under-sampled acquisition
promises faster scanning time. To this end, current State-of-The-Art (SoTA)
approaches leverage deep neural networks and supervised training to learn a
recovery model. While these approaches achieve impressive performances, the
learned model can be fragile on unseen degradation, e.g. when given a different
acceleration factor. These methods are also generally deterministic and provide
a single solution to an ill-posed problem; as such, it can be difficult for
practitioners to understand the reliability of the reconstruction. We introduce
DiffuseRecon, a novel diffusion model-based MR reconstruction method.
DiffuseRecon guides the generation process based on the observed signals and a
pre-trained diffusion model, and does not require additional training on
specific acceleration factors. DiffuseRecon is stochastic in nature and
generates results from a distribution of fully-sampled MR images; as such, it
allows us to explicitly visualize different potential reconstruction solutions.
Lastly, DiffuseRecon proposes an accelerated, coarse-to-fine Monte-Carlo
sampling scheme to approximate the most likely reconstruction candidate. The
proposed DiffuseRecon achieves SoTA performances reconstructing from raw
acquisition signals in fastMRI and SKM-TEA. Code will be open-sourced at
www.github.com/cpeng93/DiffuseRecon.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Generative Models for Molecule Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 07, 2022 </span>    
         <span class="authors"> Dwaraknath Gnaneshwar, Bharath Ramsundar, Dhairya Gandhi, Rachel Kurchin, Venkatasubramanian Viswanathan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2203.04698" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, q-bio.QM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advances in generative models have made exploring design spaces easier
for de novo molecule generation. However, popular generative models like GANs
and normalizing flows face challenges such as training instabilities due to
adversarial training and architectural constraints, respectively. Score-based
generative models sidestep these challenges by modelling the gradient of the
log probability density using a score function approximation, as opposed to
modelling the density function directly, and sampling from it using annealed
Langevin Dynamics. We believe that score-based generative models could open up
new opportunities in molecule generation due to their architectural
flexibility, such as replacing the score function with an SE(3) equivariant
model. In this work, we lay the foundations by testing the efficacy of
score-based models for molecule generation. We train a Transformer-based score
function on Self-Referencing Embedded Strings (SELFIES) representations of 1.5
million samples from the ZINC dataset and use the Moses benchmarking framework
to evaluate the generated samples on a suite of metrics.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Simulation Using Diffusion Schrodinger Bridges
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 27, 2022 </span>    
         <span class="authors"> Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.13460" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion models have recently emerged as a powerful class of
generative models. They provide state-of-the-art results, not only for
unconditional simulation, but also when used to solve conditional simulation
problems arising in a wide range of inverse problems. A limitation of these
models is that they are computationally intensive at generation time as they
require simulating a diffusion process over a long time horizon. When
performing unconditional simulation, a Schr\"odinger bridge formulation of
generative modeling leads to a theoretically grounded algorithm shortening
generation time which is complementary to other proposed acceleration
techniques. We extend the Schr\"odinger bridge framework to conditional
simulation. We demonstrate this novel methodology on various applications
including image super-resolution, optimal filtering for state-space models and
the refinement of pre-trained networks. Our code can be found at
https://github.com/vdeborto/cdsb.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Causal Models for Counterfactual Estimation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 21, 2022 </span>    
         <span class="authors"> Pedro Sanchez, Sotirios A. Tsaftaris </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.10166" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We consider the task of counterfactual estimation from observational imaging
data given a known causal structure. In particular, quantifying the causal
effect of interventions for high-dimensional data with neural networks remains
an open challenge. Herein we propose Diff-SCM, a deep structural causal model
that builds on recent advances of generative energy-based models. In our
setting, inference is performed by iteratively sampling gradients of the
marginal and conditional distributions entailed by the causal model.
Counterfactual estimation is achieved by firstly inferring latent variables
with deterministic forward diffusion, then intervening on a reverse diffusion
process using the gradients of an anti-causal predictor w.r.t the input.
Furthermore, we propose a metric for evaluating the generated counterfactuals.
We find that Diff-SCM produces more realistic and minimal counterfactuals than
baselines on MNIST data and can also be applied to ImageNet data. Code is
available https://github.com/vios-s/Diff-SCM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Pseudo Numerical Methods for Diffusion Models on Manifolds
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 20, 2022 </span>    
         <span class="authors"> Luping Liu, Yi Ren, Zhijie Lin, Zhou Zhao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.09778" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, cs.NA, math.NA, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality
samples such as image and audio samples. However, DDPMs require hundreds to
thousands of iterations to produce final samples. Several prior works have
successfully accelerated DDPMs through adjusting the variance schedule (e.g.,
Improved Denoising Diffusion Probabilistic Models) or the denoising equation
(e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these
acceleration methods cannot maintain the quality of samples and even introduce
new noise at a high speedup rate, which limit their practicability. To
accelerate the inference process while keeping the sample quality, we provide a
fresh perspective that DDPMs should be treated as solving differential
equations on manifolds. Under such a perspective, we propose pseudo numerical
methods for diffusion models (PNDMs). Specifically, we figure out how to solve
differential equations on manifolds and show that DDIMs are simple cases of
pseudo numerical methods. We change several classical numerical methods to
corresponding pseudo numerical methods and find that the pseudo linear
multi-step method is the best in most situations. According to our experiments,
by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can
generate higher quality synthetic images with only 50 steps compared with
1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps
(by around 0.4 in FID) and have good generalization on different variance
schedules. Our implementation is available at
https://github.com/luping-liu/PNDM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Truncated Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 19, 2022 </span>    
         <span class="authors"> Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.09671" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Employing a forward diffusion chain to gradually map the data to a noise
distribution, diffusion-based generative models learn how to generate the data
by inferring a reverse diffusion chain. However, this approach is slow and
costly because it needs many forward and reverse steps. We propose a faster and
cheaper approach that adds noise not until the data become pure random noise,
but until they reach a hidden noisy-data distribution that we can confidently
learn. Then, we use fewer reverse steps to generate data by starting from this
hidden distribution that is made similar to the noisy data. We reveal that the
proposed model can be cast as an adversarial auto-encoder empowered by both the
diffusion process and a learnable implicit prior. Experimental results show
even with a significantly smaller number of reverse diffusion steps, the
proposed truncated diffusion probabilistic models can provide consistent
improvements over the non-truncated ones in terms of performance in both
unconditional and text-guided image generations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Understanding DDPM Latent Codes Through Optimal Transport
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 14, 2022 </span>    
         <span class="authors"> Valentin Khrulkov, Gleb Ryzhakov, Andrei Chertkov, Ivan Oseledets </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.07477" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.AI, cs.LG, cs.NA, math.AP, math.NA
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently outperformed alternative approaches to model
the distribution of natural images, such as GANs. Such diffusion models allow
for deterministic sampling via the probability flow ODE, giving rise to a
latent space and an encoder map. While having important practical applications,
such as estimation of the likelihood, the theoretical properties of this map
are not yet fully understood. In the present work, we partially address this
question for the popular case of the VP SDE (DDPM) approach. We show that,
perhaps surprisingly, the DDPM encoder map coincides with the optimal transport
map for common distributions; we support this claim theoretically and by
extensive numerical experiments.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 11, 2022 </span>    
         <span class="authors"> Daniel Watson, William Chan, Jonathan Ho, Mohammad Norouzi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.05830" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have emerged as an expressive family of generative models
rivaling GANs in sample quality and autoregressive models in likelihood scores.
Standard diffusion models typically require hundreds of forward passes through
the model to generate a single high-fidelity sample. We introduce
Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast
samplers for any pre-trained diffusion model by differentiating through sample
quality scores. We also present Generalized Gaussian Diffusion Models (GGDM), a
family of flexible non-Markovian samplers for diffusion models. We show that
optimizing the degrees of freedom of GGDM samplers by maximizing sample quality
scores via gradient descent leads to improved sample quality. Our optimization
procedure backpropagates through the sampling process using the
reparametrization trick and gradient rematerialization. DDSS achieves strong
results on unconditional image generation across various datasets (e.g., FID
scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82
with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines).
Our method is compatible with any pre-trained diffusion model without
fine-tuning or re-training required.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Diffusion Probabilistic Model for Speech Enhancement
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 10, 2022 </span>    
         <span class="authors"> Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, Yu Tsao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.05256" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Speech enhancement is a critical component of many user-oriented audio
applications, yet current systems still suffer from distorted and unnatural
outputs. While generative models have shown strong potential in speech
synthesis, they are still lagging behind in speech enhancement. This work
leverages recent advances in diffusion probabilistic models, and proposes a
novel speech enhancement algorithm that incorporates characteristics of the
observed noisy speech signal into the diffusion and reverse processes. More
specifically, we propose a generalized formulation of the diffusion
probabilistic model named conditional diffusion probabilistic model that, in
its reverse process, can adapt to non-Gaussian real noises in the estimated
speech signal. In our experiments, we demonstrate strong performance of the
proposed approach compared to representative generative models, and investigate
the generalization capability of our models to other datasets with noise
characteristics unseen during training.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion bridges vector quantized Variational AutoEncoders
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 10, 2022 </span>    
         <span class="authors"> Max Cohen, Guillaume Quispe, Sylvain Le Corff, Charles Ollion, Eric Moulines </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.04895" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Vector Quantized-Variational AutoEncoders (VQ-VAE) are generative models
based on discrete latent representations of the data, where inputs are mapped
to a finite set of learned embeddings.To generate new samples, an
autoregressive prior distribution over the discrete states must be trained
separately. This prior is generally very complex and leads to slow generation.
In this work, we propose a new model to train the prior and the encoder/decoder
networks simultaneously. We build a diffusion bridge between a continuous coded
vector and a non-informative prior distribution. The latent discrete states are
then given as random functions of these continuous vectors. We show that our
model is competitive with the autoregressive prior on the mini-Imagenet and
CIFAR dataset and is efficient in both optimization and sampling. Our framework
also extends the standard VQ-VAE and enables end-to-end training.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 08, 2022 </span>    
         <span class="authors"> Zehua Chen, Xu Tan, Ke Wang, Shifeng Pan, Danilo Mandic, Lei He, Sheng Zhao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.03751" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.AI, cs.CL, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (diffusion models for short) require
a large number of iterations in inference to achieve the generation quality
that matches or surpasses the state-of-the-art generative models, which
invariably results in slow inference speed. Previous approaches aim to optimize
the choice of inference schedule over a few iterations to speed up inference.
However, this results in reduced generation quality, mainly because the
inference process is optimized separately, without jointly optimizing with the
training process. In this paper, we propose InferGrad, a diffusion model for
vocoder that incorporates inference process into training, to reduce the
inference iterations while maintaining high generation quality. More
specifically, during training, we generate data from random noise through a
reverse process under inference schedules with a few iterations, and impose a
loss to minimize the gap between the generated and ground-truth data samples.
Then, unlike existing approaches, the training of InferGrad considers the
inference process. The advantages of InferGrad are demonstrated through
experiments on the LJSpeech dataset showing that InferGrad achieves better
voice quality than the baseline WaveGrad under same conditions while
maintaining the same voice quality as the baseline but with $3$x speedup ($2$
iterations for InferGrad vs $6$ iterations for WaveGrad).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Riemannian Score-Based Generative Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 06, 2022 </span>    
         <span class="authors"> Valentin De Bortoli, Emile Mathieu, Michael Hutchinson, James Thornton, Yee Whye Teh, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.02763" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, math.PR, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) are a powerful class of generative
models that exhibit remarkable empirical performance. Score-based generative
modelling (SGM) consists of a ``noising'' stage, whereby a diffusion is used to
gradually add Gaussian noise to data, and a generative model, which entails a
``denoising'' process defined by approximating the time-reversal of the
diffusion. Existing SGMs assume that data is supported on a Euclidean space,
i.e. a manifold with flat geometry. In many domains such as robotics,
geoscience or protein modelling, data is often naturally described by
distributions living on Riemannian manifolds and current SGM techniques are not
appropriate. We introduce here Riemannian Score-based Generative Models
(RSGMs), a class of generative models extending SGMs to Riemannian manifolds.
We demonstrate our approach on a variety of manifolds, and in particular with
earth and climate science spherical data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 05, 2022 </span>    
         <span class="authors"> Jaehyeong Jo, Seul Lee, Sung Ju Hwang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.02514" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generating graph-structured data requires learning the underlying
distribution of graphs. Yet, this is a challenging problem, and the previous
graph generative methods either fail to capture the permutation-invariance
property of graphs or cannot sufficiently model the complex dependency between
nodes and edges, which is crucial for generating real-world graphs such as
molecules. To overcome such limitations, we propose a novel score-based
generative model for graphs with a continuous-time framework. Specifically, we
propose a new graph diffusion process that models the joint distribution of the
nodes and edges through a system of stochastic differential equations (SDEs).
Then, we derive novel score matching objectives tailored for the proposed
diffusion process to estimate the gradient of the joint log-density with
respect to each component, and introduce a new solver for the system of SDEs to
efficiently sample from the reverse diffusion process. We validate our graph
generation method on diverse datasets, on which it either achieves
significantly superior or competitive performance to the baselines. Further
analysis shows that our method is able to generate molecules that lie close to
the training distribution yet do not violate the chemical valency rule,
demonstrating the effectiveness of the system of SDEs in modeling the node-edge
relationships. Our code is available at https://github.com/harryjo97/GDSS.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Progressive Distillation for Fast Sampling of Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 01, 2022 </span>    
         <span class="authors"> Tim Salimans, Jonathan Ho </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2202.00512" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently shown great promise for generative modeling,
outperforming GANs on perceptual quality and autoregressive models at density
estimation. A remaining downside is their slow sampling time: generating high
quality samples takes many hundreds or thousands of model evaluations. Here we
make two contributions to help eliminate this downside: First, we present new
parameterizations of diffusion models that provide increased stability when
using few sampling steps. Second, we present a method to distill a trained
deterministic diffusion sampler, using many steps, into a new diffusion model
that takes half as many sampling steps. We then keep progressively applying
this distillation procedure to our model, halving the number of required
sampling steps each time. On standard image generation benchmarks like
CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers
taking as many as 8192 steps, and are able to distill down to models taking as
few as 4 steps without losing much perceptual quality; achieving, for example,
a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive
distillation procedure does not take more time than it takes to train the
original model, thus representing an efficient solution for generative modeling
using diffusion at both train and test time.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## From data to functa: Your data point is a function and you should treat it like one
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 28, 2022 </span>    
         <span class="authors"> Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Rezende, Dan Rosenbaum </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2201.12204" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 It is common practice in deep learning to represent a measurement of the
world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying
signal represented by these measurements is often continuous, e.g. the scene
depicted in an image. A powerful continuous alternative is then to represent
these measurements using an implicit neural representation, a neural function
trained to output the appropriate measurement value for any input spatial
location. In this paper, we take this idea to its next level: what would it
take to perform deep learning on these functions instead, treating them as
data? In this context we refer to the data as functa, and propose a framework
for deep learning on functa. This view presents a number of challenges around
efficient conversion from data to functa, compact representation of functa, and
effectively solving downstream tasks on functa. We outline a recipe to overcome
these challenges and apply it to a wide range of data modalities including
images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We
demonstrate that this approach has various compelling properties across data
modalities, in particular on the canonical tasks of generative modeling, data
imputation, novel view synthesis and classification. Code:
https://github.com/deepmind/functa
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 28, 2022 </span>    
         <span class="authors"> Songxiang Liu, Dan Su, Dong Yu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2201.11972" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.CL, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) are expressive generative
models that have been used to solve a variety of speech synthesis problems.
However, because of their high sampling costs, DDPMs are difficult to use in
real-time speech processing applications. In this paper, we introduce
DiffGAN-TTS, a novel DDPM-based text-to-speech (TTS) model achieving
high-fidelity and efficient speech synthesis. DiffGAN-TTS is based on denoising
diffusion generative adversarial networks (GANs), which adopt an
adversarially-trained expressive model to approximate the denoising
distribution. We show with multi-speaker TTS experiments that DiffGAN-TTS can
generate high-fidelity speech samples within only 4 denoising steps. We present
an active shallow diffusion mechanism to further speed up inference. A
two-stage training scheme is proposed, with a basic TTS acoustic model trained
at stage one providing valuable prior information for a DDPM trained at stage
two. Our experiments show that DiffGAN-TTS can achieve high synthesis
performance with only 1 denoising step.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Diffusion Restoration Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2022 </span>    
         <span class="authors"> Bahjat Kawar, Michael Elad, Stefano Ermon, Jiaming Song </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2201.11793" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Many interesting tasks in image restoration can be cast as linear inverse
problems. A recent family of approaches for solving these problems uses
stochastic algorithms that sample from the posterior distribution of natural
images given the measurements. However, efficient solutions often require
problem-specific supervised training to model the posterior, whereas
unsupervised methods that are not problem-specific typically rely on
inefficient iterative methods. This work addresses these issues by introducing
Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised
posterior sampling method. Motivated by variational inference, DDRM takes
advantage of a pre-trained denoising diffusion generative model for solving any
linear inverse problem. We demonstrate DDRM's versatility on several image
datasets for super-resolution, deblurring, inpainting, and colorization under
various amounts of measurement noise. DDRM outperforms the current leading
unsupervised methods on the diverse ImageNet dataset in reconstruction quality,
perceptual quality, and runtime, being 5x faster than the nearest competitor.
DDRM also generalizes well for natural images out of the distribution of the
observed ImageNet training set.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 27, 2022 </span>    
         <span class="authors"> Dewei Hu, Yuankai K. Tao, Ipek Oguz </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2201.11760" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Optical coherence tomography (OCT) is a prevalent non-invasive imaging method
which provides high resolution volumetric visualization of retina. However, its
inherent defect, the speckle noise, can seriously deteriorate the tissue
visibility in OCT. Deep learning based approaches have been widely used for
image restoration, but most of these require a noise-free reference image for
supervision. In this study, we present a diffusion probabilistic model that is
fully unsupervised to learn from noise instead of signal. A diffusion process
is defined by adding a sequence of Gaussian noise to self-fused OCT b-scans.
Then the reverse process of diffusion, modeled by a Markov chain, provides an
adjustable level of denoising. Our experiment results demonstrate that our
method can significantly improve the image quality with a simple working
pipeline and a small amount of training data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## RePaint: Inpainting using Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 24, 2022 </span>    
         <span class="authors"> Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, Luc Van Gool </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2201.09865" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Free-form inpainting is the task of adding new content to an image in the
regions specified by an arbitrary binary mask. Most existing approaches train
for a certain distribution of masks, which limits their generalization
capabilities to unseen mask types. Furthermore, training with pixel-wise and
perceptual losses often leads to simple textural extensions towards the missing
areas instead of semantically meaningful generation. In this work, we propose
RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting
approach that is applicable to even extreme masks. We employ a pretrained
unconditional DDPM as the generative prior. To condition the generation
process, we only alter the reverse diffusion iterations by sampling the
unmasked regions using the given image information. Since this technique does
not modify or condition the original DDPM network itself, the model produces
high-quality and diverse output images for any inpainting form. We validate our
method for both faces and general-purpose image inpainting using standard and
extreme masks.
  RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for
at least five out of six mask distributions.
  Github Repository: git.io/RePaint
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 17, 2022 </span>    
         <span class="authors"> Fan Bao, Chongxuan Li, Jun Zhu, Bo Zhang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2201.06503" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPMs) represent a class of powerful
generative models. Despite their success, the inference of DPMs is expensive
since it generally needs to iterate over thousands of timesteps. A key problem
in the inference is to estimate the variance in each timestep of the reverse
process. In this work, we present a surprising result that both the optimal
reverse variance and the corresponding optimal KL divergence of a DPM have
analytic forms w.r.t. its score function. Building upon it, we propose
Analytic-DPM, a training-free inference framework that estimates the analytic
forms of the variance and KL divergence using the Monte Carlo method and a
pretrained score-based model. Further, to correct the potential bias caused by
the score-based model, we derive both lower and upper bounds of the optimal
variance and clip the estimate for a better result. Empirically, our
analytic-DPM improves the log-likelihood of various DPMs, produces high-quality
samples, and meanwhile enjoys a 20x to 80x speed up.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Probabilistic Mass Mapping with Neural Score Estimation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 14, 2022 </span>    
         <span class="authors"> Benjamin Remy, Francois Lanusse, Niall Jeffrey, Jia Liu, Jean-Luc Starck, Ken Osato, Tim Schrabback </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2201.05561" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  astro-ph.CO, astro-ph.IM, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Weak lensing mass-mapping is a useful tool to access the full distribution of
dark matter on the sky, but because of intrinsic galaxy ellipticies and finite
fields/missing data, the recovery of dark matter maps constitutes a challenging
ill-posed inverse problem. We introduce a novel methodology allowing for
efficient sampling of the high-dimensional Bayesian posterior of the weak
lensing mass-mapping problem, and relying on simulations for defining a fully
non-Gaussian prior. We aim to demonstrate the accuracy of the method on
simulations, and then proceed to applying it to the mass reconstruction of the
HST/ACS COSMOS field. The proposed methodology combines elements of Bayesian
statistics, analytic theory, and a recent class of Deep Generative Models based
on Neural Score Matching. This approach allows us to do the following: 1) Make
full use of analytic cosmological theory to constrain the 2pt statistics of the
solution. 2) Learn from cosmological simulations any differences between this
analytic prior and full simulations. 3) Obtain samples from the full Bayesian
posterior of the problem for robust Uncertainty Quantification. We demonstrate
the method on the $\kappa$TNG simulations and find that the posterior mean
significantly outperfoms previous methods (Kaiser-Squires, Wiener filter,
Sparsity priors) both on root-mean-square error and in terms of the Pearson
correlation. We further illustrate the interpretability of the recovered
posterior by establishing a close correlation between posterior convergence
values and SNR of clusters artificially introduced into a field. Finally, we
apply the method to the reconstruction of the HST/ACS COSMOS field and yield
the highest quality convergence map of this field to date.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 02, 2022 </span>    
         <span class="authors"> Kushagra Pandey, Avideep Mukherjee, Piyush Rai, Abhishek Kumar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2201.00308" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models have been shown to generate state-of-the-art
results on several competitive image synthesis benchmarks but lack a
low-dimensional, interpretable latent space, and are slow at generation. On the
other hand, standard Variational Autoencoders (VAEs) typically have access to a
low-dimensional latent space but exhibit poor sample quality. We present
DiffuseVAE, a novel generative framework that integrates VAE within a diffusion
model framework, and leverage this to design novel conditional
parameterizations for diffusion models. We show that the resulting model equips
diffusion models with a low-dimensional VAE inferred latent code which can be
used for downstream tasks like controllable synthesis. The proposed method also
improves upon the speed vs quality tradeoff exhibited in standard unconditional
DDPM/DDIM models (for instance, FID of 16.47 vs 34.36 using a standard DDIM on
the CelebA-HQ-128 benchmark using T=10 reverse process steps) without having
explicitly trained for such an objective. Furthermore, the proposed model
exhibits synthesis quality comparable to state-of-the-art models on standard
image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most
existing VAE-based methods. Lastly, we show that the proposed method exhibits
inherent generalization to different types of noise in the conditioning signal.
For reproducibility, our source code is publicly available at
https://github.com/kpandey008/DiffuseVAE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Ito-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 26, 2021 </span>    
         <span class="authors"> Hideyuki Tachibana, Mocho Go, Muneyoshi Inahara, Yotaro Katayama, Yotaro Watanabe </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.13339" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CV, cs.LG, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion generative models have emerged as a new challenger to popular deep
neural generative models such as GANs, but have the drawback that they often
require a huge number of neural function evaluations (NFEs) during synthesis
unless some sophisticated sampling strategies are employed. This paper proposes
new efficient samplers based on the numerical schemes derived by the familiar
Taylor expansion, which directly solves the ODE/SDE of interest. In general, it
is not easy to compute the derivatives that are required in higher-order Taylor
schemes, but in the case of diffusion models, this difficulty is alleviated by
the trick that the authors call ``ideal derivative substitution,'' in which the
higher-order derivatives are replaced by tractable ones. To derive ideal
derivatives, the authors argue the ``single point approximation,'' in which the
true score function is approximated by a conditional one, holds in many cases,
and considered the derivatives of this approximation. Applying thus obtained
new quasi-Taylor samplers to image generation tasks, the authors experimentally
confirmed that the proposed samplers could synthesize plausible images in small
number of NFEs, and that the performance was better or at the same level as
DDIM and Runge-Kutta methods. The paper also argues the relevance of the
proposed samplers to the existing ones mentioned above.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## High-Resolution Image Synthesis with Latent Diffusion Models 
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 20, 2021 </span>    
         <span class="authors"> Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.10752" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 By decomposing the image formation process into a sequential application of
denoising autoencoders, diffusion models (DMs) achieve state-of-the-art
synthesis results on image data and beyond. Additionally, their formulation
allows for a guiding mechanism to control the image generation process without
retraining. However, since these models typically operate directly in pixel
space, optimization of powerful DMs often consumes hundreds of GPU days and
inference is expensive due to sequential evaluations. To enable DM training on
limited computational resources while retaining their quality and flexibility,
we apply them in the latent space of powerful pretrained autoencoders. In
contrast to previous work, training diffusion models on such a representation
allows for the first time to reach a near-optimal point between complexity
reduction and detail preservation, greatly boosting visual fidelity. By
introducing cross-attention layers into the model architecture, we turn
diffusion models into powerful and flexible generators for general conditioning
inputs such as text or bounding boxes and high-resolution synthesis becomes
possible in a convolutional manner. Our latent diffusion models (LDMs) achieve
a new state of the art for image inpainting and highly competitive performance
on various tasks, including unconditional image generation, semantic scene
synthesis, and super-resolution, while significantly reducing computational
requirements compared to pixel-based DMs. Code is available at
https://github.com/CompVis/latent-diffusion .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 20, 2021 </span>    
         <span class="authors"> Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.10741" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently been shown to generate high-quality synthetic
images, especially when paired with a guidance technique to trade off diversity
for fidelity. We explore diffusion models for the problem of text-conditional
image synthesis and compare two different guidance strategies: CLIP guidance
and classifier-free guidance. We find that the latter is preferred by human
evaluators for both photorealism and caption similarity, and often produces
photorealistic samples. Samples from a 3.5 billion parameter text-conditional
diffusion model using classifier-free guidance are favored by human evaluators
to those from DALL-E, even when the latter uses expensive CLIP reranking.
Additionally, we find that our models can be fine-tuned to perform image
inpainting, enabling powerful text-driven image editing. We train a smaller
model on a filtered dataset and release the code and weights at
https://github.com/openai/glide-text2im.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Heavy-tailed denoising score matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 17, 2021 </span>    
         <span class="authors"> Jacob Deasy, Nikola Simidjievski, Pietro Liò </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.09788" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based model research in the last few years has produced state of the
art generative models by employing Gaussian denoising score-matching (DSM).
However, the Gaussian noise assumption has several high-dimensional
limitations, motivating a more concrete route toward even higher dimension PDF
estimation in future. We outline this limitation, before extending the theory
to a broader family of noising distributions -- namely, the generalised normal
distribution. To theoretically ground this, we relax a key assumption in
(denoising) score matching theory, demonstrating that distributions which are
differentiable almost everywhere permit the same objective simplification as
Gaussians. For noise vector norm distributions, we demonstrate favourable
concentration of measure in the high-dimensional spaces prevalent in deep
learning. In the process, we uncover a skewed noise vector norm distribution
and develop an iterative noise scaling algorithm to consistently initialise the
multiple levels of noise in annealed Langevin dynamics (LD). On the practical
side, our use of heavy-tailed DSM leads to improved score estimation,
controllable sampling convergence, and more balanced unconditional generative
performance for imbalanced datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Tackling the Generative Learning Trilemma with Denoising Diffusion GANs
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 15, 2021 </span>    
         <span class="authors"> Zhisheng Xiao, Karsten Kreis, Arash Vahdat </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.07804" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 A wide variety of deep generative models has been developed in the past
decade. Yet, these models often struggle with simultaneously addressing three
key requirements including: high sample quality, mode coverage, and fast
sampling. We call the challenge imposed by these requirements the generative
learning trilemma, as the existing models often trade some of them for others.
Particularly, denoising diffusion models have shown impressive sample quality
and diversity, but their expensive sampling does not yet allow them to be
applied in many real-world applications. In this paper, we argue that slow
sampling in these models is fundamentally attributed to the Gaussian assumption
in the denoising step which is justified only for small step sizes. To enable
denoising with large steps, and hence, to reduce the total number of denoising
steps, we propose to model the denoising distribution using a complex
multimodal distribution. We introduce denoising diffusion generative
adversarial networks (denoising diffusion GANs) that model each denoising step
using a multimodal conditional GAN. Through extensive evaluations, we show that
denoising diffusion GANs obtain sample quality and diversity competitive with
original diffusion models while being 2000$\times$ faster on the CIFAR-10
dataset. Compared to traditional GANs, our model exhibits better mode coverage
and sample diversity. To the best of our knowledge, denoising diffusion GAN is
the first model that reduces sampling cost in diffusion models to an extent
that allows them to be applied to real-world applications inexpensively.
Project page and code can be found at
https://nvlabs.github.io/denoising-diffusion-gan
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Generative Modeling with Critically-Damped Langevin Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 14, 2021 </span>    
         <span class="authors"> Tim Dockhorn, Arash Vahdat, Karsten Kreis </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.07068" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) have demonstrated remarkable synthesis
quality. SGMs rely on a diffusion process that gradually perturbs the data
towards a tractable distribution, while the generative model learns to denoise.
The complexity of this denoising task is, apart from the data distribution
itself, uniquely determined by the diffusion process. We argue that current
SGMs employ overly simplistic diffusions, leading to unnecessarily complex
denoising processes, which limit generative modeling performance. Based on
connections to statistical mechanics, we propose a novel critically-damped
Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior
performance. CLD can be interpreted as running a joint diffusion in an extended
space, where the auxiliary variables can be considered "velocities" that are
coupled to the data variables as in Hamiltonian dynamics. We derive a novel
score matching objective for CLD and show that the model only needs to learn
the score function of the conditional distribution of the velocity given data,
an easier task than learning scores of the data directly. We also derive a new
sampling scheme for efficient synthesis from CLD-based diffusion models. We
find that CLD outperforms previous SGMs in synthesis quality for similar
network architectures and sampling compute budgets. We show that our novel
sampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our
framework provides new insights into score-based denoising diffusion models and
can be readily used for high-resolution image synthesis. Project page and code:
https://nv-tlabs.github.io/CLD-SGM.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Step-unrolled Denoising Autoencoders for Text Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 13, 2021 </span>    
         <span class="authors"> Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, Aaron van den Oord </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.06749" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper we propose a new generative model of text, Step-unrolled
Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models.
Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a
sequence of tokens, starting from random inputs and improving them each time
until convergence. We present a simple new improvement operator that converges
in fewer iterations than diffusion methods, while qualitatively producing
better samples on natural language datasets. SUNDAE achieves state-of-the-art
results (among non-autoregressive methods) on the WMT'14 English-to-German
translation task and good qualitative results on unconditional language
modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python
code from GitHub. The non-autoregressive nature of SUNDAE opens up
possibilities beyond left-to-right prompted generation, by filling in arbitrary
blank patterns in a template.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## More Control for Free! Image Synthesis with Semantic Diffusion Guidance
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 10, 2021 </span>    
         <span class="authors"> Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, Trevor Darrell </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.05744" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Controllable image synthesis models allow creation of diverse images based on
text instructions or guidance from a reference image. Recently, denoising
diffusion probabilistic models have been shown to generate more realistic
imagery than prior methods, and have been successfully demonstrated in
unconditional and class-conditional settings. We investigate fine-grained,
continuous control of this model class, and introduce a novel unified framework
for semantic diffusion guidance, which allows either language or image
guidance, or both. Guidance is injected into a pretrained unconditional
diffusion model using the gradient of image-text or image matching scores,
without re-training the diffusion model. We explore CLIP-based language
guidance as well as both content and style-based image guidance in a unified
framework. Our text-guided synthesis approach can be applied to datasets
without associated text annotations. We conduct experiments on FFHQ and LSUN
datasets, and show results on fine-grained text-guided image synthesis,
synthesis of images related to a style or content reference image, and examples
with both textual and image guidance.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffuseMorph: Unsupervised Deformable Image Registration Along Continuous Trajectory Using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 09, 2021 </span>    
         <span class="authors"> Boah Kim, Inhwa Han, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.05149" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deformable image registration is one of the fundamental tasks in medical
imaging. Classical registration algorithms usually require a high computational
cost for iterative optimizations. Although deep-learning-based methods have
been developed for fast image registration, it is still challenging to obtain
realistic continuous deformations from a moving image to a fixed image with
less topological folding problem. To address this, here we present a novel
diffusion-model-based image registration method, called DiffuseMorph.
DiffuseMorph not only generates synthetic deformed images through reverse
diffusion but also allows image registration by deformation fields.
Specifically, the deformation fields are generated by the conditional score
function of the deformation between the moving and fixed images, so that the
registration can be performed from continuous deformation by simply scaling the
latent feature of the score. Experimental results on 2D facial and 3D medical
image registration tasks demonstrate that our method provides flexible
deformations with topology preservation capability.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 09, 2021 </span>    
         <span class="authors"> Hyungjin Chung, Byeongsu Sim, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.05146" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion models have recently attained significant interest within the
community owing to their strong performance as generative models. Furthermore,
its application to inverse problems have demonstrated state-of-the-art
performance. Unfortunately, diffusion models have a critical downside - they
are inherently slow to sample from, needing few thousand steps of iteration to
generate images from pure Gaussian noise. In this work, we show that starting
from Gaussian noise is unnecessary. Instead, starting from a single forward
diffusion with better initialization significantly reduces the number of
sampling steps in the reverse conditional diffusion. This phenomenon is
formally explained by the contraction theory of the stochastic difference
equations like our conditional diffusion strategy - the alternating
applications of reverse diffusion followed by a non-expansive data consistency
step. The new sampling strategy, dubbed Come-Closer-Diffuse-Faster (CCDF), also
reveals a new insight on how the existing feed-forward neural network
approaches for inverse problems can be synergistically combined with the
diffusion models. Experimental results with super-resolution, image inpainting,
and compressed sensing MRI demonstrate that our method can achieve
state-of-the-art reconstruction performance at significantly reduced sampling
steps.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Conditional Point DIffusion-Refinement Paradigm for 3D Point Cloud Completion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 07, 2021 </span>    
         <span class="authors"> Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, Dahua Lin </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.03530" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 3D point cloud is an important 3D representation for capturing real world 3D
objects. However, real-scanned 3D point clouds are often incomplete, and it is
important to recover complete point clouds for downstream applications. Most
existing point cloud completion methods use Chamfer Distance (CD) loss for
training. The CD loss estimates correspondences between two point clouds by
searching nearest neighbors, which does not capture the overall point density
distribution on the generated shape, and therefore likely leads to non-uniform
point cloud generation. To tackle this problem, we propose a novel Point
Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of
a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The
CGNet uses a conditional generative model called the denoising diffusion
probabilistic model (DDPM) to generate a coarse completion conditioned on the
partial observation. DDPM establishes a one-to-one pointwise mapping between
the generated point cloud and the uniform ground truth, and then optimizes the
mean squared error loss to realize uniform generation. The RFNet refines the
coarse output of the CGNet and further improves quality of the completed point
cloud. Furthermore, we develop a novel dual-path architecture for both
networks. The architecture can (1) effectively and efficiently extract
multi-level features from partially observed point clouds to guide completion,
and (2) accurately manipulate spatial locations of 3D points to obtain smooth
surfaces and sharp details. Extensive experimental results on various benchmark
datasets show that our PDR paradigm outperforms previous state-of-the-art
methods for point cloud completion. Remarkably, with the help of the RFNet, we
can accelerate the iterative generation process of the DDPM by up to 50 times
without much performance drop.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Deblurring via Stochastic Refinement 
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 05, 2021 </span>    
         <span class="authors"> Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, Peyman Milanfar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.02475" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image deblurring is an ill-posed problem with multiple plausible solutions
for a given input image. However, most existing methods produce a deterministic
estimate of the clean image and are trained to minimize pixel-level distortion.
These metrics are known to be poorly correlated with human perception, and
often lead to unrealistic reconstructions. We present an alternative framework
for blind deblurring based on conditional diffusion models. Unlike existing
techniques, we train a stochastic sampler that refines the output of a
deterministic predictor and is capable of producing a diverse set of plausible
reconstructions for a given input. This leads to a significant improvement in
perceptual quality over existing state-of-the-art methods across multiple
standard benchmarks. Our predict-and-refine approach also enables much more
efficient sampling compared to typical diffusion models. Combined with a
carefully tuned network architecture and inference procedure, our method is
competitive in terms of distortion metrics such as PSNR. These results show
clear benefits of our diffusion-based method for deblurring and challenge the
widely used strategy of producing a single, deterministic reconstruction.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Noise Distribution Adaptive Self-Supervised Image Denoising using Tweedie Distribution and Score-Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 05, 2021 </span>    
         <span class="authors"> Kwanyoung Kim, Taesung Kwon, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.03696" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Tweedie distributions are a special case of exponential dispersion models,
which are often used in classical statistics as distributions for generalized
linear models. Here, we reveal that Tweedie distributions also play key roles
in modern deep learning era, leading to a distribution independent
self-supervised image denoising formula without clean reference images.
Specifically, by combining with the recent Noise2Score self-supervised image
denoising approach and the saddle point approximation of Tweedie distribution,
we can provide a general closed-form denoising formula that can be used for
large classes of noise distributions without ever knowing the underlying noise
distribution. Similar to the original Noise2Score, the new approach is composed
of two successive steps: score matching using perturbed noisy images, followed
by a closed form image denoising formula via distribution-independent Tweedie's
formula. This also suggests a systematic algorithm to estimate the noise model
and noise parameters for a given noisy image data set. Through extensive
experiments, we demonstrate that the proposed method can accurately estimate
noise models and parameters, and provide the state-of-the-art self-supervised
image denoising performance in the benchmark dataset and real-world dataset.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SegDiff: Image Segmentation with Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 01, 2021 </span>    
         <span class="authors"> Tomer Amit, Tal Shaharbany, Eliya Nachmani, Lior Wolf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2112.00390" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion Probabilistic Methods are employed for state-of-the-art image
generation. In this work, we present a method for extending such models for
performing image segmentation. The method learns end-to-end, without relying on
a pre-trained backbone. The information in the input image and in the current
estimation of the segmentation map is merged by summing the output of two
encoders. Additional encoding layers and a decoder are then used to iteratively
refine the segmentation map, using a diffusion model. Since the diffusion model
is probabilistic, it is applied multiple times, and the results are merged into
a final segmentation map. The new method produces state-of-the-art results on
the Cityscapes validation set, the Vaihingen building segmentation benchmark,
and the MoNuSeg dataset.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Autoencoders: Toward a Meaningful and Decodable Representation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 30, 2021 </span>    
         <span class="authors"> Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, Supasorn Suwajanakorn </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.15640" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion probabilistic models (DPMs) have achieved remarkable quality in
image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent
variables that lack semantic meaning and cannot serve as a useful
representation for other tasks. This paper explores the possibility of using
DPMs for representation learning and seeks to extract a meaningful and
decodable representation of an input image via autoencoding. Our key idea is to
use a learnable encoder for discovering the high-level semantics, and a DPM as
the decoder for modeling the remaining stochastic variations. Our method can
encode any image into a two-part latent code, where the first part is
semantically meaningful and linear, and the second part captures stochastic
details, allowing near-exact reconstruction. This capability enables
challenging applications that currently foil GAN-based methods, such as
attribute manipulation on real images. We also show that this two-level
encoding improves denoising efficiency and naturally facilitates various
downstream tasks including few-shot conditional sampling. Please visit our
project page: https://Diff-AE.github.io/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Vector Quantized Diffusion Model for Text-to-Image Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 29, 2021 </span>    
         <span class="authors"> Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.14822" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present the vector quantized diffusion (VQ-Diffusion) model for
text-to-image generation. This method is based on a vector quantized
variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional
variant of the recently developed Denoising Diffusion Probabilistic Model
(DDPM). We find that this latent-space method is well-suited for text-to-image
generation tasks because it not only eliminates the unidirectional bias with
existing methods but also allows us to incorporate a mask-and-replace diffusion
strategy to avoid the accumulation of errors, which is a serious problem with
existing methods. Our experiments show that the VQ-Diffusion produces
significantly better text-to-image generation results when compared with
conventional autoregressive (AR) models with similar numbers of parameters.
Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can
handle more complex scenes and improve the synthesized image quality by a large
margin. Finally, we show that the image generation computation in our method
can be made highly efficient by reparameterization. With traditional AR
methods, the text-to-image generation time increases linearly with the output
image resolution and hence is quite time consuming even for normal size images.
The VQ-Diffusion allows us to achieve a better trade-off between quality and
speed. Our experiments indicate that the VQ-Diffusion model with the
reparameterization is fifteen times faster than traditional AR methods while
achieving a better image quality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Blended Diffusion for Text-driven Editing of Natural Images
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 29, 2021 </span>    
         <span class="authors"> Omri Avrahami, Dani Lischinski, Ohad Fried </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.14818" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.GR, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Natural language offers a highly intuitive interface for image editing. In
this paper, we introduce the first solution for performing local (region-based)
edits in generic natural images, based on a natural language description along
with an ROI mask. We achieve our goal by leveraging and combining a pretrained
language-image model (CLIP), to steer the edit towards a user-provided text
prompt, with a denoising diffusion probabilistic model (DDPM) to generate
natural-looking results. To seamlessly fuse the edited region with the
unchanged parts of the image, we spatially blend noised versions of the input
image with the local text-guided diffusion latent at a progression of noise
levels. In addition, we show that adding augmentations to the diffusion process
mitigates adversarial results. We compare against several baselines and related
methods, both qualitatively and quantitatively, and show that our method
outperforms these solutions in terms of overall realism, ability to preserve
the background and matching the text. Finally, we show several text-driven
editing applications, including adding a new object to an image,
removing/replacing/altering existing objects, background replacement, and image
extrapolation. Code is available at:
https://omriavrahami.com/blended-diffusion-page/
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Conditional Image Generation with Score-Based Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 26, 2021 </span>    
         <span class="authors"> Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.13606" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based diffusion models have emerged as one of the most promising
frameworks for deep generative modelling. In this work we conduct a systematic
comparison and theoretical analysis of different approaches to learning
conditional probability distributions with score-based diffusion models. In
particular, we prove results which provide a theoretical justification for one
of the most successful estimators of the conditional score. Moreover, we
introduce a multi-speed diffusion framework, which leads to a new estimator for
the conditional score, performing on par with previous state-of-the-art
approaches. Our theoretical and experimental findings are accompanied by an
open source library MSDiff which allows for application and further research of
multi-speed diffusion models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Solving Inverse Problems in Medical Imaging with Score-Based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 15, 2021 </span>    
         <span class="authors"> Yang Song, Liyue Shen, Lei Xing, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.08005" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Reconstructing medical images from partial measurements is an important
inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging
(MRI). Existing solutions based on machine learning typically train a model to
directly map measurements to medical images, leveraging a training dataset of
paired images and measurements. These measurements are typically synthesized
from images using a fixed physical model of the measurement process, which
hinders the generalization capability of models to unknown measurement
processes. To address this issue, we propose a fully unsupervised technique for
inverse problem solving, leveraging the recently introduced score-based
generative models. Specifically, we first train a score-based generative model
on medical images to capture their prior distribution. Given measurements and a
physical model of the measurement process at test time, we introduce a sampling
method to reconstruct an image consistent with both the prior and the observed
measurements. Our method does not assume a fixed measurement process during
training, and can thus be flexibly adapted to different measurement processes
at test time. Empirically, we observe comparable or better performance to
supervised learning techniques in several medical imaging tasks in CT and MRI,
while demonstrating significantly better generalization to unknown measurement
processes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Simulating Diffusion Bridges with Score Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 14, 2021 </span>    
         <span class="authors"> Jeremy Heng, Valentin De Bortoli, Arnaud Doucet, James Thornton </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.07243" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.CO, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We consider the problem of simulating diffusion bridges, which are diffusion
processes that are conditioned to initialize and terminate at two given states.
The simulation of diffusion bridges has applications in diverse scientific
fields and plays a crucial role in the statistical inference of
discretely-observed diffusions. This is known to be a challenging problem that
has received much attention in the last two decades. This article contributes
to this rich body of literature by presenting a new avenue to obtain diffusion
bridge approximations. Our approach is based on a backward time representation
of a diffusion bridge, which may be simulated if one can time-reverse the
unconditioned diffusion. We introduce a variational formulation to learn this
time-reversal with function approximation and rely on a score matching method
to circumvent intractability. Another iteration of our proposed methodology
approximates the Doob's $h$-transform defining the forward time representation
of a diffusion bridge. We discuss algorithmic considerations and extensions,
and present numerical results on an Ornstein--Uhlenbeck process, a model from
financial econometrics for interest rates, and a model from genetics for cell
differentiation and development to illustrate the effectiveness of our
approach.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Palette: Image-to-Image Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 10, 2021 </span>    
         <span class="authors"> Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, Mohammad Norouzi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.05826" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper develops a unified framework for image-to-image translation based
on conditional diffusion models and evaluates this framework on four
challenging image-to-image translation tasks, namely colorization, inpainting,
uncropping, and JPEG restoration. Our simple implementation of image-to-image
diffusion models outperforms strong GAN and regression baselines on all tasks,
without task-specific hyper-parameter tuning, architecture customization, or
any auxiliary loss or sophisticated new techniques needed. We uncover the
impact of an L2 vs. L1 loss in the denoising diffusion objective on sample
diversity, and demonstrate the importance of self-attention in the neural
architecture through empirical studies. Importantly, we advocate a unified
evaluation protocol based on ImageNet, with human evaluation and sample quality
scores (FID, Inception Score, Classification Accuracy of a pre-trained
ResNet-50, and Perceptual Distance against original images). We expect this
standardized evaluation protocol to play a role in advancing image-to-image
translation research. Finally, we show that a generalist, multi-task diffusion
model performs as well or better than task-specific specialist counterparts.
Check out https://diffusion-palette.github.io for an overview of the results.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Estimating High Order Gradients of the Data Distribution by Denoising
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 08, 2021 </span>    
         <span class="authors"> Chenlin Meng, Yang Song, Wenzhe Li, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.04726" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The first order derivative of a data density can be estimated efficiently by
denoising score matching, and has become an important component in many
applications, such as image generation and audio synthesis. Higher order
derivatives provide additional local information about the data distribution
and enable new applications. Although they can be estimated via automatic
differentiation of a learned density model, this can amplify estimation errors
and is expensive in high dimensional settings. To overcome these limitations,
we propose a method to directly estimate high order derivatives (scores) of a
data density from samples. We first show that denoising score matching can be
interpreted as a particular case of Tweedie's formula. By leveraging Tweedie's
formula on higher order moments, we generalize denoising score matching to
estimate higher order derivatives. We demonstrate empirically that models
trained with the proposed method can approximate second order derivatives more
efficiently and accurately than via automatic differentiation. We show that our
models can be used to quantify uncertainty in denoising and to improve the
mixing speed of Langevin dynamics via Ozaki discretization for sampling
synthetic data and natural images.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Realistic galaxy image simulation via score-based generative models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2021 </span>    
         <span class="authors"> Michael J. Smith, James E. Geach, Ryan A. Jackson, Nikhil Arora, Connor Stone, Stéphane Courteau </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.01713" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  astro-ph.IM, astro-ph.GA, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We show that a Denoising Diffusion Probabalistic Model (DDPM), a class of
score-based generative model, can be used to produce realistic mock images that
mimic observations of galaxies. Our method is tested with Dark Energy
Spectroscopic Instrument (DESI) grz imaging of galaxies from the Photometry and
Rotation curve OBservations from Extragalactic Surveys (PROBES) sample and
galaxies selected from the Sloan Digital Sky Survey. Subjectively, the
generated galaxies are highly realistic when compared with samples from the
real dataset. We quantify the similarity by borrowing from the deep generative
learning literature, using the `Fr\'echet Inception Distance' to test for
subjective and morphological similarity. We also introduce the `Synthetic
Galaxy Distance' metric to compare the emergent physical properties (such as
total magnitude, colour and half light radius) of a ground truth parent and
synthesised child dataset. We argue that the DDPM approach produces sharper and
more realistic images than other generative methods such as Adversarial
Networks (with the downside of more costly inference), and could be used to
produce large samples of synthetic observations tailored to a specific imaging
survey. We demonstrate two potential uses of the DDPM: (1) accurate in-painting
of occluded data, such as satellite trails, and (2) domain transfer, where new
input images can be processed to mimic the properties of the DDPM training set.
Here we `DESI-fy' cartoon images as a proof of concept for domain transfer.
Finally, we suggest potential applications for score-based approaches that
could motivate further research on this topic within the astronomical
community.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Zero-Shot Translation using Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 02, 2021 </span>    
         <span class="authors"> Eliya Nachmani, Shaked Dovrat </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2111.01471" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we show a novel method for neural machine translation (NMT),
using a denoising diffusion probabilistic model (DDPM), adjusted for textual
data, following recent advances in the field. We show that it's possible to
translate sentences non-autoregressively using a diffusion model conditioned on
the source sentence. We also show that our model is able to translate between
pairs of languages unseen during training (zero-shot learning).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Likelihood Training of Schrodinger Bridge using Forward-Backward SDEs Theory
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 21, 2021 </span>    
         <span class="authors"> Tianrong Chen, Guan-Horng Liu, Evangelos A. Theodorou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.11291" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, math.AP, math.OC
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Schr\"odinger Bridge (SB) is an entropy-regularized optimal transport problem
that has received increasing attention in deep generative modeling for its
mathematical flexibility compared to the Scored-based Generative Model (SGM).
However, it remains unclear whether the optimization principle of SB relates to
the modern training of deep generative models, which often rely on constructing
log-likelihood objectives.This raises questions on the suitability of SB models
as a principled alternative for generative applications. In this work, we
present a novel computational framework for likelihood training of SB models
grounded on Forward-Backward Stochastic Differential Equations Theory - a
mathematical methodology appeared in stochastic optimal control that transforms
the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be
used to construct the likelihood objectives for SB that, surprisingly,
generalizes the ones for SGM as special cases. This leads to a new optimization
principle that inherits the same SB optimality yet without losing applications
of modern generative training techniques, and we show that the resulting
training algorithm achieves comparable results on generating realistic images
on MNIST, CelebA, and CIFAR10. Our code is available at
https://github.com/ghliu/SB-FBSDE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Controllable and Compositional Generation with Latent-Space Energy-Based Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 21, 2021 </span>    
         <span class="authors"> Weili Nie, Arash Vahdat, Anima Anandkumar </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.10873" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Controllable generation is one of the key requirements for successful
adoption of deep generative models in real-world applications, but it still
remains as a great challenge. In particular, the compositional ability to
generate novel concept combinations is out of reach for most current models. In
this work, we use energy-based models (EBMs) to handle compositional generation
over a set of attributes. To make them scalable to high-resolution image
generation, we introduce an EBM in the latent space of a pre-trained generative
model such as StyleGAN. We propose a novel EBM formulation representing the
joint distribution of data and attributes together, and we show how sampling
from it is formulated as solving an ordinary differential equation (ODE). Given
a pre-trained generator, all we need for controllable generation is to train an
attribute classifier. Sampling with ODEs is done efficiently in the latent
space and is robust to hyperparameters. Thus, our method is simple, fast to
train, and efficient to sample. Experimental results show that our method
outperforms the state-of-the-art in both conditional sampling and sequential
editing. In compositional generation, our method excels at zero-shot generation
of unseen attribute combinations. Also, by composing energy functions with
logical operators, this work is the first to achieve such compositionality in
generating photo-realistic images of resolution 1024x1024. Code is available at
https://github.com/NVlabs/LACE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Normalizing Flow
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 14, 2021 </span>    
         <span class="authors"> Qinsheng Zhang, Yongxin Chen </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.07579" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a novel generative modeling method called diffusion normalizing
flow based on stochastic differential equations (SDEs). The algorithm consists
of two neural SDEs: a forward SDE that gradually adds noise to the data to
transform the data into Gaussian random noise, and a backward SDE that
gradually removes the noise to sample from the data distribution. By jointly
training the two neural SDEs to minimize a common cost function that quantifies
the difference between the two, the backward SDE converges to a diffusion
process the starts with a Gaussian distribution and ends with the desired data
distribution. Our method is closely related to normalizing flow and diffusion
probabilistic models and can be viewed as a combination of the two. Compared
with normalizing flow, diffusion normalizing flow is able to learn
distributions with sharp boundaries. Compared with diffusion probabilistic
models, diffusion normalizing flow requires fewer discretization steps and thus
has better sampling efficiency. Our algorithm demonstrates competitive
performance in both high-dimension data density estimation and image generation
tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Crystal Diffusion Variational Autoencoder for Periodic Material Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 12, 2021 </span>    
         <span class="authors"> Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, Tommi Jaakkola </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.06197" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cond-mat.mtrl-sci, physics.comp-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generating the periodic structure of stable materials is a long-standing
challenge for the material design community. This task is difficult because
stable materials only exist in a low-dimensional subspace of all possible
periodic arrangements of atoms: 1) the coordinates must lie in the local energy
minimum defined by quantum mechanics, and 2) global stability also requires the
structure to follow the complex, yet specific bonding preferences between
different atom types. Existing methods fail to incorporate these factors and
often lack proper invariances. We propose a Crystal Diffusion Variational
Autoencoder (CDVAE) that captures the physical inductive bias of material
stability. By learning from the data distribution of stable materials, the
decoder generates materials in a diffusion process that moves atomic
coordinates towards a lower energy state and updates atom types to satisfy
bonding preferences between neighbors. Our model also explicitly encodes
interactions across periodic boundaries and respects permutation, translation,
rotation, and periodic invariances. We significantly outperform past methods in
three tasks: 1) reconstructing the input structure, 2) generating valid,
diverse, and realistic materials, and 3) generating materials that optimize a
specific property. We also provide several standard datasets and evaluation
metrics for the broader machine learning community.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based diffusion models for accelerated MRI
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 08, 2021 </span>    
         <span class="authors"> Hyungjin Chung, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.05243" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.AI, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based diffusion models provide a powerful way to model images using the
gradient of the data distribution. Leveraging the learned score function as a
prior, here we introduce a way to sample data from a conditional distribution
given the measurements, such that the model can be readily used for solving
inverse problems in imaging, especially for accelerated MRI. In short, we train
a continuous time-dependent score function with denoising score matching. Then,
at the inference stage, we iterate between numerical SDE solver and data
consistency projection step to achieve reconstruction. Our model requires
magnitude images only for training, and yet is able to reconstruct
complex-valued data, and even extends to parallel imaging. The proposed method
is agnostic to sub-sampling patterns, and can be used with any sampling
schemes. Also, due to its generative nature, our approach can quantify
uncertainty, which is not possible with standard regression settings. On top of
all the advantages, our method also has very strong performance, even beating
the models trained with full supervision. With extensive experiments, we verify
the superiority of our method in terms of quality and practicality.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Generative Neural Networks for Large-Scale Optimal Transport
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 07, 2021 </span>    
         <span class="authors"> Max Daniels, Tyler Maunu, Paul Hand </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.03237" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We consider the fundamental problem of sampling the optimal transport
coupling between given source and target distributions. In certain cases, the
optimal transport plan takes the form of a one-to-one mapping from the source
support to the target support, but learning or even approximating such a map is
computationally challenging for large and high-dimensional datasets due to the
high cost of linear programming routines and an intrinsic curse of
dimensionality. We study instead the Sinkhorn problem, a regularized form of
optimal transport whose solutions are couplings between the source and the
target distribution. We introduce a novel framework for learning the Sinkhorn
coupling between two distributions in the form of a score-based generative
model. Conditioned on source data, our procedure iterates Langevin Dynamics to
sample target data according to the regularized optimal coupling. Key to this
approach is a neural network parametrization of the Sinkhorn problem, and we
prove convergence of gradient descent with respect to network parameters in
this formulation. We demonstrate its empirical success on a variety of large
scale optimal transport tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 06, 2021 </span>    
         <span class="authors"> Gwanghyun Kim, Taesung Kwon, Jong Chul Ye </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.02711v6" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, GAN inversion methods combined with Contrastive Language-Image
Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts.
However, their applications to diverse real images are still difficult due to
the limited GAN inversion capability. Specifically, these approaches often have
difficulties in reconstructing images with novel poses, views, and highly
variable contents compared to the training data, altering object identity, or
producing unwanted image artifacts. To mitigate these problems and enable
faithful manipulation of real images, we propose a novel method, dubbed
DiffusionCLIP, that performs text-driven image manipulation using diffusion
models. Based on full inversion capability and high-quality image generation
power of recent diffusion models, our method performs zero-shot image
manipulation successfully even between unseen domains and takes another step
towards general application by manipulating images from a widely varying
ImageNet dataset. Furthermore, we propose a novel noise combination method that
allows straightforward multi-attribute manipulation. Extensive experiments and
human evaluation confirmed robust and superior manipulation performance of our
methods compared to the existing baselines. Code is available at
https://github.com/gwang-kim/DiffusionCLIP.git.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Autoregressive Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 05, 2021 </span>    
         <span class="authors"> Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.02037" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce Autoregressive Diffusion Models (ARDMs), a model class
encompassing and generalizing order-agnostic autoregressive models (Uria et
al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we
show are special cases of ARDMs under mild assumptions. ARDMs are simple to
implement and easy to train. Unlike standard ARMs, they do not require causal
masking of model representations, and can be trained using an efficient
objective similar to modern probabilistic diffusion models that scales
favourably to highly-dimensional data. At test time, ARDMs support parallel
generation which can be adapted to fit any given generation budget. We find
that ARDMs require significantly fewer steps than discrete diffusion models to
attain the same performance. Finally, we apply ARDMs to lossless compression,
and show that they are uniquely suited to this task. Contrary to existing
approaches based on bits-back coding, ARDMs obtain compelling results not only
on complete datasets, but also on compressing single data points. Moreover,
this can be done using a modest number of network calls for (de)compression due
to the model's adaptable parallel generation.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Generative Classifiers
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 01, 2021 </span>    
         <span class="authors"> Roland S. Zimmermann, Lukas Schott, Yang Song, Benjamin A. Dunn, David A. Klindt </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2110.00473" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The tremendous success of generative models in recent years raises the
question whether they can also be used to perform classification. Generative
models have been used as adversarially robust classifiers on simple datasets
such as MNIST, but this robustness has not been observed on more complex
datasets like CIFAR-10. Additionally, on natural image datasets, previous
results have suggested a trade-off between the likelihood of the data and
classification accuracy. In this work, we investigate score-based generative
models as classifiers for natural images. We show that these models not only
obtain competitive likelihood values but simultaneously achieve
state-of-the-art classification accuracy for generative classifiers on
CIFAR-10. Nevertheless, we find that these models are only slightly, if at all,
more robust than discriminative baseline models on out-of-distribution tasks
based on common image corruptions. Similarly and contrary to prior results, we
find that score-based are prone to worst-case distribution shifts in the form
of adversarial perturbations. Our work highlights that score-based generative
models are closing the gap in classification accuracy compared to standard
discriminative models. While they do not yet deliver on the promise of
adversarial and out-of-domain robustness, they provide a different approach to
classification that warrants further research.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 28, 2021 </span>    
         <span class="authors"> Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, Jiansheng Wei </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2109.13821" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Voice conversion is a common speech synthesis task which can be solved in
different ways depending on a particular real-world scenario. The most
challenging one often referred to as one-shot many-to-many voice conversion
consists in copying the target voice from only one reference utterance in the
most general case when both source and target speakers do not belong to the
training dataset. We present a scalable high-quality solution based on
diffusion probabilistic modeling and demonstrate its superior quality compared
to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on
real-time applications, we investigate general principles which can make
diffusion models faster while keeping synthesis quality at a high level. As a
result, we develop a novel Stochastic Differential Equations solver suitable
for various diffusion model types and generative tasks as shown through
empirical studies and justify it by theoretical analysis.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Bilateral Denoising Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 26, 2021 </span>    
         <span class="authors"> Max W. Y. Lam, Jun Wang, Rongjie Huang, Dan Su, Dong Yu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2108.11514" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.SD, eess.AS, eess.SP
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) have emerged as competitive
generative models yet brought challenges to efficient sampling. In this paper,
we propose novel bilateral denoising diffusion models (BDDMs), which take
significantly fewer steps to generate high-quality samples. From a bilateral
modeling objective, BDDMs parameterize the forward and reverse processes with a
score network and a scheduling network, respectively. We show that a new lower
bound tighter than the standard evidence lower bound can be derived as a
surrogate objective for training the two networks. In particular, BDDMs are
efficient, simple-to-train, and capable of further improving any pre-trained
DDPM by optimizing the inference noise schedules. Our experiments demonstrated
that BDDMs can generate high-fidelity samples with as few as 3 sampling steps
and produce comparable or even higher quality samples than DDPMs using 1000
steps with only 16 sampling steps (a 62x speedup).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 19, 2021 </span>    
         <span class="authors"> Patrick Esser, Robin Rombach, Andreas Blattmann, Björn Ommer </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2108.08827" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Autoregressive models and their sequential factorization of the data
likelihood have recently demonstrated great potential for image representation
and synthesis. Nevertheless, they incorporate image context in a linear 1D
order by attending only to previously synthesized image patches above or to the
left. Not only is this unidirectional, sequential bias of attention unnatural
for images as it disregards large parts of a scene until synthesis is almost
complete. It also processes the entire image on a single scale, thus ignoring
more global contextual information up to the gist of the entire scene. As a
remedy we incorporate a coarse-to-fine hierarchy of context by combining the
autoregressive formulation with a multinomial diffusion process: Whereas a
multistage diffusion process successively removes information to coarsen an
image, we train a (short) Markov chain to invert this process. In each stage,
the resulting autoregressive ImageBART model progressively incorporates context
from previous stages in a coarse-to-fine manner. Experiments show greatly
improved image modification capabilities over autoregressive models while also
providing high-fidelity image generation, both of which are enabled through
efficient training in a compressed latent space. Specifically, our approach can
take unrestricted, user-provided masks into account to perform local image
editing. Thus, in contrast to pure autoregressive models, it can solve
free-form image inpainting and, in the case of conditional models, local,
text-guided image modification without requiring mask-specific training.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## IVLR: Conditioning Method for Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 06, 2021 </span>    
         <span class="authors"> Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2108.02938" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPM) have shown remarkable
performance in unconditional image generation. However, due to the
stochasticity of the generative process in DDPM, it is challenging to generate
images with the desired semantics. In this work, we propose Iterative Latent
Variable Refinement (ILVR), a method to guide the generative process in DDPM to
generate high-quality images based on a given reference image. Here, the
refinement of the generative process in DDPM enables a single DDPM to sample
images from various sets directed by the reference image. The proposed ILVR
method generates high-quality images while controlling the generation. The
controllability of our method allows adaptation of a single DDPM without any
additional learning in various image generation tasks, such as generation from
various downsampling factors, multi-domain image translation, paint-to-image,
and editing with scribbles.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Robust Compressed Sensing MRI with Deep Generative Priors
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 03, 2021 </span>    
         <span class="authors"> Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G. Dimakis, Jonathan I. Tamir </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2108.01368" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.IT, math.IT, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep
generative priors can be powerful tools for solving inverse problems. However,
to date this framework has been empirically successful only on certain datasets
(for example, human faces and MNIST digits), and it is known to perform poorly
on out-of-distribution samples. In this paper, we present the first successful
application of the CSGM framework on clinical MRI data. We train a generative
prior on brain scans from the fastMRI dataset, and show that posterior sampling
via Langevin dynamics achieves high quality reconstructions. Furthermore, our
experiments and theory show that posterior sampling is robust to changes in the
ground-truth distribution and measurement process. Our code and models are
available at: \url{https://github.com/utcsilab/csgm-mri-langevin}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SDEdit: Image Synthesis and Editing with Stochastic Differential Equations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 02, 2021 </span>    
         <span class="authors"> Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2108.01073" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Guided image synthesis enables everyday users to create and edit
photo-realistic images with minimum effort. The key challenge is balancing
faithfulness to the user input (e.g., hand-drawn colored strokes) and realism
of the synthesized image. Existing GAN-based methods attempt to achieve such
balance using either conditional GANs or GAN inversions, which are challenging
and often require additional training data or loss functions for individual
applications. To address these issues, we introduce a new image synthesis and
editing method, Stochastic Differential Editing (SDEdit), based on a diffusion
model generative prior, which synthesizes realistic images by iteratively
denoising through a stochastic differential equation (SDE). Given an input
image with user guide of any type, SDEdit first adds noise to the input, then
subsequently denoises the resulting image through the SDE prior to increase its
realism. SDEdit does not require task-specific training or inversions and can
naturally achieve the balance between realism and faithfulness. SDEdit
significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on
realism and 91.72% on overall satisfaction scores, according to a human
perception study, on multiple tasks, including stroke-based image synthesis and
editing as well as image compositing.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Point Cloud Denoising
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 23, 2021 </span>    
         <span class="authors"> Shitong Luo, Wei Hu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2107.10981" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Point clouds acquired from scanning devices are often perturbed by noise,
which affects downstream tasks such as surface reconstruction and analysis. The
distribution of a noisy point cloud can be viewed as the distribution of a set
of noise-free samples $p(x)$ convolved with some noise model $n$, leading to
$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy
point cloud, we propose to increase the log-likelihood of each point from $p *
n$ via gradient ascent -- iteratively updating each point's position. Since $p
* n$ is unknown at test-time, and we only need the score (i.e., the gradient of
the log-probability function) to perform gradient ascent, we propose a neural
network architecture to estimate the score of $p * n$ given only noisy point
clouds as input. We derive objective functions for training the network and
develop a denoising algorithm leveraging on the estimated scores. Experiments
demonstrate that the proposed model outperforms state-of-the-art methods under
a variety of noise models, and shows the potential to be applied in other tasks
such as point cloud upsampling. The code is available at
\url{https://github.com/luost26/score-denoise}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Interpreting diffusion score matching using normalizing flow
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 21, 2021 </span>    
         <span class="authors"> Wenbo Gong, Yingzhen Li </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2107.10072" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Scoring matching (SM), and its related counterpart, Stein discrepancy (SD)
have achieved great success in model training and evaluations. However, recent
research shows their limitations when dealing with certain types of
distributions. One possible fix is incorporating the original score matching
(or Stein discrepancy) with a diffusion matrix, which is called diffusion score
matching (DSM) (or diffusion Stein discrepancy (DSD)). However, the lack of
interpretation of the diffusion limits its usage within simple distributions
and manually chosen matrix. In this work, we plan to fill this gap by
interpreting the diffusion matrix using normalizing flows. Specifically, we
theoretically prove that DSM (or DSD) is equivalent to the original score
matching (or Stein discrepancy) evaluated in the transformed space defined by
the normalizing flow, where the diffusion matrix is the inverse of the flow's
Jacobian matrix. In addition, we also build its connection to Riemannian
manifolds and further extend it to continuous flows, where the change of DSM is
characterized by an ODE.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Beyond In-Place Corruption: Insertion and Deletion in Denoising Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 16, 2021 </span>    
         <span class="authors"> Daniel D. Johnson, Jacob Austin, Rianne van den Berg, Daniel Tarlow </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2107.07675" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) have shown impressive
results on sequence generation by iteratively corrupting each example and then
learning to map corrupted versions back to the original. However, previous work
has largely focused on in-place corruption, adding noise to each pixel or token
individually while keeping their locations the same. In this work, we consider
a broader class of corruption processes and denoising models over sequence data
that can insert and delete elements, while still being efficient to train and
sample from. We demonstrate that these models outperform standard in-place
models on an arithmetic sequence task, and that when trained on the text8
dataset they can be used to fix spelling errors without any fine-tuning.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising diffusion probabilistic models for replica exchange
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 15, 2021 </span>    
         <span class="authors"> Yihang Wang, Lukas Herron, Pratyush Tiwary </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2107.07369" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cond-mat.stat-mech, physics.bio-ph, physics.comp-ph, physics.data-an
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Using simulations or experiments performed at some set of temperatures to
learn about the physics or chemistry at some other arbitrary temperature is a
problem of immense practical and theoretical relevance. Here we develop a
framework based on statistical mechanics and generative Artificial Intelligence
that allows solving this problem. Specifically, we work with denoising
diffusion probabilistic models, and show how these models in combination with
replica exchange molecular dynamics achieve superior sampling of the
biomolecular energy landscape at temperatures that were never even simulated
without assuming any particular slow degrees of freedom. The key idea is to
treat the temperature as a fluctuating random variable and not a control
parameter as is usually done. This allows us to directly sample from the joint
probability distribution in configuration and temperature space. The results
here are demonstrated for a chirally symmetric peptide and single-strand
ribonucleic acid undergoing conformational transitions in all-atom water. We
demonstrate how we can discover transition states and metastable states that
were previously unseen at the temperature of interest, and even bypass the need
to perform further simulations for wide range of temperatures. At the same
time, any unphysical states are easily identifiable through very low Boltzmann
weights. The procedure while shown here for a class of molecular simulations
should be more generally applicable to mixing information across simulations
and experiments with varying control parameters.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 07, 2021 </span>    
         <span class="authors"> Yusuke Tashiro, Jiaming Song, Yang Song, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2107.03502" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The imputation of missing values in time series has many applications in
healthcare and finance. While autoregressive models are natural candidates for
time series imputation, score-based diffusion models have recently outperformed
existing counterparts including autoregressive models in many tasks such as
image generation and audio synthesis, and would be promising for time series
imputation. In this paper, we propose Conditional Score-based Diffusion models
for Imputation (CSDI), a novel time series imputation method that utilizes
score-based diffusion models conditioned on observed data. Unlike existing
score-based approaches, the conditional diffusion model is explicitly trained
for imputation and can exploit correlations between observed values. On
healthcare and environmental data, CSDI improves by 40-65% over existing
probabilistic imputation methods on popular performance metrics. In addition,
deterministic imputation by CSDI reduces the error by 5-20% compared to the
state-of-the-art deterministic imputation methods. Furthermore, CSDI can also
be applied to time series interpolation and probabilistic forecasting, and is
competitive with existing baselines. The code is available at
https://github.com/ermongroup/CSDI.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Structured Denoising Diffusion Models in Discrete State-Spaces
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 07, 2021 </span>    
         <span class="authors"> Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2107.03006" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CL, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown
impressive results on image and waveform generation in continuous state spaces.
Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),
diffusion-like generative models for discrete data that generalize the
multinomial diffusion model of Hoogeboom et al. 2021, by going beyond
corruption processes with uniform transition probabilities. This includes
corruption with transition matrices that mimic Gaussian kernels in continuous
space, matrices based on nearest neighbors in embedding space, and matrices
that introduce absorbing states. The third allows us to draw a connection
between diffusion models and autoregressive and mask-based generative models.
We show that the choice of transition matrix is an important design decision
that leads to improved results in image and text domains. We also introduce a
new loss function that combines the variational lower bound with an auxiliary
cross entropy loss. For text, this model class achieves strong results on
character-level text generation while scaling to large vocabularies on LM1B. On
the image dataset CIFAR-10, our models approach the sample quality and exceed
the log-likelihood of the continuous-space DDPM model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Variational Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 01, 2021 </span>    
         <span class="authors"> Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2107.00630" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based generative models have demonstrated a capacity for
perceptually impressive synthesis, but can they also be great likelihood-based
models? We answer this in the affirmative, and introduce a family of
diffusion-based generative models that obtain state-of-the-art likelihoods on
standard image density estimation benchmarks. Unlike other diffusion-based
models, our method allows for efficient optimization of the noise schedule
jointly with the rest of the model. We show that the variational lower bound
(VLB) simplifies to a remarkably short expression in terms of the
signal-to-noise ratio of the diffused data, thereby improving our theoretical
understanding of this model class. Using this insight, we prove an equivalence
between several models proposed in the literature. In addition, we show that
the continuous-time VLB is invariant to the noise schedule, except for the
signal-to-noise ratio at its endpoints. This enables us to learn a noise
schedule that minimizes the variance of the resulting VLB estimator, leading to
faster optimization. Combining these advances with architectural improvements,
we obtain state-of-the-art likelihoods on image density estimation benchmarks,
outperforming autoregressive models that have dominated these benchmarks for
many years, with often significantly faster optimization. In addition, we show
how to use the model as part of a bits-back compression scheme, and demonstrate
lossless compression rates close to the theoretical optimum. Code is available
at https://github.com/google-research/vdm .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Predicting Molecular Conformation via Dynamic Graph Score Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> 01/12/2021 </span>    
         <span class="authors"> Shitong Luo, Chence Shi, Minkai Xu, Jian Tang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://proceedings.neurips.cc/paper/2021/hash/a45a1d12ee0fb7f1f872ab91da18f899-Abstract.html" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		    
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> 29/09/2021 </span>    
         <span class="authors"> Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, Jian Tang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://openreview.net/forum?id=PzcvxEMzvQC" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		    
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Priors in Variational Autoencoders
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 29, 2021 </span>    
         <span class="authors"> Antoine Wehenkel, Gilles Louppe </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.15671" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Among likelihood-based approaches for deep generative modelling, variational
autoencoders (VAEs) offer scalable amortized posterior inference and fast
sampling. However, VAEs are also more and more outperformed by competing models
such as normalizing flows (NFs), deep-energy models, or the new denoising
diffusion probabilistic models (DDPMs). In this preliminary work, we improve
VAEs by demonstrating how DDPMs can be used for modelling the prior
distribution of the latent variables. The diffusion prior model improves upon
Gaussian priors of classical VAEs and is competitive with NF-based priors.
Finally, we hypothesize that hierarchical VAEs could similarly benefit from the
enhanced capacity of diffusion priors.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Deep Generative Learning via Schrödinger Bridge
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 19, 2021 </span>    
         <span class="authors"> Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, Can Yang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.10410" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose to learn a generative model via entropy interpolation with a
Schr\"{o}dinger Bridge. The generative learning task can be formulated as
interpolating between a reference distribution and a target distribution based
on the Kullback-Leibler divergence. At the population level, this entropy
interpolation is characterized via an SDE on $[0,1]$ with a time-varying drift
term. At the sample level, we derive our Schr\"{o}dinger Bridge algorithm by
plugging the drift term estimated by a deep score estimator and a deep density
ratio estimator into the Euler-Maruyama method. Under some mild smoothness
assumptions of the target distribution, we prove the consistency of both the
score estimator and the density ratio estimator, and then establish the
consistency of the proposed Schr\"{o}dinger Bridge approach. Our theoretical
results guarantee that the distribution learned by our approach converges to
the target distribution. Experimental results on multimodal synthetic data and
benchmark data support our theoretical findings and indicate that the
generative model via Schr\"{o}dinger Bridge is comparable with state-of-the-art
GANs, suggesting a new formulation of generative learning. We demonstrate its
usefulness in image interpolation and image inpainting.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 18, 2021 </span>    
         <span class="authors"> Tijin Yan, Hongwei Zhang, Tong Zhou, Yufeng Zhan, Yuanqing Xia </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.10121" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Multivariate time series prediction has attracted a lot of attention because
of its wide applications such as intelligence transportation, AIOps. Generative
models have achieved impressive results in time series modeling because they
can model data distribution and take noise into consideration. However, many
existing works can not be widely used because of the constraints of functional
form of generative models or the sensitivity to hyperparameters. In this paper,
we propose ScoreGrad, a multivariate probabilistic time series forecasting
framework based on continuous energy-based generative models. ScoreGrad is
composed of time series feature extraction module and conditional stochastic
differential equation based score matching module. The prediction can be
achieved by iteratively solving reverse-time SDE. To the best of our knowledge,
ScoreGrad is the first continuous energy based generative model used for time
series forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on
six real-world datasets. The impact of hyperparameters and sampler types on the
performance are also explored. Code is available at
https://github.com/yantijin/ScoreGradPred.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Non Gaussian Denoising Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 14, 2021 </span>    
         <span class="authors"> Eliya Nachmani, Robin San Roman, Lior Wolf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.07582" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative diffusion processes are an emerging and effective tool for image
and speech generation. In the existing methods, the underline noise
distribution of the diffusion process is Gaussian noise. However, fitting
distributions with more degrees of freedom, could help the performance of such
generative models. In this work, we investigate other types of noise
distribution for the diffusion process. Specifically, we show that noise from
Gamma distribution provides improved results for image and speech generation.
Moreover, we show that using a mixture of Gaussian noise variables in the
diffusion process improves the performance over a diffusion process that is
based on a single distribution. Our approach preserves the ability to
efficiently sample state in the training diffusion process while using Gamma
noise and a mixture of noise.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 14, 2021 </span>    
         <span class="authors"> Simon Rouard, Gaëtan Hadjeres </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.07431" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we propose a novel score-base generative model for
unconditional raw audio synthesis. Our proposal builds upon the latest
developments on diffusion process modeling with stochastic differential
equations, which already demonstrated promising results on image generation. We
motivate novel heuristics for the choice of the diffusion processes better
suited for audio generation, and consider the use of a conditional U-Net to
approximate the score function. While previous approaches on diffusion models
on audio were mainly designed as speech vocoders in medium resolution, our
method termed CRASH (Controllable Raw Audio Synthesis with High-resolution)
allows us to generate short percussive sounds in 44.1kHz in a controllable way.
Through extensive experiments, we showcase on a drum sound generation task the
numerous sampling schemes offered by our method (unconditional generation,
deterministic generation, inpainting, interpolation, variations,
class-conditional sampling) and propose the class-mixing sampling, a novel way
to generate "hybrid" sounds. Our proposed method closes the gap with GAN-based
methods on raw audio, while offering more flexible generation capabilities with
lighter and easier-to-train models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## D2C: Diffusion-Denoising Models for Few-shot Conditional Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 12, 2021 </span>    
         <span class="authors"> Abhishek Sinha, Jiaming Song, Chenlin Meng, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.06819" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Conditional generative models of high-dimensional images have many
applications, but supervision signals from conditions to images can be
expensive to acquire. This paper describes Diffusion-Decoding models with
Contrastive representations (D2C), a paradigm for training unconditional
variational autoencoders (VAEs) for few-shot conditional image generation. D2C
uses a learned diffusion-based prior over the latent representations to improve
generation and contrastive self-supervised learning to improve representation
quality. D2C can adapt to novel generation tasks conditioned on labels or
manipulation constraints, by learning from as few as 100 labeled examples. On
conditional generation from new labels, D2C achieves superior performance over
state-of-the-art VAEs and diffusion models. On conditional image manipulation,
D2C generations are two orders of magnitude faster to produce over StyleGAN2
ones and are preferred by 50% - 60% of the human evaluators in a double-blind
study.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Driven Adaptive Prior
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 11, 2021 </span>    
         <span class="authors"> Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, Tie-Yan Liu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.06406" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models have been recently proposed to
generate high-quality samples by estimating the gradient of the data density.
The framework defines the prior noise as a standard Gaussian distribution,
whereas the corresponding data distribution may be more complicated than the
standard Gaussian distribution, which potentially introduces inefficiency in
denoising the prior noise into the data sample because of the discrepancy
between the data and the prior. In this paper, we propose PriorGrad to improve
the efficiency of the conditional diffusion model for speech synthesis (for
example, a vocoder using a mel-spectrogram as the condition) by applying an
adaptive prior derived from the data statistics based on the conditional
information. We formulate the training and sampling procedures of PriorGrad and
demonstrate the advantages of an adaptive prior through a theoretical analysis.
Focusing on the speech synthesis domain, we consider the recently proposed
diffusion-based speech generative models based on both the spectral and time
domains and show that PriorGrad achieves faster convergence and inference with
superior performance, leading to an improved perceptual quality and robustness
to a smaller network capacity, and thereby demonstrating the efficiency of a
data-dependent adaptive prior.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Adversarial purification with Score-based generative models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 11, 2021 </span>    
         <span class="authors"> Jongmin Yoon, Sung Ju Hwang, Juho Lee </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.06041" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While adversarial training is considered as a standard defense method against
adversarial attacks for image classifiers, adversarial purification, which
purifies attacked images into clean images with a standalone purification
model, has shown promises as an alternative defense method. Recently, an
Energy-Based Model (EBM) trained with Markov-Chain Monte-Carlo (MCMC) has been
highlighted as a purification model, where an attacked image is purified by
running a long Markov-chain using the gradients of the EBM. Yet, the
practicality of the adversarial purification using an EBM remains questionable
because the number of MCMC steps required for such purification is too large.
In this paper, we propose a novel adversarial purification method based on an
EBM trained with Denoising Score-Matching (DSM). We show that an EBM trained
with DSM can quickly purify attacked images within a few steps. We further
introduce a simple yet effective randomized purification scheme that injects
random noises into images before purification. This process screens the
adversarial perturbations imposed on images by the random noises and brings the
images to the regime where the EBM can denoise well. We show that our
purification method is robust against various attacks and demonstrate its
state-of-the-art performances.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-based Generative Modeling in Latent Space
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 10, 2021 </span>    
         <span class="authors"> Arash Vahdat, Karsten Kreis, Jan Kautz </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.05931" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models (SGMs) have recently demonstrated impressive
results in terms of both sample quality and distribution coverage. However,
they are usually applied directly in data space and often require thousands of
network evaluations for sampling. Here, we propose the Latent Score-based
Generative Model (LSGM), a novel approach that trains SGMs in a latent space,
relying on the variational autoencoder framework. Moving from data to latent
space allows us to train more expressive generative models, apply SGMs to
non-continuous data, and learn smoother SGMs in a smaller space, resulting in
fewer network evaluations and faster sampling. To enable training LSGMs
end-to-end in a scalable and stable manner, we (i) introduce a new
score-matching objective suitable to the LSGM setting, (ii) propose a novel
parameterization of the score function that allows SGM to focus on the mismatch
of the target distribution with respect to a simple Normal one, and (iii)
analytically derive multiple techniques for variance reduction of the training
objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10,
outperforming all existing generative results on this dataset. On
CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while
outperforming them in sampling time by two orders of magnitude. In modeling
binary images, LSGM achieves state-of-the-art likelihood on the binarized
OMNIGLOT dataset. Our project page and code can be found at
https://nvlabs.github.io/LSGM .
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 10, 2021 </span>    
         <span class="authors"> Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, Il-Chul Moon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.05527" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recent advances in diffusion models bring state-of-the-art performance on
image generation tasks. However, empirical results from previous research in
diffusion models imply an inverse correlation between density estimation and
sample generation performances. This paper investigates with sufficient
empirical evidence that such inverse correlation happens because density
estimation is significantly contributed by small diffusion time, whereas sample
generation mainly depends on large diffusion time. However, training a score
network well across the entire diffusion time is demanding because the loss
scale is significantly imbalanced at each diffusion time. For successful
training, therefore, we introduce Soft Truncation, a universally applicable
training technique for diffusion models, that softens the fixed and static
truncation hyperparameter into a random variable. In experiments, Soft
Truncation achieves state-of-the-art performance on CIFAR-10, CelebA, CelebA-HQ
256x256, and STL-10 datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning to Efficiently Sample from Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 07, 2021 </span>    
         <span class="authors"> Daniel Watson, Jonathan Ho, Mohammad Norouzi, William Chan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.03802" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Diffusion Probabilistic Models (DDPMs) have emerged as a powerful
family of generative models that can yield high-fidelity samples and
competitive log-likelihoods across a range of domains, including image and
speech synthesis. Key advantages of DDPMs include ease of training, in contrast
to generative adversarial networks, and speed of generation, in contrast to
autoregressive models. However, DDPMs typically require hundreds-to-thousands
of steps to generate a high fidelity sample, making them prohibitively
expensive for high dimensional problems. Fortunately, DDPMs allow trading
generation speed for sample quality through adjusting the number of refinement
steps as a post process. Prior work has been successful in improving generation
speed through handcrafting the time schedule by trial and error. We instead
view the selection of the inference time schedules as an optimization problem,
and introduce an exact dynamic programming algorithm that finds the optimal
discrete time schedules for any pre-trained DDPM. Our method exploits the fact
that ELBO can be decomposed into separate KL terms, and given any computation
budget, discovers the time schedule that maximizes the training ELBO exactly.
Our method is efficient, has no hyper-parameters of its own, and can be applied
to any pre-trained DDPM with no retraining. We discover inference time
schedules requiring as few as 32 refinement steps, while sacrificing less than
0.1 bits per dimension compared to the default 4,000 steps used on ImageNet
64x64 [Ho et al., 2020; Nichol and Dhariwal, 2021].
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## A Variational Perspective on Diffusion-Based Generative Models and Score Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 05, 2021 </span>    
         <span class="authors"> Chin-Wei Huang, Jae Hyun Lim, Aaron Courville </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.02808" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Discrete-time diffusion-based generative models and score matching methods
have shown promising results in modeling high-dimensional image data. Recently,
Song et al. (2021) show that diffusion processes that transform data into noise
can be reversed via learning the score function, i.e. the gradient of the
log-density of the perturbed data. They propose to plug the learned score
function into an inverse formula to define a generative diffusion process.
Despite the empirical success, a theoretical underpinning of this procedure is
still lacking. In this work, we approach the (continuous-time) generative
diffusion directly and derive a variational framework for likelihood
estimation, which includes continuous-time normalizing flows as a special case,
and can be seen as an infinitely deep variational autoencoder. Under this
framework, we show that minimizing the score-matching loss is equivalent to
maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed
by Song et al. (2021), bridging the theoretical gap.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 01, 2021 </span>    
         <span class="authors"> Valentin De Bortoli, James Thornton, Jeremy Heng, Arnaud Doucet </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.01357" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG, math.PR
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Progressively applying Gaussian noise transforms complex data distributions
to approximately Gaussian. Reversing this dynamic defines a generative model.
When the forward noising process is given by a Stochastic Differential Equation
(SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the
associated reverse-time SDE may be estimated using score-matching. A limitation
of this approach is that the forward-time SDE must be run for a sufficiently
long time for the final distribution to be approximately Gaussian. In contrast,
solving the Schr\"odinger Bridge problem (SB), i.e. an entropy-regularized
optimal transport problem on path spaces, yields diffusions which generate
samples from the data distribution in finite time. We present Diffusion SB
(DSB), an original approximation of the Iterative Proportional Fitting (IPF)
procedure to solve the SB problem, and provide theoretical analysis along with
generative modeling experiments. The first DSB iteration recovers the
methodology proposed by Song et al. (2021), with the flexibility of using
shorter time intervals, as subsequent DSB iterations reduce the discrepancy
between the final-time marginal of the forward (resp. backward) SDE with
respect to the prior (resp. data) distribution. Beyond generative modeling, DSB
offers a widely applicable computational optimal transport tool as the
continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi,
2013).
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On Fast Sampling of Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 31, 2021 </span>    
         <span class="authors"> Zhifeng Kong, Wei Ping </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.00132" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we propose FastDPM, a unified framework for fast sampling in
diffusion probabilistic models. FastDPM generalizes previous methods and gives
rise to new algorithms with improved sample quality. We systematically
investigate the fast sampling methods under this framework across different
domains, on different datasets, and with different amount of conditional
information provided for generation. We find the performance of a particular
method depends on data domains (e.g., image or audio), the trade-off between
sampling speed and sample quality, and the amount of conditional information.
We further provide insights and recipes on the choice of methods for
practitioners.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## SNIPS: Solving Noisy Inverse Problems Stochastically
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 31, 2021 </span>    
         <span class="authors"> Bahjat Kawar, Gregory Vaksman, Michael Elad </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2105.14951" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work we introduce a novel stochastic algorithm dubbed SNIPS, which
draws samples from the posterior distribution of any linear inverse problem,
where the observation is assumed to be contaminated by additive white Gaussian
noise. Our solution incorporates ideas from Langevin dynamics and Newton's
method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian
denoiser. The proposed approach relies on an intricate derivation of the
posterior score function that includes a singular value decomposition (SVD) of
the degradation operator, in order to obtain a tractable iterative algorithm
for the desired sampling. Due to its stochasticity, the algorithm can produce
multiple high perceptual quality samples for the same noisy observation. We
demonstrate the abilities of the proposed paradigm for image deblurring,
super-resolution, and compressive sensing. We show that the samples produced
are sharp, detailed and consistent with the given measurements, and their
diversity exposes the inherent uncertainty in the inverse problem being solved.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Cascaded Diffusion Models for High Fidelity Image Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 30, 2021 </span>    
         <span class="authors"> Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, Tim Salimans </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2106.15282" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We show that cascaded diffusion models are capable of generating high
fidelity images on the class-conditional ImageNet generation benchmark, without
any assistance from auxiliary image classifiers to boost sample quality. A
cascaded diffusion model comprises a pipeline of multiple diffusion models that
generate images of increasing resolution, beginning with a standard diffusion
model at the lowest resolution, followed by one or more super-resolution
diffusion models that successively upsample the image and add higher resolution
details. We find that the sample quality of a cascading pipeline relies
crucially on conditioning augmentation, our proposed method of data
augmentation of the lower resolution conditioning inputs to the
super-resolution models. Our experiments show that conditioning augmentation
prevents compounding error during sampling in a cascaded model, helping us to
train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at
128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and
classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,
outperforming VQ-VAE-2.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Representation Learning in Continuous-Time Score-Based Generative Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 29, 2021 </span>    
         <span class="authors"> Korbinian Abstreiter, Sarthak Mittal, Stefan Bauer, Bernhard Schölkopf, Arash Mehrjou </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2105.14257" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Diffusion-based methods represented as stochastic differential equations on a
continuous-time domain have recently proven successful as a non-adversarial
generative model. Training such models relies on denoising score matching,
which can be seen as multi-scale denoising autoencoders. Here, we augment the
denoising score matching framework to enable representation learning without
any supervised signal. GANs and VAEs learn representations by directly
transforming latent codes to data samples. In contrast, the introduced
diffusion-based representation learning relies on a new formulation of the
denoising score matching objective and thus encodes the information needed for
denoising. We illustrate how this difference allows for manual control of the
level of details encoded in the representation. Using the same approach, we
propose to learn an infinite-dimensional latent code that achieves improvements
of state-of-the-art models on semi-supervised image classification. We also
compare the quality of learned representations of diffusion score matching with
other methods like autoencoder and contrastively trained systems through their
performances on downstream tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Gotta Go Fast When Generating Data with Score-Based Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 28, 2021 </span>    
         <span class="authors"> Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, Ioannis Mitliagkas </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2105.14080" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, math.OC, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based (denoising diffusion) generative models have recently gained a
lot of success in generating realistic and diverse data. These approaches
define a forward diffusion process for transforming data to noise and generate
data by reversing it (thereby going from noise to data). Unfortunately, current
score-based models generate data very slowly due to the sheer number of score
network evaluations required by numerical SDE solvers.
  In this work, we aim to accelerate this process by devising a more efficient
SDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which
uses a fixed step size. We found that naively replacing it with other SDE
solvers fares poorly - they either result in low-quality samples or become
slower than EM. To get around this issue, we carefully devise an SDE solver
with adaptive step sizes tailored to score-based generative models piece by
piece. Our solver requires only two score function evaluations, rarely rejects
samples, and leads to high-quality samples. Our approach generates data 2 to 10
times faster than EM while achieving better or equal sample quality. For
high-resolution images, our method leads to significantly higher quality
samples than all other methods tested. Our SDE solver has the benefit of
requiring no step size tuning.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 28, 2021 </span>    
         <span class="authors"> Songxiang Liu, Yuewen Cao, Dan Su, Helen Meng </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2105.13871" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.CL, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Singing voice conversion (SVC) is one promising technique which can enrich
the way of human-computer interaction by endowing a computer the ability to
produce high-fidelity and expressive singing voice. In this paper, we propose
DiffSVC, an SVC system based on denoising diffusion probabilistic model.
DiffSVC uses phonetic posteriorgrams (PPGs) as content features. A denoising
module is trained in DiffSVC, which takes destroyed mel spectrogram produced by
the diffusion/forward process and its corresponding step information as input
to predict the added Gaussian noise. We use PPGs, fundamental frequency
features and loudness features as auxiliary input to assist the denoising
process. Experiments show that DiffSVC can achieve superior conversion
performance in terms of naturalness and voice similarity to current
state-of-the-art SVC approaches.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Grad-tts: A diffusion probabilistic model for text-to-speech
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 13, 2021 </span>    
         <span class="authors"> Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2105.06337" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CL, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Recently, denoising diffusion probabilistic models and generative score
matching have shown high potential in modelling complex data distributions
while stochastic calculus has provided a unified point of view on these
techniques allowing for flexible inference schemes. In this paper we introduce
Grad-TTS, a novel text-to-speech model with score-based decoder producing
mel-spectrograms by gradually transforming noise predicted by encoder and
aligned with text input by means of Monotonic Alignment Search. The framework
of stochastic differential equations helps us to generalize conventional
diffusion probabilistic models to the case of reconstructing data from noise
with different parameters and allows to make this reconstruction flexible by
explicitly controlling trade-off between sound quality and inference speed.
Subjective human evaluation shows that Grad-TTS is competitive with
state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We
will make the code publicly available shortly.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion models beat gans on image synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 11, 2021 </span>    
         <span class="authors"> Prafulla Dhariwal, Alex Nichol </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2105.05233" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We show that diffusion models can achieve image sample quality superior to
the current state-of-the-art generative models. We achieve this on
unconditional image synthesis by finding a better architecture through a series
of ablations. For conditional image synthesis, we further improve sample
quality with classifier guidance: a simple, compute-efficient method for
trading off diversity for fidelity using gradients from a classifier. We
achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet
256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep
even with as few as 25 forward passes per sample, all while maintaining better
coverage of the distribution. Finally, we find that classifier guidance
combines well with upsampling diffusion models, further improving FID to 3.94
on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our
code at https://github.com/openai/guided-diffusion
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning Gradient Fields for Molecular Conformation Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 09, 2021 </span>    
         <span class="authors"> Chence Shi, Shitong Luo, Minkai Xu, Jian Tang </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2105.03902" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, physics.chem-ph, q-bio.BM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We study a fundamental problem in computational chemistry known as molecular
conformation generation, trying to predict stable 3D structures from 2D
molecular graphs. Existing machine learning approaches usually first predict
distances between atoms and then generate a 3D structure satisfying the
distances, where noise in predicted distances may induce extra errors during 3D
coordinate generation. Inspired by the traditional force field methods for
molecular dynamics simulation, in this paper, we propose a novel approach
called ConfGF by directly estimating the gradient fields of the log density of
atomic coordinates. The estimated gradient fields allow directly generating
stable conformations via Langevin dynamics. However, the problem is very
challenging as the gradient fields are roto-translation equivariant. We notice
that estimating the gradient fields of atomic coordinates can be translated to
estimating the gradient fields of interatomic distances, and hence develop a
novel algorithm based on recent score-based generative models to effectively
estimate these gradients. Experimental results across multiple tasks show that
ConfGF outperforms previous state-of-the-art baselines by a significant margin.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> May 06, 2021 </span>    
         <span class="authors"> Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, Zhou Zhao </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2105.02446" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Singing voice synthesis (SVS) systems are built to synthesize high-quality
and expressive singing voice, in which the acoustic model generates the
acoustic features (e.g., mel-spectrogram) given a music score. Previous singing
acoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial
network (GAN) to reconstruct the acoustic features, while they suffer from
over-smoothing and unstable training issues respectively, which hinder the
naturalness of synthesized singing. In this work, we propose DiffSinger, an
acoustic model for SVS based on the diffusion probabilistic model. DiffSinger
is a parameterized Markov chain that iteratively converts the noise into
mel-spectrogram conditioned on the music score. By implicitly optimizing
variational bound, DiffSinger can be stably trained and generate realistic
outputs. To further improve the voice quality and speed up inference, we
introduce a shallow diffusion mechanism to make better use of the prior
knowledge learned by the simple loss. Specifically, DiffSinger starts
generation at a shallow step smaller than the total number of diffusion steps,
according to the intersection of the diffusion trajectories of the ground-truth
mel-spectrogram and the one predicted by a simple mel-spectrogram decoder.
Besides, we propose boundary prediction methods to locate the intersection and
determine the shallow step adaptively. The evaluations conducted on a Chinese
singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS
work. Extensional experiments also prove the generalization of our methods on
text-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io.
Codes: https://github.com/MoonInTheRiver/DiffSinger. The old title of this
work: "Diffsinger: Diffusion acoustic model for singing voice synthesis".
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Image super-resolution via iterative refinement
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 15, 2021 </span>    
         <span class="authors"> Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2104.07636" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present SR3, an approach to image Super-Resolution via Repeated
Refinement. SR3 adapts denoising diffusion probabilistic models to conditional
image generation and performs super-resolution through a stochastic denoising
process. Inference starts with pure Gaussian noise and iteratively refines the
noisy output using a U-Net model trained on denoising at various noise levels.
SR3 exhibits strong performance on super-resolution tasks at different
magnification factors, on faces and natural images. We conduct human evaluation
on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA
GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic
outputs, while GANs do not exceed a fool rate of 34%. We further show the
effectiveness of SR3 in cascaded image generation, where generative models are
chained with super-resolution models, yielding a competitive FID score of 11.3
on ImageNet.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 12, 2021 </span>    
         <span class="authors"> Hiroshi Sasaki, Chris G. Willcocks, Toby P. Breckon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2104.05358" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG, eess.IV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a novel unpaired image-to-image translation method that uses
denoising diffusion probabilistic models without requiring adversarial
training. Our method, UNpaired Image Translation with Denoising Diffusion
Probabilistic Models (UNIT-DDPM), trains a generative model to infer the joint
distribution of images over both domains as a Markov chain by minimising a
denoising score matching objective conditioned on the other domain. In
particular, we update both domain translation models simultaneously, and we
generate target domain images by a denoising Markov Chain Monte Carlo approach
that is conditioned on the input source domain images, based on Langevin
dynamics. Our approach provides stable model training for image-to-image
translation and generates high-quality image outputs. This enables
state-of-the-art Fr\'echet Inception Distance (FID) performance on several
public datasets, including both colour and multispectral imagery, significantly
outperforming the contemporary adversarial image-to-image translation methods.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## On tuning consistent annealed sampling for denoising score matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 08, 2021 </span>    
         <span class="authors"> Joan Serrà, Santiago Pascual, Jordi Pons </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2104.03725" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, cs.CV, cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models provide state-of-the-art quality for image and
audio synthesis. Sampling from these models is performed iteratively, typically
employing a discretized series of noise levels and a predefined scheme. In this
note, we first overview three common sampling schemes for models trained with
denoising score matching. Next, we focus on one of them, consistent annealed
sampling, and study its hyper-parameter boundaries. We then highlight a
possible formulation of such hyper-parameter that explicitly considers those
boundaries and facilitates tuning when using few or a variable number of steps.
Finally, we highlight some connections of the formulation with other sampling
schemes.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## 3D Shape Generation and Completion through Point-Voxel Diffusion
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 08, 2021 </span>    
         <span class="authors"> Linqi Zhou, Yilun Du, Jiajun Wu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2104.03670" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We propose a novel approach for probabilistic generative modeling of 3D
shapes. Unlike most existing models that learn to deterministically translate a
latent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified,
probabilistic formulation for unconditional shape generation and conditional,
multi-modal shape completion. PVD marries denoising diffusion models with the
hybrid, point-voxel representation of 3D shapes. It can be viewed as a series
of denoising steps, reversing the diffusion process from observed point cloud
data to Gaussian noise, and is trained by optimizing a variational lower bound
to the (conditional) likelihood function. Experiments demonstrate that PVD is
capable of synthesizing high-fidelity shapes, completing partial point clouds,
and generating multiple completion results from single-view depth scans of real
objects.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Noise Estimation for Generative Diffusion Models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 06, 2021 </span>    
         <span class="authors"> Robin San-Roman, Eliya Nachmani, Lior Wolf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2104.02600" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative diffusion models have emerged as leading models in speech and
image generation. However, in order to perform well with a small number of
denoising steps, a costly tuning of the set of noise parameters is needed. In
this work, we present a simple and versatile learning scheme that can
step-by-step adjust those noise parameters, for any given number of steps,
while the previous work needs to retune for each number separately.
Furthermore, without modifying the weights of the diffusion model, we are able
to significantly improve the synthesis results, for a small number of steps.
Our approach comes at a negligible computation cost.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> April 06, 2021 </span>    
         <span class="authors"> Junhyeok Lee, Seungu Han </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2104.02321" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.AI, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we introduce NU-Wave, the first neural audio upsampling model
to produce waveforms of sampling rate 48kHz from coarse 16kHz or 24kHz inputs,
while prior works could generate only up to 16kHz. NU-Wave is the first
diffusion probabilistic model for audio super-resolution which is engineered
based on neural vocoders. NU-Wave generates high-quality audio that achieves
high performance in terms of signal-to-noise ratio (SNR), log-spectral distance
(LSD), and accuracy of the ABX test. In all cases, NU-Wave outperforms the
baseline models despite the substantially smaller model capacity (3.0M
parameters) than baselines (5.4-21%). The audio samples of our model are
available at https://mindslab-ai.github.io/nuwave, and the code will be made
available soon.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Symbolic music generation with diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 30, 2021 </span>    
         <span class="authors"> Gautam Mittal, Jesse Engel, Curtis Hawthorne, Ian Simon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2103.16091" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, cs.LG, eess.AS, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models and diffusion probabilistic models have been
successful at generating high-quality samples in continuous domains such as
images and audio. However, due to their Langevin-inspired sampling mechanisms,
their application to discrete and sequential data has been limited. In this
work, we present a technique for training diffusion models on sequential data
by parameterizing the discrete domain in the continuous latent space of a
pre-trained variational autoencoder. Our method is non-autoregressive and
learns to generate sequences of latent embeddings through the reverse process
and offers parallel generation with a constant number of iterative refinement
steps. We apply this technique to modeling symbolic music and show strong
unconditional generation and post-hoc conditional infilling results compared to
autoregressive language models operating over the same continuous embeddings.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffusion Probabilistic Models for 3D Point Cloud Generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 02, 2021 </span>    
         <span class="authors"> Shitong Luo, Wei Hu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2103.01458" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present a probabilistic model for point cloud generation, which is
fundamental for various 3D vision tasks such as shape completion, upsampling,
synthesis and data augmentation. Inspired by the diffusion process in
non-equilibrium thermodynamics, we view points in point clouds as particles in
a thermodynamic system in contact with a heat bath, which diffuse from the
original distribution to a noise distribution. Point cloud generation thus
amounts to learning the reverse diffusion process that transforms the noise
distribution to the distribution of a desired shape. Specifically, we propose
to model the reverse diffusion process for point clouds as a Markov chain
conditioned on certain shape latent. We derive the variational bound in closed
form for training and provide implementations of the model. Experimental
results demonstrate that our model achieves competitive performance in point
cloud generation and auto-encoding. The code is available at
\url{https://github.com/luost26/diffusion-point-cloud}.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improved denoising diffusion probabilistic models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 18, 2021 </span>    
         <span class="authors"> Alex Nichol, Prafulla Dhariwal </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2102.09672" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPM) are a class of generative
models which have recently been shown to produce excellent samples. We show
that with a few simple modifications, DDPMs can also achieve competitive
log-likelihoods while maintaining high sample quality. Additionally, we find
that learning variances of the reverse diffusion process allows sampling with
an order of magnitude fewer forward passes with a negligible difference in
sample quality, which is important for the practical deployment of these
models. We additionally use precision and recall to compare how well DDPMs and
GANs cover the target distribution. Finally, we show that the sample quality
and likelihood of these models scale smoothly with model capacity and training
compute, making them easily scalable. We release our code at
https://github.com/openai/improved-diffusion
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Argmax flows and multinomial diffusion: Towards non-autoregressive language models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 10, 2021 </span>    
         <span class="authors"> Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, Max Welling </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2102.05379" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CL, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Generative flows and diffusion models have been predominantly trained on
ordinal data, for example natural images. This paper introduces two extensions
of flows and diffusion for categorical data such as language or image
segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined
by a composition of a continuous distribution (such as a normalizing flow), and
an argmax function. To optimize this model, we learn a probabilistic inverse
for the argmax that lifts the categorical data to a continuous space.
Multinomial Diffusion gradually adds categorical noise in a diffusion process,
for which the generative denoising process is learned. We demonstrate that our
method outperforms existing dequantization approaches on text modelling and
modelling on image segmentation maps in log-likelihood.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 28, 2021 </span>    
         <span class="authors"> Kashif Rasul, Calvin Seward, Ingmar Schuster, Roland Vollgraf </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2101.12072" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.AI
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we propose \texttt{TimeGrad}, an autoregressive model for
multivariate probabilistic time series forecasting which samples from the data
distribution at each time step by estimating its gradient. To this end, we use
diffusion probabilistic models, a class of latent variable models closely
connected to score matching and energy-based methods. Our model learns
gradients by optimizing a variational bound on the data likelihood and at
inference time converts white noise into a sample of the distribution of
interest through a Markov chain using Langevin sampling. We demonstrate
experimentally that the proposed autoregressive denoising diffusion model is
the new state-of-the-art multivariate probabilistic forecasting method on
real-world data sets with thousands of correlated dimensions. We hope that this
method is a useful tool for practitioners and lays the foundation for future
research in this area.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Stochastic Image Denoising by Sampling from the Posterior Distribution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 23, 2021 </span>    
         <span class="authors"> Bahjat Kawar, Gregory Vaksman, Michael Elad </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2101.09552" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.IV, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Image denoising is a well-known and well studied problem, commonly targeting
a minimization of the mean squared error (MSE) between the outcome and the
original image. Unfortunately, especially for severe noise levels, such Minimum
MSE (MMSE) solutions may lead to blurry output images. In this work we propose
a novel stochastic denoising approach that produces viable and high perceptual
quality results, while maintaining a small MSE. Our method employs Langevin
dynamics that relies on a repeated application of any given MMSE denoiser,
obtaining the reconstructed image by effectively sampling from the posterior
distribution. Due to its stochasticity, the proposed algorithm can produce a
variety of high-quality outputs for a given noisy input, all shown to be
legitimate denoising results. In addition, we present an extension of our
algorithm for handling the inpainting problem, recovering missing pixels while
removing noise from partially given data.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Maximum likelihood training of score-based diffusion models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 22, 2021 </span>    
         <span class="authors"> Yang Song, Conor Durkan, Iain Murray, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2101.09258" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based diffusion models synthesize samples by reversing a stochastic
process that diffuses data to noise, and are trained by minimizing a weighted
combination of score matching losses. The log-likelihood of score-based
diffusion models can be tractably computed through a connection to continuous
normalizing flows, but log-likelihood is not directly optimized by the weighted
combination of score matching losses. We show that for a specific weighting
scheme, the objective upper bounds the negative log-likelihood, thus enabling
approximate maximum likelihood training of score-based diffusion models. We
empirically observe that maximum likelihood training consistently improves the
likelihood of score-based diffusion models across multiple datasets, stochastic
processes, and model architectures. Our best models achieve negative
log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32x32
without any data augmentation, on a par with state-of-the-art autoregressive
models on these tasks.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Knowledge distillation in iterative generative models for improved sampling speed
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> January 07, 2021 </span>    
         <span class="authors"> Eric Luhman, Troy Luhman </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2101.02388" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Iterative generative models, such as noise conditional score networks and
denoising diffusion probabilistic models, produce high quality samples by
gradually denoising an initial noise vector. However, their denoising process
has many steps, making them 2-3 orders of magnitude slower than other
generative models such as GANs and VAEs. In this paper, we establish a novel
connection between knowledge distillation and image generation with a technique
that distills a multi-step denoising process into a single step, resulting in a
sampling speed similar to other single-step generative models. Our Denoising
Student generates high quality samples comparable to GANs on the CIFAR-10 and
CelebA datasets, without adversarial training. We demonstrate that our method
scales to higher resolutions through experiments on 256 x 256 LSUN. Code and
checkpoints are available at https://github.com/tcl9876/Denoising_Student
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning energy-based models by diffusion recovery likelihood
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> December 15, 2020 </span>    
         <span class="authors"> Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, Diederik P. Kingma </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2012.08125" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 While energy-based models (EBMs) exhibit a number of desirable properties,
training and sampling on high-dimensional datasets remains challenging.
Inspired by recent progress on diffusion probabilistic models, we present a
diffusion recovery likelihood method to tractably learn and sample from a
sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM
is trained with recovery likelihood, which maximizes the conditional
probability of the data at a certain noise level given their noisy versions at
a higher noise level. Optimizing recovery likelihood is more tractable than
marginal likelihood, as sampling from the conditional distributions is much
easier than sampling from the marginal distributions. After training,
synthesized images can be generated by the sampling process that initializes
from Gaussian white noise distribution and progressively samples the
conditional distributions at decreasingly lower noise levels. Our method
generates high fidelity samples on various image datasets. On unconditional
CIFAR-10 our method achieves FID 9.58 and inception score 8.30, superior to the
majority of GANs. Moreover, we demonstrate that unlike previous work on EBMs,
our long-run MCMC samples from the conditional distributions do not diverge and
still represent realistic images, allowing us to accurately estimate the
normalized density of data even for high-dimensional datasets. Our
implementation is available at https://github.com/ruiqigao/recovery_likelihood.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Score-Based Generative Modeling through Stochastic Differential Equations
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 26, 2020 </span>    
         <span class="authors"> Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2011.13456" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Creating noise from data is easy; creating data from noise is generative
modeling. We present a stochastic differential equation (SDE) that smoothly
transforms a complex data distribution to a known prior distribution by slowly
injecting noise, and a corresponding reverse-time SDE that transforms the prior
distribution back into the data distribution by slowly removing the noise.
Crucially, the reverse-time SDE depends only on the time-dependent gradient
field (\aka, score) of the perturbed data distribution. By leveraging advances
in score-based generative modeling, we can accurately estimate these scores
with neural networks, and use numerical SDE solvers to generate samples. We
show that this framework encapsulates previous approaches in score-based
generative modeling and diffusion probabilistic modeling, allowing for new
sampling procedures and new modeling capabilities. In particular, we introduce
a predictor-corrector framework to correct errors in the evolution of the
discretized reverse-time SDE. We also derive an equivalent neural ODE that
samples from the same distribution as the SDE, but additionally enables exact
likelihood computation, and improved sampling efficiency. In addition, we
provide a new way to solve inverse problems with score-based models, as
demonstrated with experiments on class-conditional generation, image
inpainting, and colorization. Combined with multiple architectural
improvements, we achieve record-breaking performance for unconditional image
generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a
competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity
generation of 1024 x 1024 images for the first time from a score-based
generative model.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Probabilistic Mapping of Dark Matter by Neural Score Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 16, 2020 </span>    
         <span class="authors"> Benjamin Remy, Francois Lanusse, Zaccharie Ramzi, Jia Liu, Niall Jeffrey, Jean-Luc Starck </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2011.08271" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  astro-ph.CO, astro-ph.IM
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 The Dark Matter present in the Large-Scale Structure of the Universe is
invisible, but its presence can be inferred through the small gravitational
lensing effect it has on the images of far away galaxies. By measuring this
lensing effect on a large number of galaxies it is possible to reconstruct maps
of the Dark Matter distribution on the sky. This, however, represents an
extremely challenging inverse problem due to missing data and noise dominated
measurements. In this work, we present a novel methodology for addressing such
inverse problems by combining elements of Bayesian statistics, analytic
physical theory, and a recent class of Deep Generative Models based on Neural
Score Matching. This approach allows to do the following: (1) make full use of
analytic cosmological theory to constrain the 2pt statistics of the solution,
(2) learn from cosmological simulations any differences between this analytic
prior and full simulations, and (3) obtain samples from the full Bayesian
posterior of the problem for robust Uncertainty Quantification. We present an
application of this methodology on the first deep-learning-assisted Dark Matter
map reconstruction of the Hubble Space Telescope COSMOS field.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising Score-Matching for Uncertainty Quantification in Inverse Problems
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> November 16, 2020 </span>    
         <span class="authors"> Zaccharie Ramzi, Benjamin Remy, Francois Lanusse, Jean-Luc Starck, Philippe Ciuciu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2011.08698" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  stat.ML, cs.CV, cs.LG, eess.SP, physics.med-ph
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Deep neural networks have proven extremely efficient at solving a wide
rangeof inverse problems, but most often the uncertainty on the solution they
provideis hard to quantify. In this work, we propose a generic Bayesian
framework forsolving inverse problems, in which we limit the use of deep neural
networks tolearning a prior distribution on the signals to recover. We adopt
recent denoisingscore matching techniques to learn this prior from data, and
subsequently use it aspart of an annealed Hamiltonian Monte-Carlo scheme to
sample the full posteriorof image inverse problems. We apply this framework to
Magnetic ResonanceImage (MRI) reconstruction and illustrate how this approach
not only yields highquality reconstructions but can also be used to assess the
uncertainty on particularfeatures of a reconstructed image.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## VoiceGrad: Non-Parallel Any-to-Many Voice Conversion with Annealed Langevin Dynamics
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 06, 2020 </span>    
         <span class="authors"> Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, Nobukatsu Hojo, Shogo Seki </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2010.02977" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.SD, eess.AS
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this paper, we propose a non-parallel any-to-many voice conversion (VC)
method termed VoiceGrad. Inspired by WaveGrad, a recently introduced novel
waveform generation method, VoiceGrad is based upon the concepts of score
matching and Langevin dynamics. It uses weighted denoising score matching to
train a score approximator, a fully convolutional network with a U-Net
structure designed to predict the gradient of the log density of the speech
feature sequences of multiple speakers, and performs VC by using annealed
Langevin dynamics to iteratively update an input feature sequence towards the
nearest stationary point of the target distribution based on the trained score
approximator network. Thanks to the nature of this concept, VoiceGrad enables
any-to-many VC, a VC scenario in which the speaker of input speech can be
arbitrary, and allows for non-parallel training, which requires no parallel
utterances or transcriptions.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising diffusion implicit models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> October 06, 2020 </span>    
         <span class="authors"> Jiaming Song, Chenlin Meng, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2010.02502" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising diffusion probabilistic models (DDPMs) have achieved high quality
image generation without adversarial training, yet they require simulating a
Markov chain for many steps to produce a sample. To accelerate sampling, we
present denoising diffusion implicit models (DDIMs), a more efficient class of
iterative implicit probabilistic models with the same training procedure as
DDPMs. In DDPMs, the generative process is defined as the reverse of a
Markovian diffusion process. We construct a class of non-Markovian diffusion
processes that lead to the same training objective, but whose reverse process
can be much faster to sample from. We empirically demonstrate that DDIMs can
produce high quality samples $10 \times$ to $50 \times$ faster in terms of
wall-clock time compared to DDPMs, allow us to trade off computation for sample
quality, and can perform semantically meaningful image interpolation directly
in the latent space.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Diffwave: A versatile diffusion model for audio synthesis
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 21, 2020 </span>    
         <span class="authors"> Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2009.09761" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.CL, cs.LG, cs.SD, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we propose DiffWave, a versatile diffusion probabilistic model
for conditional and unconditional waveform generation. The model is
non-autoregressive, and converts the white noise signal into structured
waveform through a Markov chain with a constant number of steps at synthesis.
It is efficiently trained by optimizing a variant of variational bound on the
data likelihood. DiffWave produces high-fidelity audios in different waveform
generation tasks, including neural vocoding conditioned on mel spectrogram,
class-conditional generation, and unconditional generation. We demonstrate that
DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44
versus 4.43), while synthesizing orders of magnitude faster. In particular, it
significantly outperforms autoregressive and GAN-based waveform models in the
challenging unconditional generation task in terms of audio quality and sample
diversity from various automatic and human evaluations.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Adversarial score matching and improved sampling for image generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 11, 2020 </span>    
         <span class="authors"> Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Rémi Tachet des Combes, Ioannis Mitliagkas </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2009.05475" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has
recently found success in generative modeling. The approach works by first
training a neural network to estimate the score of a distribution, and then
using Langevin dynamics to sample from the data distribution assumed by the
score network. Despite the convincing visual quality of samples, this method
appears to perform worse than Generative Adversarial Networks (GANs) under the
Fr\'echet Inception Distance, a standard metric for generative models.
  We show that this apparent gap vanishes when denoising the final Langevin
samples using the score network. In addition, we propose two improvements to
DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to
Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of
both Denoising Score Matching and adversarial objectives. By combining these
two techniques and exploring different network architectures, we elevate score
matching methods and obtain results competitive with state-of-the-art image
generation on CIFAR-10.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Wavegrad: Estimating gradients for waveform generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> September 02, 2020 </span>    
         <span class="authors"> Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, William Chan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2009.00713" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  eess.AS, cs.LG, cs.SD, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 This paper introduces WaveGrad, a conditional model for waveform generation
which estimates gradients of the data density. The model is built on prior work
on score matching and diffusion probabilistic models. It starts from a Gaussian
white noise signal and iteratively refines the signal via a gradient-based
sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to
trade inference speed for sample quality by adjusting the number of refinement
steps, and bridges the gap between non-autoregressive and autoregressive models
in terms of audio quality. We find that it can generate high fidelity audio
samples using as few as six iterations. Experiments reveal WaveGrad to generate
high fidelity audio, outperforming adversarial non-autoregressive baselines and
matching a strong likelihood-based autoregressive baseline using fewer
sequential operations. Audio samples are available at
https://wavegrad.github.io/.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Learning gradient fields for shape generation
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> August 14, 2020 </span>    
         <span class="authors"> Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, Bharath Hariharan </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2008.06520" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.CV, cs.LG
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 In this work, we propose a novel technique to generate shapes from point
cloud data. A point cloud can be viewed as samples from a distribution of 3D
points whose density is concentrated near the surface of the shape. Point cloud
generation thus amounts to moving randomly sampled points to high-density
areas. We generate point clouds by performing stochastic gradient ascent on an
unnormalized probability density, thereby moving sampled points toward the
high-likelihood regions. Our model directly predicts the gradient of the log
density field and can be trained with a simple objective adapted from
score-based generative models. We show that our method can reach
state-of-the-art performance for point cloud auto-encoding and generation,
while also allowing for extraction of a high-quality implicit surface. Code is
available at https://github.com/RuojinCai/ShapeGF.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Efficient Learning of Generative Models via Finite-Difference Score Matching
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 07, 2020 </span>    
         <span class="authors"> Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, Jun Zhu </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2007.03317" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Several machine learning applications involve the optimization of
higher-order derivatives (e.g., gradients of gradients) during training, which
can be expensive in respect to memory and computation even with automatic
differentiation. As a typical example in generative modeling, score matching
(SM) involves the optimization of the trace of a Hessian. To improve computing
efficiency, we rewrite the SM objective and its variants in terms of
directional derivatives, and present a generic strategy to efficiently
approximate any-order directional derivative with finite difference (FD). Our
approximation only involves function evaluations, which can be executed in
parallel, and no gradient computations. Thus, it reduces the total
computational cost while also improving numerical stability. We provide two
instantiations by reformulating variants of SM objectives into the FD forms.
Empirically, we demonstrate that our methods produce results comparable to the
gradient-based counterparts while being much more computationally efficient.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Denoising diffusion probabilistic models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 19, 2020 </span>    
         <span class="authors"> Jonathan Ho, Ajay Jain, Pieter Abbeel </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2006.11239" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We present high quality image synthesis results using diffusion probabilistic
models, a class of latent variable models inspired by considerations from
nonequilibrium thermodynamics. Our best results are obtained by training on a
weighted variational bound designed according to a novel connection between
diffusion probabilistic models and denoising score matching with Langevin
dynamics, and our models naturally admit a progressive lossy decompression
scheme that can be interpreted as a generalization of autoregressive decoding.
On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and
a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality
similar to ProgressiveGAN. Our implementation is available at
https://github.com/hojonathanho/diffusion
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Rethinking the Role of Gradient-Based Attribution Methods for Model Interpretability
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 16, 2020 </span>    
         <span class="authors"> Suraj Srinivas, Francois Fleuret </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2006.09128" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Current methods for the interpretability of discriminative deep neural
networks commonly rely on the model's input-gradients, i.e., the gradients of
the output logits w.r.t. the inputs. The common assumption is that these
input-gradients contain information regarding $p_{\theta} ( y \mid x)$, the
model's discriminative capabilities, thus justifying their use for
interpretability. However, in this work we show that these input-gradients can
be arbitrarily manipulated as a consequence of the shift-invariance of softmax
without changing the discriminative function. This leaves an open question: if
input-gradients can be arbitrary, why are they highly structured and
explanatory in standard models?
  We investigate this by re-interpreting the logits of standard softmax-based
classifiers as unnormalized log-densities of the data distribution and show
that input-gradients can be viewed as gradients of a class-conditional density
model $p_{\theta}(x \mid y)$ implicit within the discriminative model. This
leads us to hypothesize that the highly structured and explanatory nature of
input-gradients may be due to the alignment of this class-conditional model
$p_{\theta}(x \mid y)$ with that of the ground truth data distribution
$p_{\text{data}} (x \mid y)$. We test this hypothesis by studying the effect of
density alignment on gradient explanations. To achieve this alignment we use
score-matching, and propose novel approximations to this algorithm to enable
training large-scale models.
  Our experiments show that improving the alignment of the implicit density
model with the data distribution enhances gradient structure and explanatory
power while reducing this alignment has the opposite effect. Overall, our
finding that input-gradients capture information regarding an implicit
generative model implies that we need to re-think their use for interpreting
discriminative models.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Improved techniques for training score-based generative models
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> June 16, 2020 </span>    
         <span class="authors"> Yang Song, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2006.09011" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cs.CV, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Score-based generative models can produce high quality image samples
comparable to GANs, without requiring adversarial optimization. However,
existing training procedures are limited to images of low resolution (typically
below 32x32), and can be unstable under some settings. We provide a new
theoretical analysis of learning and sampling from score models in high
dimensional spaces, explaining existing failure modes and motivating new
solutions that generalize across datasets. To enhance stability, we also
propose to maintain an exponential moving average of model weights. With these
improvements, we can effortlessly scale score-based generative models to images
with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based
models can generate high-fidelity samples that rival best-in-class GANs on
various image datasets, including CelebA, FFHQ, and multiple LSUN categories.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Permutation invariant graph generation via score-based generative modeling
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 02, 2020 </span>    
         <span class="authors"> Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2003.00638" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Learning generative models for graph-structured data is challenging because
graphs are discrete, combinatorial, and the underlying data distribution is
invariant to the ordering of nodes. However, most of the existing generative
models for graphs are not invariant to the chosen ordering, which might lead to
an undesirable bias in the learned distribution. To address this difficulty, we
propose a permutation invariant approach to modeling graphs, using the recent
framework of score-based generative modeling. In particular, we design a
permutation equivariant, multi-channel graph neural network to model the
gradient of the data distribution at the input graph (a.k.a., the score
function). This permutation equivariant model of gradients implicitly defines a
permutation invariant distribution for graphs. We train this graph neural
network with score matching and sample from it with annealed Langevin dynamics.
In our experiments, we first demonstrate the capacity of this new architecture
in learning discrete graph algorithms. For graph generation, we find that our
learning approach achieves better or comparable results to existing models on
benchmark datasets.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Source Separation with Deep Generative Prior
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> February 19, 2020 </span>    
         <span class="authors"> Vivek Jayaram, John Thickstun </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/2002.07942" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 Despite substantial progress in signal source separation, results for richly
structured data continue to contain perceptible artifacts. In contrast, recent
deep generative models can produce authentic samples in a variety of domains
that are indistinguishable from samples of the data distribution. This paper
introduces a Bayesian approach to source separation that uses generative models
as priors over the components of a mixture of sources, and noise-annealed
Langevin dynamics to sample from the posterior distribution of sources given a
mixture. This decouples the source separation problem from generative modeling,
enabling us to directly use cutting-edge generative models as priors. The
method achieves state-of-the-art performance for MNIST digit separation. We
introduce new methodology for evaluating separation quality on richer datasets,
providing quantitative evaluation of separation results on CIFAR-10. We also
provide qualitative results on LSUN.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Generative Modeling by Estimating Gradients of the Data Distribution
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> July 12, 2019 </span>    
         <span class="authors"> Yang Song, Stefano Ermon </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/1907.05600" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 We introduce a new generative model where samples are produced via Langevin
dynamics using gradients of the data distribution estimated with score
matching. Because gradients can be ill-defined and hard to estimate when the
data resides on low-dimensional manifolds, we perturb the data with different
levels of Gaussian noise, and jointly estimate the corresponding scores, i.e.,
the vector fields of gradients of the perturbed data distribution for all noise
levels. For sampling, we propose an annealed Langevin dynamics where we use
gradients corresponding to gradually decreasing noise levels as the sampling
process gets closer to the data manifold. Our framework allows flexible model
architectures, requires no sampling during training or the use of adversarial
methods, and provides a learning objective that can be used for principled
model comparisons. Our models produce samples comparable to GANs on MNIST,
CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score
of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn
effective representations via image inpainting experiments.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->


<!--Post--------------------------------------------------------------------------------------------------------------------------------------------->
{% assign id = id | plus:1 %}
<div class="entry">
	<div class="row">
	  <div class="column1" >
{% capture x %}
## Deep Unsupervised Learning using Nonequilibrium Thermodynamics
{% endcapture %}{{ x | markdownify }}

<div id="block_container">
		<div id="bloc2">
         <span class="dates"> March 12, 2015 </span>    
         <span class="authors"> Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli </span>
		</div>  
</div>

	  </div>
	  <div class="column2" >
	    <div id="block_container">
		<div id="bloc2">
		<a href="https://arxiv.org/abs/1503.03585" target="_blank" class="btn btn-info" role="button"> Paper </a>
		</div>  
		<div id="bloc3">
		     <p>
		      <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse{{ id }}" aria-expanded="false" aria-controls="collapse{{ id }}">
				Abstract
		      </button>
		    </p>
        
		</div>  
	     </div>
	  </div>
	</div>
	<div class="row">
	  <div class="column1" >
	  </div>
	  <div class="column2">
		  <div class="tags">
			  cs.LG, cond-mat.dis-nn, q-bio.NC, stat.ML
		  </div>
		  
	  </div>
	</div>

<div class="collapse" id="collapse{{ id }}">
{% capture x %}
 A central problem in machine learning involves modeling complex data-sets
using highly flexible families of probability distributions in which learning,
sampling, inference, and evaluation are still analytically or computationally
tractable. Here, we develop an approach that simultaneously achieves both
flexibility and tractability. The essential idea, inspired by non-equilibrium
statistical physics, is to systematically and slowly destroy structure in a
data distribution through an iterative forward diffusion process. We then learn
a reverse diffusion process that restores structure in data, yielding a highly
flexible and tractable generative model of the data. This approach allows us to
rapidly learn, sample from, and evaluate probabilities in deep generative
models with thousands of layers or time steps, as well as to compute
conditional and posterior probabilities under the learned model. We
additionally release an open source reference implementation of the algorithm.
{% endcapture %}{{ x | markdownify }}
</div>
</div>
<!--EndPost--------------------------------------------------------------------------------------------------------------------------------------------->



  <script src="https://utteranc.es/client.js"
        repo="ScoreBasedGenerativeModeling/scorebasedgenerativemodeling.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns" crossorigin="anonymous"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-select/1.13.1/js/bootstrap-select.min.js"></script>

</div>

<script>

$('#yearselection').change(function (e) {
   	var items = document.getElementsByClassName('entry');
	var tags_selected = $(this).val();
   	for (let i = 0; i < items.length; i++) {
	     var tag_items = items[i].getElementsByClassName("dates")[0];
	     var year_tag = tag_items.textContent.trim().split(',')[1]; 
       console.log(year_tag);
      	const found = tags_selected.some(r=> year_tag.includes(r));
	      if (found || tags_selected.length === 0) {
		items[i].style.display  = 'unset';
	      }
		else {
		items[i].style.display  = 'none';
		}
	    }
})


$('#authorselection').change(function (e) {
   	var items = document.getElementsByClassName('entry');
	var tags_selected = $(this).val();
   	for (let i = 0; i < items.length; i++) {
	     var tag_items = items[i].getElementsByClassName("authors")[0];
	     var item_tags = tag_items.textContent.trim().split(','); 
	     item_tags =  item_tags.map(v => v.replace("\n", '').replace("\t", '').trim());
      	      const found = item_tags.some(r=> tags_selected.includes(r));
	      if (found || tags_selected.length === 0) {
		items[i].style.display  = 'unset';
	      }
		else {
		items[i].style.display  = 'none';
		}
	    }
})
	
$('#tagselection').change(function (e) {
   	var items = document.getElementsByClassName('entry');
	var tags_selected = $(this).val();
   	for (let i = 0; i < items.length; i++) {
	     var tag_items = items[i].getElementsByClassName("tags")[0];
	     var item_tags = tag_items.textContent.trim().split(','); 
	     item_tags =  item_tags.map(v => v.replace("\n", '').replace("\t", '').trim());
	      console.log("item tags");
	      console.log(item_tags);
      	      const found = item_tags.some(r=> tags_selected.includes(r));
	      if (found || tags_selected.length === 0) {
		items[i].style.display  = 'unset';
	      }
		else {
		items[i].style.display  = 'none';
		}
	    }
})

$('#titleselection').change(function (e) {
   	var items = document.getElementsByClassName('entry');
	var tags_selected = $(this).val();
   	for (let i = 0; i < items.length; i++) {
	     var tag_items = items[i].getElementsByClassName("title");
	     var item_tags = tag_items.textContent.trim().split(','); 
	     item_tags =  item_tags.map(v => v.replace("\n", '').replace("\t", '').trim());
	      console.log("item tags");
	      console.log(item_tags);
      	      const found = item_tags.some(r=> tags_selected.includes(r));
	      if (found || tags_selected.length === 0) {
		items[i].style.display  = 'unset';
	      }
		else {
		items[i].style.display  = 'none';
		}
	    }
})
</script>

